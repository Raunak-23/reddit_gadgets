type,id,author,title,score,created_utc,url,num_comments,subreddit,body,parent_id,depth,post_id
post,1jellhn,chrisdh79,"Nvidia announces DGX desktop “personal AI supercomputers” | Asus, Dell, HP, and others to produce powerful desktop machines that run AI models locally.",869,2025-03-19 01:22:08,https://www.reddit.com/r/gadgets/comments/1jellhn/nvidia_announces_dgx_desktop_personal_ai/,258.0,gadgets,,,,
comment,mijm3kd,zirky,,837,2025-03-19 01:26:17,,,,can i just buy a regular ass graphics card at a reasonable price,,0.0,1jellhn
comment,mijmamg,legendov,,392,2025-03-19 01:27:26,,,,No,mijm3kd,1.0,1jellhn
comment,mil054y,spudddly,,110,2025-03-19 08:01:08,,,,you must pay 15000 so you can run a chatbot on your desktop because we invested 15 trillion into it and would reallly like some money back,mijmamg,2.0,1jellhn
comment,mimfr5i,thirteennineteen,,6,2025-03-19 14:22:26,,,,To be fair I do really want Chat GPT running locally _ツ_,mil054y,3.0,1jellhn
comment,mimjnew,Cthulhar,,1,2025-03-19 14:42:42,,,,Just get Jan ai,mimfr5i,4.0,1jellhn
comment,mikqznh,half-baked_axx,,64,2025-03-19 06:23:50,,,,why waste chip on 32gb 2000 consumer card when you can sell 96gb workstation monsters for 10000 were fucked,mijmamg,2.0,1jellhn
comment,mim0508,hbdgas,,11,2025-03-19 12:53:39,,,,Thats OK just wait 510 years and youll be able to get one of those workstations used for 20000,mikqznh,3.0,1jellhn
comment,mily7j7,alidan,,12,2025-03-19 12:41:38,,,,you can keep everything you want at the 3nm or whatever the fuck node it is go back to the last node you used and older and just make gpus on those and sell them at whatever price makes sense hell you could make 700mm dies or multiple smaller dies and no one would care because instead of 20k per wafer they would be 15k per wafer on mature nodes that no one really has a demand for stop screwing over everyone with fusing crap off your dies and just make a unified architecture you can use for high end workstations and normal gpus your bread and butter market is all stupidly high end headless crap anyway why segment a market anymore,mikqznh,3.0,1jellhn
comment,mink8ot,parisidiot,,4,2025-03-19 17:41:08,,,,but it doesnt work that way tsmc or whoever has limited capacity say they can only make 10000 chips in a month or whatever nvidia has customers for 10000 datacenter chips why would they bother making any of the lower end chips that earn them less money if they can sell all of the more expensive ones,mily7j7,4.0,1jellhn
comment,mixtbaf,zzazzzz,,1,2025-03-21 07:49:23,,,,fab capacity is on a per node basis you completely missed the commenters point they wouldnt be trading any capacity of their dataceter chips they would buy additional capacity of a much cheaper node,mink8ot,5.0,1jellhn
comment,miyxdmj,parisidiot,,1,2025-03-21 13:23:49,,,,does TSMC not have a maximum number of wafers they can make regardless theyre not building a new factory for each node no my assumption and it is an assumption is based on conjecture that one node would take capacity from the others im wrong on this im guessing,mixtbaf,6.0,1jellhn
comment,miz99pd,zzazzzz,,1,2025-03-21 14:27:43,,,,i mean do you think they just throw away the whole production line every time they go to a new node and if you dont need cutting edge node you dont need TSMC samsung produces more chips than TSMCeach year for example,miyxdmj,7.0,1jellhn
comment,mizwwws,parisidiot,,1,2025-03-21 16:23:28,,,,yes generally production lines in other industries are retooled when ford starts producing the 226 f150 they dont usually build a whole new factory they retool an older production line tsmc can only produce n wafers per year they dont just add on new fab capacity nonstop i mean im sure they do but they arent necessarily keeping older lines producing at the same capacity as they were before production space machinery raw materials these are all finite if they can dedicate them exclusively to higher margin units they will no,miz99pd,8.0,1jellhn
comment,miqzcah,Moscato359,,1,2025-03-20 05:10:34,,,,Nvidia doesnt make any products at all Remember this,mily7j7,4.0,1jellhn
comment,mijwnqs,sargonas,,12,2025-03-19 02:27:32,,,,Why Theres no profit in it for them to do that,mijmamg,2.0,1jellhn
comment,ming081,Hour_Reindeer834,,1,2025-03-19 17:21:03,,,,I think they could sell enough small cool quiet reliable GPUs easy,mijwnqs,3.0,1jellhn
comment,minym4r,sargonas,,2,2025-03-19 18:50:14,,,,Yeah but for every silicon chip that comes out of TSMCs fab on their hardware they can sell as a consumer GPU for 1k or put that same chip on an enterprise AI card for 25100k,ming081,4.0,1jellhn
comment,mikjw9p,trashacount12345,,-33,2025-03-19 05:15:19,,,,Indeed Why should they bust their ass for you,mijwnqs,3.0,1jellhn
comment,mijnb39,Bangaladore,,56,2025-03-19 01:33:15,,,,I get the frustration on the GPU side but to be clear the highest end consumer GPU has like 32 GB of usable memory for AI models These systems go up to 784GB of unified memory for AI models,mijm3kd,1.0,1jellhn
comment,mijq2xa,ericmoon,,60,2025-03-19 01:49:11,,,,Can I use it while microwaving something without tripping a breaker,mijnb39,2.0,1jellhn
comment,mik3f4i,StaticFanatic3,,8,2025-03-19 03:09:52,,,,Im guessing its going to be something like the AMD Strix halo chips in which case itll probably use less power than a typical desktop PC with a discrete graphics card,mijq2xa,3.0,1jellhn
comment,mime78g,sprucenoose,,3,2025-03-19 14:14:24,,,,Depends Do you have any friends at your local power company With some mild rolling brownouts they can probably throw enough juice your way,mijq2xa,3.0,1jellhn
comment,mijvfbn,renome,,9,2025-03-19 02:20:11,,,,Damn how much electricity will these bad boys suck up,mijnb39,2.0,1jellhn
comment,mik3zxh,xiodeman,,20,2025-03-19 03:13:40,,,,Dont worry the expansion port has a dangling nuclear reactor,mijvfbn,3.0,1jellhn
comment,mil8471,worotan,,12,2025-03-19 09:27:52,,,,But dont worry were taking climate change seriously so you dont need to reduce consumption just keep buying more stuff and hope the problem disappears magically Whatever you do dont feel any responsibility to do anything but keep buying stuff despite that being specifically what climate scientists are now shouting at us is disastrous,mijvfbn,3.0,1jellhn
comment,mil95wf,Anduin1357,,-6,2025-03-19 09:38:51,,,,All that this proves is that the only way to scale ecofriendly energy is up not sideways Besides the reason why AI is energy intensive at the moment is because of how immature AI is today When hardware is ready to fossilize mainstream inferencing well have ASIC NPUs take over and drive down the kWprompt tokeninference token NPUs are specifically the hardware that we will all be obligated to buy and replace GPUs for AI use which would probably be like 5 years away 2 hardware cycles,mil8471,4.0,1jellhn
comment,milqq6l,worotan,,2,2025-03-19 11:50:54,,,,So we just tell climate change to pause while we sort things out Doesnt work like that How about you deal with it in the real world rather than repeating marketing pr We need to reduce consumption of resources not increase them Thats called serious climate science,mil95wf,5.0,1jellhn
comment,milu7kt,Anduin1357,,0,2025-03-19 12:15:06,,,,And why should I reduce my consumption if my life can only get better by increasing consumption It has been more than a decade of climate policies and while there was some good a lot of it shows that even the climate activists arent serious about the movement At this rate I would much rather that humanity fix our governance so that we are prepared for legitimate change not the current shame the individuals campaign going on right now You want to reduce my climate impact Go nuclear Fusion or fission I dont care and you cant make me not consume Edit By the way those werent marketing PR They were facts,milqq6l,6.0,1jellhn
comment,mikr5qe,Spirited-Pause,,4,2025-03-19 06:25:34,,,,Yes,mijvfbn,3.0,1jellhn
comment,mimm6ij,econpol,,2,2025-03-19 14:55:40,,,,However much it is itll be worth it for some furry romance novel brain storming sessions,mijvfbn,3.0,1jellhn
comment,mil5ibo,Fairuse,,3,2025-03-19 09:00:18,,,,Unless there are multiple GPU dies it sounds like it will basically use as much power as a typical GPU The main thing with these devices is larger fast RAM which doesnt take that much power to run,mijvfbn,3.0,1jellhn
comment,mijvi9a,Optimus_Prime_Day,,5,2025-03-19 02:20:40,,,,At what cost though 10k,mijnb39,2.0,1jellhn
comment,mik6kop,ye_olde_green_eyes,,8,2025-03-19 03:31:07,,,,Since the systems will be manufactured by different companies Nvidia did not mention pricing for the units However in January Nvidia mentioned that the baselevel configuration for a DGX Sparklike computer would retail for around 3000,mijvi9a,3.0,1jellhn
comment,mik8j34,Bangaladore,,10,2025-03-19 03:44:40,,,,Key word base level,mik6kop,4.0,1jellhn
comment,mikxrq5,AndersDreth,,-5,2025-03-19 07:35:19,,,,In this day and age I wouldnt pay that kind of money but if AI keeps getting smarter Ill bet were all scrambling to get the best AI just like we do with graphic cards,mik6kop,4.0,1jellhn
comment,milhm2u,VampireFrown,,0,2025-03-19 10:36:52,,,,I wouldnt because I dont have room temperature IQ and can adequately research and create stuff for myself,mikxrq5,5.0,1jellhn
comment,milwvsu,Primary_Opal_6597,,1,2025-03-19 12:32:59,,,,Okay but have you ever tried finding a recipe online,milhm2u,6.0,1jellhn
comment,mily7gb,VampireFrown,,2,2025-03-19 12:41:37,,,,Yes Its not hard,milwvsu,7.0,1jellhn
comment,millxdm,AndersDreth,,2,2025-03-19 11:13:49,,,,Because thats the only thing AI is used for s,milhm2u,6.0,1jellhn
comment,milo733,VampireFrown,,-2,2025-03-19 11:31:54,,,,And what research applications do you envisage AI being useful for from the comfort of your own fucking bedroom lol,millxdm,7.0,1jellhn
comment,milppq1,AndersDreth,,1,2025-03-19 11:43:29,,,,You dont get it do you In a couple of decades your Alexa isnt just some dumb microphone that can tell you fart jokes and order things from Amazon Everyone is going to have an AI that can actually do shit reliably but how reliable depends on the solution you end up going for,milo733,8.0,1jellhn
comment,milur58,VampireFrown,,-1,2025-03-19 12:18:46,,,,No I do get it and likely to a far deeper degree of technical expertise than you given that Ive been in the mix for 10 years by now and not merely the past couple of years like most others You didnt answer my question What do you specifically envisage needing an AI model for in your own bedroom Lets assume it does whatever the thing is perfectly accurately sure what do you need it for,milppq1,9.0,1jellhn
comment,mim23y1,AndersDreth,,1,2025-03-19 13:05:38,,,,Partly for the same reason I would prefer outright buying a house or a car instead of leasing them and partly because I consider the things I do in my bedroom or home for that matter private I dont trust companies for shit when it comes to privacy and with good reason so why in the everliving fuck would I want to subscribe to a cloudbased AI where theres concerns like latency due to traffic concerns about privacy and concerns about how well the longterm memory of the AI is maintained,milur58,10.0,1jellhn
comment,millimg,684beach,,0,2025-03-19 11:10:25,,,,Says that and then says ressarch Who knows maybe in your research youre the type of person to confuse lighting with lightning,milhm2u,6.0,1jellhn
comment,mijvnno,Bangaladore,,2,2025-03-19 02:21:34,,,,Probably more Who knows,mijvi9a,3.0,1jellhn
comment,mik6ttu,typo180,,2,2025-03-19 03:32:50,,,,3k with 1TB SSD 4k with 4TB,mijvi9a,3.0,1jellhn
comment,mik8nci,Bangaladore,,2,2025-03-19 03:45:30,,,,That doesnt get you much gpu memory They will have many tiers at many times higher prices,mik6ttu,4.0,1jellhn
comment,mik98yj,typo180,,2,2025-03-19 03:49:49,,,,Ah I misread the context when I replied Yes I think those prices get you 128 GB of unified memory,mik8nci,5.0,1jellhn
comment,mim0if8,lostinspaz,,1,2025-03-19 12:55:53,,,,its specifically 3999,mijvi9a,3.0,1jellhn
comment,mil5ynh,Fairuse,,1,2025-03-19 09:05:07,,,,At 10k for 784GB of RAM it would beat the shit out of the newly released M3 Ultra with 512GB of RAM The M3 Ultra only saving grace right now is that it has tons of RAM A 5090 with same amount of RAM would run circles around the M3 Ultra Even at 15k it will still make the M3 Ultra obsolete The DGX station with 784GB of RAM would need to be like 30k to make anyone consider M3 Ultra 512GB 10k,mijvi9a,3.0,1jellhn
comment,mil9t4o,Anduin1357,,0,2025-03-19 09:45:23,,,,288GB HBM3e paired with an ARM CPU Yeah no Whats the use case Be an inference and training server,mil5ynh,4.0,1jellhn
comment,miksoa1,None,,-2,2025-03-19 06:41:06,,,,Theyll be leased at 1k a month but energy bills will be 9k per month so yes 10k per month,mijvi9a,3.0,1jellhn
comment,mikpixi,techieman33,,2,2025-03-19 06:09:05,,,,The complaint isnt about the memory Its that fab time is going to making AI chips instead of consumer GPUs Which is understandable from a business standpoint since there is a lot more profit in AI chips But it does suck for us consumers,mijnb39,2.0,1jellhn
comment,mio19pj,norbertus,,1,2025-03-19 19:02:58,,,,NVIDIAs consumer business is a side hustle at this point Last year NVIDIA reported 1152 billion in data center revenue Their gamaing market was a mere 10 that size,mikpixi,3.0,1jellhn
comment,mimr314,wuvonthephone,,1,2025-03-19 15:20:15,,,,Isnt it sort of wild that the entire business of AAA games is dependent on two companies Even consoles will be more expensive because of this First crypto block chain nonsense now this,mijnb39,2.0,1jellhn
comment,miloiqu,NZafe,,5,2025-03-19 11:34:24,,,,AMD has entered the chat,mijm3kd,1.0,1jellhn
comment,milxs6m,garry4321,,3,2025-03-19 12:38:52,,,,gifgiphyj2GLSp9qUpcTfv4w11downsized,mijm3kd,1.0,1jellhn
comment,mimh3ee,GameOvaries18,,4,2025-03-19 14:29:24,,,,No and you will have to pay 250 a year to update Chat GPT which you didnt want in the first place,mijm3kd,1.0,1jellhn
comment,mijn7gj,PineappleLemur,,9,2025-03-19 01:32:40,,,,AMD,mijm3kd,1.0,1jellhn
comment,mijuuwd,MagicOrpheus310,,5,2025-03-19 02:16:48,,,,You mean AMD Haha,mijm3kd,1.0,1jellhn
comment,mijz6dp,Starfox-sf,,8,2025-03-19 02:42:47,,,,It will need a special AIgenerated ass graphics card,mijm3kd,1.0,1jellhn
comment,mip18h4,FallingUpwardz,,2,2025-03-19 22:02:18,,,,How else do you think theyre paying for these,mijm3kd,1.0,1jellhn
comment,miqzakm,Moscato359,,2,2025-03-20 05:10:08,,,,Sorry they make ai compute modules that can also be used to display video instead,mijm3kd,1.0,1jellhn
comment,mijojxu,brickyardjimmy,,2,2025-03-19 01:40:30,,,,Never sir never,mijm3kd,1.0,1jellhn
comment,mijupoq,MagicOrpheus310,,183,2025-03-19 02:15:58,,,,Gaming really is just a hobby for NVIDIA at this point,,0.0,1jellhn
comment,mijzzau,None,,79,2025-03-19 02:47:46,,,,its less than 10 of their revenue they could give a fuck about gaming,mijupoq,1.0,1jellhn
comment,mikcsdf,santathe1,,43,2025-03-19 04:16:19,,,,David Mitchell explains,mijzzau,2.0,1jellhn
comment,mikguj0,bit1101,,32,2025-03-19 04:48:54,,,,How does more than half of USA get this wrong,mikcsdf,3.0,1jellhn
comment,mil3ia5,KrtekJim,,17,2025-03-19 08:38:24,,,,I think they do it on accident,mikguj0,4.0,1jellhn
comment,min0ak0,None,,3,2025-03-19 16:05:29,,,,I correct this every time I hear it Also I seen a guy the other day Get it together America,mil3ia5,5.0,1jellhn
comment,miq6j8f,woodcookiee,,1,2025-03-20 01:50:26,,,,I had never heard this until I took a job in a rural city for a couple years Could be having a normal intelligent conversation with someone then suddenly they tell me about how they seen something or someone wtf,min0ak0,6.0,1jellhn
comment,miktnpb,None,,3,2025-03-19 06:51:20,,,,honestly I understand it its incorrect but I only ever realize in hindsight that I said it wrong again because I only use the phrase so often now but I grew up saying it wrong a lot Old habits die hard,mikguj0,4.0,1jellhn
comment,minzclq,blank_isainmdom,,2,2025-03-19 18:53:46,,,,Ive been watching Soapbox again lately Good times,mikcsdf,3.0,1jellhn
comment,mil2bkd,TotoCocoAndBeaks,,20,2025-03-19 08:25:13,,,,Companies do care about ten percent of their revenue And thats an awful misuse of could So its just pretty funny that through bad grammar your post ended up being correct,mijzzau,2.0,1jellhn
comment,milt0rk,HiddenoO,,2,2025-03-19 12:07:00,,,,Companies do care about ten percent of their revenue They could likely more than make up for that revenue by investing those wafers into more AI and data centre chips while saving on advertising and gamingrelated development The main reason they still care about consumer GPUs is that 1 its good as advertisement for Nvidia being the best in the compute market and 2 its their fallback for when the AI bubble bursts,mil2bkd,3.0,1jellhn
comment,minn8d6,Plebius-Maximus,,2,2025-03-19 17:55:19,,,,Gaming grade chips arent workstationserver suitable though Even the 5090 isnt a full die its a defective one which is why it has cores missing Vs the rtx pro 6000 You also cant sell all the low grade stuff like 60class chips to data centres They have no need for it You can slap some lights on any kind of GPU and sell it to gamers though The profit margins on the gaming stuff are still massive Even if they arent as high as professional stuff,milt0rk,4.0,1jellhn
comment,miqzmja,Moscato359,,0,2025-03-20 05:13:09,,,,You also cant sell all the low grade stuff like 60class chips they could simply make more of the larger chips and not order the smaller chips at all Even the 5090 isnt a full die They can fuse off them and sell them to datacenters as a stepdown model,minn8d6,5.0,1jellhn
comment,min3ps5,GrayDaysGoAway,,0,2025-03-19 16:22:10,,,,They could likely more than make up for that revenue by investing those wafers into more AI and data centre chips No they cant Theyre already producing that stuff as quickly as they possibly can The bottleneck is in the packaging not a lack of chips GPUs are the only way for them to earn that 10,milt0rk,4.0,1jellhn
comment,mil3pki,None,,-11,2025-03-19 08:40:39,,,,I am aware of the correct way to say it its just force of habit While they care about gaming theyre not catering prices to gamers because theyre aware that 90 of their chips go to data centers or other businesses Theyre willing to lose that 10 because that 10 used to be more than half of their revenue Nvidia has seen an unparalleled increase in valuation due to the demand from data centers and now machine learning That 10 of revenue this year will be less than 2 by 2030 You have to be aware that Nvidias growth is almost entirely driven by these mentioned target markets in order to have an accurate understanding as possible regarding their attitudes toward the gaming segment Does Nvidia care about the gaming market Of course Do they care enough to consider them in their pricing strategy Not a chance Nvidia predicts its revenue to grow to 1 trillion by 2028 and research doesnt deny the possibility Gaming revenues are currently 11 billion Considering the gaming market is reaching some state of maturity its growth rate will be nowhere near the emerging AI and cloud computing markets I am aware this is reddit and its easier to make fun of someones grammar than it is to employ nuance I did my best to use correct grammar If I am wrong please feel free to correct me,mil2bkd,3.0,1jellhn
comment,mil8hj6,ChinoGambino,,2,2025-03-19 09:31:45,,,,Their business would have to grow by 8x to even come close to 1T in revenue Maybe Trump will destroy the USD making that possible but it seems impossible Theres only so many wafers that can be bought and products that can be thrown at the market,mil3pki,4.0,1jellhn
comment,min16ac,None,,0,2025-03-19 16:09:47,,,,bud just say you dont understand the current chip market,mil8hj6,5.0,1jellhn
comment,mimrtbu,_unsinkable_sam_,,3,2025-03-19 15:23:53,,,,they could or couldnt,mijzzau,2.0,1jellhn
comment,milhc7p,NotAnADC,,1,2025-03-19 10:34:20,,,,im still holding out hope for the shield tv pro 2 that they through together with what amounts to loose change for them,mijupoq,1.0,1jellhn
comment,mijyoi9,Johnson_N_B,,-7,2025-03-19 02:39:47,,,,Always has been,mijupoq,1.0,1jellhn
comment,mijml2s,joestaff,,113,2025-03-19 01:29:05,,,,After seeing DeepSeek I figured home AI servers were going to eventually be a thing Maybe not a common thing but not so uncommon that itd be shocking to see Like smart lights or outlets,,0.0,1jellhn
comment,mijt3yp,rocket-lawn-chair,,15,2025-03-19 02:06:32,,,,They already exist You can pop a pair of highvram cards in a chassis with a moboprocessor for LLM models of moderate size Smaller models can even run on a rasp pi 5 Its surprising what you can already do to run local chat models Its really the training of the model thats most intensive This product seems like its built for more than just a local chat bot,mijml2s,1.0,1jellhn
comment,mijwat6,geekwonk,,5,2025-03-19 02:25:23,,,,ugh i really really want to get a 16GB Pi 5 and that 26TOPS AI HAT ive got RAM for days around this house but i dont game so i can load up models quickly and watch them spend a bunch of time working on Hello World,mijt3yp,2.0,1jellhn
comment,milv2ja,HiddenoO,,1,2025-03-19 12:20:54,,,,The issue is that its costeffective for almost nobody If eg your average prompt has 1k tokens input and 1k tokens output 2k words each you can do 2000 GeminiFlash 20 requests per 1 Even at 1000 requests a day which takes heavy use likely including agents and RAG thats only 15 a month Even if your LLM workstation only cost 25k 2x used 3090 and barebones components itd take you 14 years until it pays off and thats assuming cloud LLMs wont get any cheaper Flash 20 also performs on par with or better than most modelsquants you can use with 2x 3090 so you really need very specific reasons finetuning privacy etc for the local workstation to be worth using Those exist but the vast majority of people wouldnt pay such a hefty premium for them,mijt3yp,2.0,1jellhn
comment,mimncpm,Tatu2,,6,2025-03-19 15:01:33,,,,Privacy I think would be the largest reason That way the information that youre feeding and receiving isnt shared out to the internet and stored in some location by some other company,milv2ja,3.0,1jellhn
comment,mimph2s,HiddenoO,,5,2025-03-19 15:12:14,,,,It is but the vast majority of people dont give nearly as much of a fuck about privacy in that sense as the Reddit privacy evangelists will make you believe,mimncpm,4.0,1jellhn
comment,mimrs9z,Tatu2,,3,2025-03-19 15:23:45,,,,I agree even as a security engineer This seems like a pretty niche product that I dont see too many use cases for I dont imagine this will sell well I could see businesses wanting that especially if they working with personal health information but thats not what this product is intended for Its personal use,mimph2s,5.0,1jellhn
comment,mimuk1e,IAMA_Madmartigan,,2,2025-03-19 15:37:31,,,,Yeah thats the biggest one for me Being able to link into all my personal files and run things without uploading requests to a server,mimncpm,4.0,1jellhn
comment,mj0u6b7,NihilisticAngst,,3,2025-03-21 19:03:25,,,,I agree with you that its not cost effective and especially not for an average user However depending on what youre doing with the LLMs you dont need anywhere near 2 3090s Ive been successfully running local LLMs on my personal data with only a 4070 and 12GB of VRAM Lower end LLM models are also becoming more and more capable as development continues For many people running local LLMs is viable with minimal additional investment Personally Im very interested in potentially purchasing one of these AI supercomputers in the future,milv2ja,3.0,1jellhn
comment,mj0wusp,HiddenoO,,1,2025-03-21 19:16:36,,,,The topic was about buying home AI servers not running it on your existing machine Also frankly speaking if youre not an enthusiast willing to spend the time to mess around with a lot of models andor finetune them the performancepower expenditure in will still be worse than what you get from just using Flash 2Mistral Small,mj0u6b7,4.0,1jellhn
comment,mijngyc,PM_ME_YOUR_KNEE_CAPS,,41,2025-03-19 01:34:10,,,,M3 Mac Ultra,mijml2s,1.0,1jellhn
comment,mik8piu,f-elon,,12,2025-03-19 03:45:55,,,,My M2 Ultra runs 250GB LLMs without a hitch,mijngyc,2.0,1jellhn
comment,mimysw3,mdonaberger,,6,2025-03-19 15:58:13,,,,feel how you will about Apple this shit right here is why i have been yelling to anyone who would listen about ARM servers since 2003 my first entrypoint to selfhosting was the TonidoPlug which cost a total of 2 to run 247 for a whole year,mik8piu,3.0,1jellhn
comment,mip4ejx,Bluedot55,,2,2025-03-19 22:18:55,,,,While apple is making excellent hardware right now Im not sure how much of it is arm vs good design and being willing to spend more on the cutting edge node and go for a wider core thats lower on the vf curve,mimysw3,4.0,1jellhn
comment,mip3689,xxAkirhaxx,,3,2025-03-19 22:12:25,,,,Worth noting that the mac m4 max comes at a similar albeit cheaper price point for the same amount of Unified RAM with twice the memory bandwidth It would be comparable to having a 3070 running 128gb of VRAM This thing this AI box theyre making is a joke I think its meant for people who dont know about locally running models who want something that will just work and dont want to learn Which is fair I guess But technically thats always been Apples job and I dont like that NVIDIA is outdoing them in the same dept,mik8piu,3.0,1jellhn
comment,mimn1ac,_hephaestus,,2,2025-03-19 14:59:57,,,,What quants Doesnt the M2 max out at 192 Probably a better deal than M3 since they didnt up bandwidth,mik8piu,3.0,1jellhn
comment,miof9ko,f-elon,,1,2025-03-19 20:13:24,,,,Mine is not maxed out but yeah ram caps at 192 24 core CPU 60 core GPU 32 core NE 128 GB RAM,mimn1ac,4.0,1jellhn
comment,mion26z,_hephaestus,,1,2025-03-19 20:51:00,,,,I mean for the 250gb llms dont you have to use some heavy quantization to fit that in 128gb ram,miof9ko,5.0,1jellhn
comment,mio0fjy,lucellent,,1,2025-03-19 18:58:55,,,,If LLMs are all that you want to run sure but for CUDA apps its useless,mijngyc,2.0,1jellhn
comment,mijo7x6,Moist_Broccoli_1821,,-58,2025-03-19 01:38:33,,,,20k for trash AI super PC 3599,mijngyc,2.0,1jellhn
comment,mijoeps,PM_ME_YOUR_KNEE_CAPS,,28,2025-03-19 01:39:39,,,,95k 512GB fast ram can run deepseek Cant do that on anything cheaper,mijo7x6,3.0,1jellhn
comment,mijzogw,ndjo,,7,2025-03-19 02:45:54,,,,Quantized not full r1,mijoeps,4.0,1jellhn
comment,mijojw0,Moist_Broccoli_1821,,-51,2025-03-19 01:40:29,,,,Never buy apple PC products,mijoeps,4.0,1jellhn
comment,mijuik5,MargielaFella,,20,2025-03-19 02:14:49,,,,Any actual reasoning for this Or just blind hate,mijojw0,5.0,1jellhn
comment,mijuurg,NecroCannon,,19,2025-03-19 02:16:47,,,,Windows is definitely not better right now lets be real,mijuik5,6.0,1jellhn
comment,mimlof2,Two_Shekels,,1,2025-03-19 14:53:08,,,,Idk I absolutely love when windows constantly forces updates on me and inject AI bs into literally every component of the system,mijuurg,7.0,1jellhn
comment,min119z,NecroCannon,,2,2025-03-19 16:09:07,,,,Youre right when I heard they were taking constant screenshots to train AI I just thought Theres so many cameras why not use AI to learn our facial expressions Its just AI so no worries on being toilet cammed I love the future where AI is a middleman for our privacy just being completely gone and on 247 corporate surveillance,mimlof2,8.0,1jellhn
comment,mijve6o,MargielaFella,,-6,2025-03-19 02:19:59,,,,You replied to the wrong person,mijuurg,7.0,1jellhn
comment,mijvkh1,NecroCannon,,14,2025-03-19 02:21:02,,,,No Im just agreeing with you lol Linux and MacOS are looking better by the day,mijve6o,8.0,1jellhn
comment,mijyy1k,Webfarer,,5,2025-03-19 02:41:23,,,,No sir this is a space where we hate each other profusely,mijvkh1,9.0,1jellhn
comment,mikmgx6,hans_l,,3,2025-03-19 05:39:16,,,,Aye Linux users and windows users are natural enemies Like Linux users and Mac users Or Linux users and BSD users Or Linux users and other Linux users,mijyy1k,10.0,1jellhn
comment,mik607j,MargielaFella,,1,2025-03-19 03:27:15,,,,True Yeah Windows has a monopoly on PC gaming so for now it has a use case for me Apples software division is increasingly becoming lazier and introduce more bugs than they fix with every release If Apple keeps going down this trajectory Ill leave for Linux Im using an M3 Pro right now but Ill happily switch to a Thinkpad with Ubuntu or Mint,mijvkh1,9.0,1jellhn
comment,mik3zme,DarkKumane,,0,2025-03-19 03:13:37,,,,As a certified Mac hater for years the only thing I hate about it nowadays is the overvaluation of Apple egregious overpricing of products and lack of modular componentsright to repair Despite all that I would still pick Mac over Windows right now Imo Linux is currently king and its more accessible than ever right now I highly recommend checking out a distro like mint or ubuntu to anyone reading this whos on the fence about trying a new OS,mijvkh1,9.0,1jellhn
comment,mik30fr,_RADIANTSUN_,,-12,2025-03-19 03:07:11,,,,GPU compute capacity less than a 4080 on a 30k computer lol,mijoeps,4.0,1jellhn
comment,mikea7y,FightOnForUsc,,10,2025-03-19 04:28:04,,,,With what 20x the RAM,mik30fr,5.0,1jellhn
comment,mikej8u,_RADIANTSUN_,,-12,2025-03-19 04:30:05,,,,Ya exactly Imagine how dogshit the performance of a 4080 would be on 512GB scale model Apparently the ultra performant R1 4bit quant produces a measly 18tks Lol,mikea7y,6.0,1jellhn
comment,mikesbp,FightOnForUsc,,10,2025-03-19 04:32:07,,,,Whats your point If nvidia put 512 GB of 800GBs RAM on a 4080 it would be ridiculously more expensive than it is So youre saying this thing is better but its not and then say oh but thats too expensive but it provides more,mikej8u,7.0,1jellhn
comment,mikey36,_RADIANTSUN_,,-10,2025-03-19 04:33:26,,,,The point is that the compute performance will be worse than a 1K GPU for a 30k machine lmao,mikesbp,8.0,1jellhn
comment,mikf64i,FightOnForUsc,,7,2025-03-19 04:35:15,,,,Except that it costs 9499 Shop me another GPU with access to 512 GB of RAM or similar for less,mikey36,9.0,1jellhn
comment,miki4ex,_RADIANTSUN_,,-2,2025-03-19 04:59:42,,,,Why do you keep trying to change the subject from the objectively horrible compute performance for a 10k machine,mikf64i,10.0,1jellhn
comment,mimqx42,rnobgyn,,1,2025-03-19 15:19:26,,,,Man ngl youre all over the place on this one,miki4ex,11.0,1jellhn
comment,miovhgn,_RADIANTSUN_,,0,2025-03-19 21:33:02,,,,How so Its pretty straightforward Point out what your problem is specifically Its a GPU weaker than a 5080 so it will have horrible performance for running things at such large scales eg Deepseek R1 4bit quant runs at 18tks,mimqx42,12.0,1jellhn
comment,mijvrc5,geekwonk,,1,2025-03-19 02:22:10,,,,i think it tops out at around 14K,mijo7x6,3.0,1jellhn
comment,miltdi7,smulfragPL,,2,2025-03-19 12:09:26,,,,they arleady were for a long time,mijml2s,1.0,1jellhn
comment,mimc9tq,roamingandy,,2,2025-03-19 14:04:09,,,,Id love to properly train one on my writing style from all the documents ive ever written and have it answer all emails and such for me then send to me for editing and approval Done well and that could save so much time as the majority of our online communications are a rehash of things weve said or written in the past anyway,mijml2s,1.0,1jellhn
comment,minrq6c,ilyich_commies,,2,2025-03-19 18:16:46,,,,Training it on the stuff youve written wont get it to match your style very well unless youve written enough to fill multiple libraries Unfortunately AI just doesnt work like that Youd have better luck training it on all the text youve ever read and audio youve ever listened to but it would be impossible to compile a data set like that,mimc9tq,2.0,1jellhn
comment,minhd7x,lkn240,,1,2025-03-19 17:27:29,,,,My work laptop just got apple intelligence which will respond to emails for you Its pretty unimpressive so far Like Id be embarrassed to send out some of the things it comes up with,mimc9tq,2.0,1jellhn
comment,mik76bv,GregmundFloyd,,1,2025-03-19 03:35:15,,,,Ultrahouse 3000,mijml2s,1.0,1jellhn
comment,miwe02s,BluudLust,,1,2025-03-21 01:14:28,,,,Already have one You can offload some layers to the GPU and run the rest on the CPU Its fast enough,mijml2s,1.0,1jellhn
comment,mikizjs,ResponsibleTruck4717,,-1,2025-03-19 05:07:13,,,,I dont know if we will get home servers at least not just for llm This technology as whole is still in alpha beta state at best its unstable can give wrong answers sometimes it cant perform simple tasks As the technology mature if it will survive the hardware requirements will change and better optimization will be developed,mijml2s,1.0,1jellhn
comment,miksu5u,Brasou,,4,2025-03-19 06:42:45,,,,Theres already home LLM available its slow as tits and they are far perfect But yeah its already here just slow,mikizjs,2.0,1jellhn
comment,mijwq6k,Spara-Extreme,,37,2025-03-19 02:27:58,,,,Whats the use case for this outside of researchers and hobbyists I can understand a few of these machines hitting the market but cant imagine theres a huge customer base,,0.0,1jellhn
comment,mik94ae,GrandmaPoses,,44,2025-03-19 03:48:52,,,,Porn,mijwq6k,1.0,1jellhn
comment,mil7u3q,Bokbreath,,13,2025-03-19 09:24:56,,,,There it is,mik94ae,2.0,1jellhn
comment,minvxnr,BevansDesign,,3,2025-03-19 18:37:14,,,,You know how you go to a porn site and its full of awful weird stuff you dont want to see Imagine if you could go to one and it showed you exactly what you wanted Or even created it automatically Then you just set up a few Amazon delivery subscriptions and never have to leave your house again,mik94ae,2.0,1jellhn
comment,miksz5h,plissk3n,,18,2025-03-19 06:44:13,,,,Put in all my documents mails browsing history etc Than it do my taxes remind me of things which are overdue give me a tip where I might have seen a product online etc All things I never would want in a cloud service,mijwq6k,1.0,1jellhn
comment,milvkqh,HiddenoO,,11,2025-03-19 12:24:19,,,,Half of those you wouldnt want to do locally with current models either taxes or youre better off not using LLMs remind you of things which are overdue,miksz5h,2.0,1jellhn
comment,mimq6iz,plissk3n,,-1,2025-03-19 15:15:45,,,,It was more of a thought about the future and I do think that certain AIs will develop in some kind of personal assistants or even companions,milvkqh,3.0,1jellhn
comment,mik0v2o,CosmicCreeperz,,14,2025-03-19 02:53:20,,,,It doesnt mean home AI PC Those many thousands of AI companies actually way more than that as everyone is getting into it have many tens or hundreds of thousands of data scientists and ML engineers etc I knew a few DS who would kill to run large models locally,mijwq6k,1.0,1jellhn
comment,mik2dky,Spara-Extreme,,11,2025-03-19 03:03:00,,,,Sure but those companies also have access to cloud H100s That being said thats a good use case local development for companies building AI models for their products,mik0v2o,2.0,1jellhn
comment,mik4lat,CosmicCreeperz,,6,2025-03-19 03:17:37,,,,Heh reliable access to cloud H100s is very expensive since you have to reserve them or you may lose spot instances The cheapest instance is 30 an hour,mik2dky,3.0,1jellhn
comment,mil0g5t,AgencyBasic3003,,-1,2025-03-19 08:04:27,,,,Local development is not the main use case Sometimes you have customers which want your product but they want to run it on premise In this case you want to run all your models locally so that the data doesnt leave the network This can be especially useful if it is really sensitive company data that you dont want to run on third party infrastructure,mik2dky,3.0,1jellhn
comment,milwnkg,clumsynuts,,1,2025-03-19 12:31:29,,,,Are you referring to commercial use,mil0g5t,4.0,1jellhn
comment,mikfaql,FightOnForUsc,,2,2025-03-19 04:36:18,,,,No company has 100000s of data scientists and ML engineers I dont know if any have 10000s The most you would see would be at google or meta I think and theyre likely in the 1000s,mik0v2o,2.0,1jellhn
comment,mikgk8b,CosmicCreeperz,,1,2025-03-19 04:46:34,,,,That was across the industry of course not per company These computers arent going to sell millions but they could sell hundreds of thousands Certainly as much or more of a market as the Mac Pro,mikfaql,3.0,1jellhn
comment,mimkpok,FightOnForUsc,,1,2025-03-19 14:48:11,,,,Oh yes across the industry absolutely,mikgk8b,4.0,1jellhn
comment,mikmad5,habitual_viking,,4,2025-03-19 05:37:34,,,,I work in a financial institution we cant use LLMs because of security risk with sending data to foreign clouds Having AI machines on premise is a huge deal and at a starting price point of 3000 they could easily compete with cloud subscriptions if we were using those,mijwq6k,1.0,1jellhn
comment,milwxm6,clumsynuts,,5,2025-03-19 12:33:19,,,,Theyd more likely setup some onprem server that could service the entire org rather than buy everyone their own desktop,mikmad5,2.0,1jellhn
comment,mikrsej,GuerrillaRodeo,,1,2025-03-19 06:31:57,,,,Researchers is the most probable answer Just feed them textbooks and papers and let them generate answers real quick I already tried that with medical books on my oldass 2080 Ti and its surprisingly good even at this level,mijwq6k,1.0,1jellhn
comment,milwrv6,nicman24,,1,2025-03-19 12:32:16,,,,People running deepseek without an Internet connection,mijwq6k,1.0,1jellhn
comment,mim4b16,the_tethered,,1,2025-03-19 13:18:44,,,,Trading for sure,mijwq6k,1.0,1jellhn
comment,miu06xh,NotAHost,,1,2025-03-20 17:53:00,,,,It sounds like 3d printers in a different way They see one in every house but were not going to get there for 30 years,mijwq6k,1.0,1jellhn
comment,mik06ft,shrimel,,1,2025-03-19 02:49:01,,,,I imagine some businesses that need to keep their data on prem,mijwq6k,1.0,1jellhn
comment,mik0cjt,Spara-Extreme,,3,2025-03-19 02:50:05,,,,Maybe but nvidia already offers rack servers for that This seems like a workstation,mik06ft,2.0,1jellhn
comment,mim91py,User1539,,0,2025-03-19 13:46:07,,,,Star Trek computers What LLMs are really good at is understanding commands and forming a plan then carrying it out Computers have been hard to use You cant just say Print this out you have to know what printer you want to use and all that I think the idea is that they want you to feel like your computer is the 1980s cartoon character we all imagined Youll be able to talk to it itll help you come up with ideas and collaborate with realizing those ideas No more learning Photoshop or Autodesk You can just tell your computer you want to 3D print something and itll help you design it figure out how to connect to the printer and then print it out for you Thats what they want A computer that will tell you when to use Excel and how to use Excel then use it for you to get your report done as fast as possible If things keep moving forward well have appliances from the Jetsons eventually I think thats the idea anyway,mijwq6k,1.0,1jellhn
comment,mikxvi1,DirectStreamDVR,,0,2025-03-19 07:36:27,,,,Personal assistant Ideally its paired with a mobile app Imagine chatgpt with an unlimited memory you could feed it your entire life instead of just 100 memories You could connect it with every other LLM and when the thing you ask it is outside of its capabilities it can outsource to chat gpt or grok or whatever Pair it with your home security system allowing it to actually watch your cameras and say hey a man is outside with a package it could learn that the person walking by your house is just your neighbor who walks their dog everyday at this time it could say hey theres a guy outside breaking into your car it wouldnt just be a bleep on your phone while youre sleeping it could literally yell at you until youre awake Or pair it with a speaker outside and have it attempt to scare the intruder away Pair it with your smart home you could say hey its kinda getting dark or literally anything to the regards you wouldnt have you memorize the phrase the system could lookup exactly when the sun will set and turn the lights on at the perfect time Tell it to add things to it grocery list order it to be delivered Connect it to your front door bell let it talk with visitors tell it how to handle things like deliveries ie place at back door go away whatever Pair it with your cable box hey do I have any shows on tonight Yes in 5 minutes a new episode of lost is on do you want me to put it on Nah just set it to record Ok Obviously a lot of this is far off but having the brains inside your home is the first step Modules that connect with our products will come later,mijwq6k,1.0,1jellhn
comment,milajm9,weid_flex_but_OK,,-1,2025-03-19 09:52:42,,,,Not now but in the nearish future I imagine being able to have one of these machines running my home and helping me in parts of my life Id LOVE a Jarvistype system in my house that I can talk to quickly jot ideas to or make lists for bounce those ideas around withhelp me organize my projects and calendar maybe do my taxes tell me where I can save money keep check of my house and provide warnings of thing going wrong answering the door etc etc etc In my mind itll be like having a 247 assistant,mijwq6k,1.0,1jellhn
comment,mijyhay,None,,44,2025-03-19 02:38:35,,,,Oh yeah thats what I want in my life more fucking AI,,0.0,1jellhn
comment,miks3xr,PGMetal,,-31,2025-03-19 06:35:17,,,,Do you know what this even is It seems like you dont understand what anything in the title means,mijyhay,1.0,1jellhn
comment,mikweff,PoshInBoost,,21,2025-03-19 07:20:25,,,,Not the OP but I turn off AI features in all my devices If the selling point of there is having AI then why get one The technology isnt reliable enough yet and theres more effort going into monetising the users than improving accuracy so its not going to be good any time soon,miks3xr,2.0,1jellhn
comment,mikzm79,Dereklewis930,,-14,2025-03-19 07:55:21,,,,Youre not the target not everything is made for you,mikweff,3.0,1jellhn
comment,miltla9,smulfragPL,,-12,2025-03-19 12:10:55,,,,because it is reliable and incredibly useful and you either dont have a use case or dont know how to use it,mikweff,3.0,1jellhn
comment,mirgfxb,PoshInBoost,,1,2025-03-20 08:06:00,,,,If it could be trusted Id have plenty of use cases When it fails obviously its fine you can reject the output Its the subtle fails that cause the problems,miltla9,4.0,1jellhn
comment,mijnpd0,Ok_Transition9957,,42,2025-03-19 01:35:32,,,,I just want video games,,0.0,1jellhn
comment,mijpk5z,themikker,,-44,2025-03-19 01:46:15,,,,A big barrier for modern AI in video games is that they rely on the slow online models Been theorycrafting a few ways AI could be used for story crafting and game mastering but to run stuff like that offline at the same time as the game mind requires more oomph than most PC gamers have If offline support like this becomes more popular it could have a significant impact if used correctly of course,mijnpd0,1.0,1jellhn
comment,mikf31a,renaissance_man__,,19,2025-03-19 04:34:34,,,,Practically no game uses neural nets for AI They all use state machines behavior trees,mijpk5z,2.0,1jellhn
comment,mil66t5,themikker,,-12,2025-03-19 09:07:31,,,,Obviously Thats why I said modern AI Having a game able to use a LLM to produce on the fly scenarios for generative storylines is very different from how AI is implemented in most games Even how opponents plan in strategy games could have an LLM but Im thinking more in terms of generate random quests for an RPG or something There would be some value in that if it could be done live and local That wouldnt apply to most games ofc,mikf31a,3.0,1jellhn
comment,mimmhzh,renaissance_man__,,4,2025-03-19 14:57:17,,,,An llm outputting mountains of infinite slop vs handcrafted quests and you choose the llm,mil66t5,4.0,1jellhn
comment,mimqjf9,Bluedot55,,1,2025-03-19 15:17:33,,,,Theres some interesting use cases for this already take a look at the llm Skyrim mods One of them is called CHIM afaik It allows for some interesting stuff like you could walk up to a random NPC and ask them whats in the cave near the village and they might tell you Then ask if they want to help clear out the cave but then they reply that they want something for it so you have to barter with them for a price or an item,mimmhzh,5.0,1jellhn
comment,mimo18x,themikker,,-5,2025-03-19 15:04:59,,,,I dont understand the downvotes Im not saying stop designing games and just make chatGPT generate it its about how to leverage AI in a way that supports game development A lot of games use random content generation and not just for Roguelikes that are build via an algorithm Skyrim and Left 4 Dead are other examples I imagine that a lot of the content would still be hand crafted in those scenarios It would be a minor segment of a larger whole Getting it to not spit out slop is a huge challenge which was why I also added if used correctly Not even going into the mess of generating art assets with it either You would have to do a lot of manipulations and context for prompting before it could be useful It wont work at all out of the box it would require a lot of fine tuning or a lot of prework depending on what Nvidia provides in terms of software,mimmhzh,5.0,1jellhn
comment,milh1lb,Emadec,,3,2025-03-19 10:31:39,,,,AI would have to stop hallucinating things that dont exist first,mil66t5,4.0,1jellhn
comment,mimqc1t,Bluedot55,,1,2025-03-19 15:16:31,,,,Theres some interesting use cases for this already take a look at the llm Skyrim mods One of them is called CHIM afaik It allows for some interesting stuff like you could walk up to a random NPC and ask them whats in the cave near the village and they might tell you Then ask if they want to help clear out the cave but then they reply that they want something for it so you have to barter with them for a price or an item It can get far more complex than anything that could be manually done although as you said it can occasionally mess up But I think games are one of the situations where if its right 90 of the time or more we can work with that,milh1lb,5.0,1jellhn
comment,mik7nlx,TanmanG,,4,2025-03-19 03:38:33,,,,From my boots on the ground market research most developers just use FSMs or behavior trees,mijpk5z,2.0,1jellhn
comment,mik9b9j,Adrian-The-Great,,4,2025-03-19 03:50:17,,,,I am utterly confused about the direction on nvidia over the next couple of years Its like they have outgrown graphics cards and now everything is focused on ai ai developments ai project management and now desktop pcs,,0.0,1jellhn
comment,mikcbn5,agitatedprisoner,,13,2025-03-19 04:12:47,,,,Sounds like youve got it Nvidia is planning to provide the compute for the dawning age of AI and AI robots,mik9b9j,1.0,1jellhn
comment,mikrlm7,Dragonasaur,,0,2025-03-19 06:30:03,,,,AI as the current fad but quantum computing as the next fad to analyzecompute data extrapolated thanks to AI,mik9b9j,1.0,1jellhn
comment,mijn326,Books_for_Steven,,55,2025-03-19 01:31:57,,,,I was looking for a way to accelerate the collection of my personal data,,0.0,1jellhn
comment,mik01dh,None,,28,2025-03-19 02:48:08,,,,if its local and offline how can they collect your data,mijn326,1.0,1jellhn
comment,mijsjvz,screamtracker,,3,2025-03-19 02:03:16,,,,cant they harvest any faster Huge disappointment so far,mijn326,1.0,1jellhn
comment,mil99mf,Semen_K,,7,2025-03-19 09:39:56,,,,oh and as a bonus they will become obsolete only in 2 years not three,,0.0,1jellhn
comment,mik4nh7,Deepwebexplorer,,4,2025-03-19 03:18:02,,,,IFI could trust it to manage my data and security locallyIF it would be incredible But Im not sure what is going to make me want to trust it fully Maybe AGI happens when it can convince us to trust it,,0.0,1jellhn
comment,milgwug,Emadec,,4,2025-03-19 10:30:28,,,,Now why would I want a random hallucinated sentence generator at home that cant count fingers,,0.0,1jellhn
comment,mimzphp,Pantim,,4,2025-03-19 16:02:37,,,,Can they please stop cranking out more AI devices and focus on the hallucinating problem Yet another study just came out showing that they are typically wrong 60 of the time Which mind you is the case for general internet searches anyway But stillAI needs to be held to a higher standard,,0.0,1jellhn
comment,miotoz5,IFunkymonkey,,1,2025-03-19 21:23:55,,,,Can you please link one of those studies,mimzphp,1.0,1jellhn
comment,mijqjls,_dactor_,,7,2025-03-19 01:51:49,,,,Hard pass lol,,0.0,1jellhn
comment,mijqrbm,ArseBurner,,2,2025-03-19 01:53:04,,,,Reminds me of the old SGI workstations Cool stuff,,0.0,1jellhn
comment,milxh99,alidan,,2,2025-03-19 12:36:54,,,,show me the good use case for this and I may be ok with it I want ai to type what I say I want local queries of things not sending it off the the cloud to do what can be done with no ai and 5gb of ram,,0.0,1jellhn
comment,mj14owo,NihilisticAngst,,1,2025-03-21 19:55:12,,,,You can get the AI models to make arbitrary decisions with workflow automation software like n8n You can set this up right now locally and have the AI make workflow decisions based on your programmed inputs or other triggers The benefit of this is that you can potentially avoid needing to hardcode a rigid decision tree and allow the AI to have some agency in making decisions for you This does have to be tweaked and refined to get consistent output but consistent output is achievable even with lightweight local models depending on the complexity of the decision you are wanting the LLM to make And thus at least as far as the use case Im suggesting the only benefit I could really see this AI desktop computer bringing is a more highly performant environment that can potentially do more powerful things with AI than current consumer desktops can The thing is though is that even with a moderately powerful GPU you can already run successful models that are able to run local queries and such Im not exactly sure what type of processes you might need a more powerful AI desktop PC to do that a typical system that exists today cant Maybe advanced analysis of large local datasets,milxh99,1.0,1jellhn
comment,milzcos,Dorraemon,,2,2025-03-19 12:48:50,,,,Who asked for this,,0.0,1jellhn
comment,mimf9gu,DLiltsadwj,,2,2025-03-19 14:19:54,,,,Whats the advantage of running it locally,,0.0,1jellhn
comment,mj154kv,NihilisticAngst,,1,2025-03-21 19:57:19,,,,Security of your data not being trusted with a thirdparty cloudbased software Also continued ability to use your LLM models even during an ongoing Internet outage you could run your system off a electric generator and still be able to use it,mimf9gu,1.0,1jellhn
comment,min3kcw,Macqt,,2,2025-03-19 16:21:26,,,,Hard pass,,0.0,1jellhn
comment,mikming,Leetter,,1,2025-03-19 05:39:44,,,,These desktop systems first previewed as Project DIGITS in January aim to bring AI capabilities to developers researchers and data scientists who need to prototype finetune and run large AI models locally,,0.0,1jellhn
comment,milvk9g,RiderLibertas,,1,2025-03-19 12:24:14,,,,Looks like Ill be continuing to build my own super computers for the forseeable future,,0.0,1jellhn
comment,mim7o2f,T1mely_P1neapple,,1,2025-03-19 13:38:22,,,,conversational search only 15k,,0.0,1jellhn
comment,mimc91p,cmoz226,,1,2025-03-19 14:04:01,,,,I will pay anything for a chatbot Sign me up,,0.0,1jellhn
comment,mipkxb1,drdailey,,1,2025-03-19 23:47:04,,,,My bet is 30k,,0.0,1jellhn
comment,mir4wsp,BrokkelPiloot,,1,2025-03-20 06:03:41,,,,No thanks I dont need crappy AI,,0.0,1jellhn
comment,mis3h76,giomancr,,1,2025-03-20 11:47:41,,,,Am I the only one here who just wants a gaming pc or a lifelike sex robot and nothing in between,,0.0,1jellhn
comment,mikj8hf,Itsatinyplanet,,1,2025-03-19 05:09:24,,,,Beware of sweatyfivehead zuckerberg lizards offering AI models that run locally,,0.0,1jellhn
comment,milylh3,dch528,,-1,2025-03-19 12:44:05,,,,I dont understand the joke You can already run lots of models locally and with consumer hardware At no extra cost From Zuckerberg too,mikj8hf,1.0,1jellhn
comment,mijt612,ohiocodernumerouno,,1,2025-03-19 02:06:51,,,,not enough vram,,0.0,1jellhn
comment,mijy5xh,Johnson_N_B,,1,2025-03-19 02:36:40,,,,What does it all mean Basil,,0.0,1jellhn
comment,mim8yyw,Classic_Cream_4792,,1,2025-03-19 13:45:41,,,,Last ditch effort to sell ai literally using the personal computer which is the oldest of technology and overly mature in the marketplace Is that a tower and not laptop too Wow They are praying for stock price to go up it seems,,0.0,1jellhn
comment,mimyikg,xRockTripodx,,1,2025-03-19 15:56:50,,,,I dont fucking want AI locally or anywhere else for that matter All it does is replace human intelligence ingenuity and jobs with a fucking algorithm,,0.0,1jellhn
comment,mikjrzq,The_Pandalorian,,-1,2025-03-19 05:14:14,,,,Awesome I cant wait to not buy this piece of shit that nobody will want,,0.0,1jellhn
comment,mijrynw,Hoggel123,,0,2025-03-19 01:59:54,,,,I dont know if I want local ai yet,,0.0,1jellhn
comment,mijyfg5,NotAPreppie,,0,2025-03-19 02:38:16,,,,I already do this on my Mac and gaming PC,,0.0,1jellhn
comment,mikx530,powerexcess,,0,2025-03-19 07:28:26,,,,Are these good for training models,,0.0,1jellhn
comment,mil5b6t,ChowAreUs,,0,2025-03-19 08:58:09,,,,Im actually excited for these but not the fucking prices,,0.0,1jellhn
comment,mikkzx9,Rfksemperfi,,-4,2025-03-19 05:25:21,,,,My m1 Mac has done this for quite a while Why is this news I was waaay off there is a massive difference While the M1 Mac can effectively run pretrained models like LLaMA and Mistral for local tasks such as inference chatting and summarizing it is not suitable for fullscale model training or finetuning These tasks require significant resources including hundreds of GBs of VRAM and highend hardware like NVIDIA DGX desktops which can cost over 10K Most personal computers including the M1 Mac are not designed for such demanding processes The distinction is between users who run smaller models locally which is practical and efficient and researchers who need to build or refine large models requiring professionalgrade equipment,,0.0,1jellhn
comment,milzqgx,Elios000,,2,2025-03-19 12:51:12,,,,M1 Mac cant run the whole model locally this can this isnt about running the final AI code this for TRAINING the AI in the first place These desktop systems first previewed as Project DIGITS in January aim to bring AI capabilities to developers researchers and data scientists who need to prototype finetune and run large AI models locally,mikkzx9,1.0,1jellhn
comment,miju5vf,Left_on_Pause,,-1,2025-03-19 02:12:45,,,,Ill pick one up from Craigslist,,0.0,1jellhn
comment,milto2t,smulfragPL,,-1,2025-03-19 12:11:28,,,,people on here know so very little about ai lol,,0.0,1jellhn
comment,mijoinj,brickyardjimmy,,-9,2025-03-19 01:40:18,,,,Now this is interesting,,0.0,1jellhn
comment,mijyqhg,Johnson_N_B,,4,2025-03-19 02:40:07,,,,Now this is pod racing,mijoinj,1.0,1jellhn
comment,mik039i,has_left_the_gam3,,2,2025-03-19 02:48:28,,,,gifgiphy3owzWgnMr5vS37fBsc,mijyqhg,2.0,1jellhn
