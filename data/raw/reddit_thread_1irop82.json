{
  "post": {
    "title": "AMD's Frank Azor says no 32 GB RX 9070 XT for you, probably because a 32 GB mid-range GPU didn't make much sense in the first place",
    "author": "a_Ninja_b0y",
    "id": "1irop82",
    "score": 595,
    "created_utc": 1739811830.0,
    "selftext": "",
    "num_comments": 114,
    "subreddit": "gadgets",
    "url": "https://www.reddit.com/r/gadgets/comments/1irop82/amds_frank_azor_says_no_32_gb_rx_9070_xt_for_you/"
  },
  "comments": [
    {
      "id": "mdawaje",
      "author": "None",
      "body": "TFW $700 is \"midrange\"\n\n![gif](giphy|qZgHBlenHa1zKqy6Zn)",
      "score": 394,
      "created_utc": 1739821560.0,
      "replies": [
        {
          "id": "mdbvpc7",
          "author": "Fake_Disciple",
          "body": "I remember as a kid I use to watch a lot channels where they build pcs back then $700+ pc with everything was considered high end now that\u2019s just the price of a shitty GPU",
          "score": 72,
          "created_utc": 1739831575.0,
          "replies": [
            {
              "id": "mdd7vub",
              "author": "evonebo",
              "body": "I think either you are not accounting for inflation or you're not remembering correctly.",
              "score": 6,
              "created_utc": 1739847556.0,
              "replies": [
                {
                  "id": "mdfhtfq",
                  "author": "GhostDan",
                  "body": "In 1995, my first non-used computer cost me about $1200. It would be considered mid-range at the time.\nIn 2000, my 2nd computer cost me about $500 and it had a Voodoo graphics card, it was solidly mid-range if not higher with that card. \n\nBefore the early 2000s markups were so high your local neighborhood kids were setting up computer 'manufacturing' companies, buying up the parts at OEM and putting them together and getting 2-3x profits sometimes.",
                  "score": 8,
                  "created_utc": 1739887423.0,
                  "replies": []
                },
                {
                  "id": "mdd90z0",
                  "author": "NorthEagle298",
                  "body": "Let me take you to a time... before LED fans... before tempered glass cases... before SSDs... and... the system only had to run the Quake Arena demo and Diablo 1. Oh hell yeah we were building $500 PCs in the late 90s which is still <$1000 adjusted for inflation.",
                  "score": 40,
                  "created_utc": 1739847966.0,
                  "replies": [
                    {
                      "id": "mddbgjl",
                      "author": "evonebo",
                      "body": "I honestly don't remember pcs being $500.  Like emachines maybe later when they came out.\n\nI was building them at a shop in silicon valley in the 90s\n\n\nhttps://www.latimes.com/archives/la-xpm-1993-12-23-fi-4940-story.html",
                      "score": 3,
                      "created_utc": 1739848846.0,
                      "replies": [
                        {
                          "id": "mddd3zm",
                          "author": "NorthEagle298",
                          "body": "Prices really started coming down in the late 90s/early 00's.",
                          "score": 12,
                          "created_utc": 1739849448.0,
                          "replies": []
                        }
                      ]
                    },
                    {
                      "id": "mdk2db9",
                      "author": "colin_colout",
                      "body": "This reads like a confidently incorrect LLM hallucination from a 1 year old account (Mr AdjNounNumber).",
                      "score": -1,
                      "created_utc": 1739937180.0,
                      "replies": []
                    }
                  ]
                },
                {
                  "id": "mdes19m",
                  "author": "Fake_Disciple",
                  "body": "This was 10 to 15 years ago not 30",
                  "score": 6,
                  "created_utc": 1739876028.0,
                  "replies": []
                },
                {
                  "id": "mdg9kvx",
                  "author": "MercenaryOne",
                  "body": "He's definitely not remembering correctly, my first PC was nearly $3000 in components, considered high end, and that was in 93/94. It was before Win95 because I remember waiting in line at CompUSA for a copy. P1 75mhz, Sound Blaster 16, 8MB EDO RAM, 1GB HDD, and a 4MB Hercules GFX card. When I worked retail in 2002, I was astonished we were selling basic PC's that ran circles around my first built PC for nearly $500.",
                  "score": 2,
                  "created_utc": 1739896064.0,
                  "replies": [
                    {
                      "id": "mdglfrj",
                      "author": "evonebo",
                      "body": "Lol I dunno if you remember, but back then it was a rough choice. \n\nDo you put cdrom,  modem, or sound card in the build.\n\nCan't have all three cause on a budget.\n\nI remember going to frys electronics (central) before frys every chance I had to look at components.",
                      "score": 1,
                      "created_utc": 1739899407.0,
                      "replies": []
                    }
                  ]
                }
              ]
            },
            {
              "id": "mdfio1v",
              "author": "imnotokayandthatso-k",
              "body": "Back then a dozen eggs was 3$",
              "score": 1,
              "created_utc": 1739887725.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mdb0uhb",
          "author": "tartare4562",
          "body": "> the face when when",
          "score": 36,
          "created_utc": 1739822825.0,
          "replies": [
            {
              "id": "mdbd8tv",
              "author": "None",
              "body": "goddammit autocorrect",
              "score": 8,
              "created_utc": 1739826300.0,
              "replies": []
            },
            {
              "id": "mdcjriz",
              "author": "tubular1845",
              "body": "That feeling when",
              "score": 4,
              "created_utc": 1739839304.0,
              "replies": []
            },
            {
              "id": "mdeco77",
              "author": "chunckybydesign",
              "body": "I know right! Like when when, and then when!",
              "score": 2,
              "created_utc": 1739866652.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mdbf9sc",
          "author": "Eve_newbie",
          "body": "Is it being listed for that? I thought MSRP was close to 5-550",
          "score": 8,
          "created_utc": 1739826868.0,
          "replies": [
            {
              "id": "mdbu7ad",
              "author": "MiloIsTheBest",
              "body": "There are no official numbers.\n\nThere have been some listings randomly appear for between $700 and like $900, but they are not official yet and might be placeholders.\n\nIf the cards are $500 or $600 and MSRP stays to conversion in Australia (unlike NVIDIA's padded rort MSRP which wasn't even met) they might actually come in at below $1000AUD which would be nice, being that the 7900XT was about $1000-$1100AUD and this is a **70** that performs similarly, hopefully with better RT (but no indication it will quite match nvidia), 2 years later.\n\nBut I bet they don't want to do that. I bet they want to take the piss on pricing.",
              "score": 8,
              "created_utc": 1739831131.0,
              "replies": [
                {
                  "id": "mdbwkpd",
                  "author": "Eve_newbie",
                  "body": "Plus with the tariff that keep being talked about it makes it really hard to give a MSRP if there's suddenly a 10-50 price increase to get it into America",
                  "score": 1,
                  "created_utc": 1739831838.0,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "mdb3a14",
          "author": "Hattix",
          "body": "Just correcting for inflation... and in GBP, because that's what I paid...\n\nThe \u00a3250 of a Radeon 9700 Pro, king of the hill, top of the shop, undisputed framerate champion would be \u00a3454 today. It'd buy me a basic level 7800XT.",
          "score": 24,
          "created_utc": 1739823513.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mda1kt5",
      "author": "CMDR_omnicognate",
      "body": "The speculation was I think that it would be used for ai stuff rather than for gaming, since most ai tasks are incredibly vram heavy",
      "score": 111,
      "created_utc": 1739813177.0,
      "replies": [
        {
          "id": "mdae1kv",
          "author": "Hattix",
          "body": "It probably will be, maybe a Radeon Pro WX9070 or whatever.\n\nConsumer gaming cards don't need 32 GB at this level of performance.",
          "score": 42,
          "created_utc": 1739816585.0,
          "replies": [
            {
              "id": "mdcwryo",
              "author": "Leafy0",
              "body": "As what level of performance? They\u2019re not out yet. With how lame the generational uplift was with the 50 series I wouldn\u2019t be surprised if the card and was targeting to go against the 5070 ends up closer to the 5080 on non-rtx workloads.",
              "score": 2,
              "created_utc": 1739843646.0,
              "replies": []
            },
            {
              "id": "mdehczs",
              "author": "alidan",
              "body": "honestly, textures do far more for improving graphics than anything else, lighting comes in second just because the only methods we get are ray tracing which is garbage but easier to implement, or incompetent people trying to optimize older or newer methods for inserting lighting.",
              "score": 2,
              "created_utc": 1739869529.0,
              "replies": []
            },
            {
              "id": "mdehjq6",
              "author": "i_mormon_stuff",
              "body": "Problem is the price of a WX9070 will likely be too high compared to the performance it can offer within a 32GB VRAM budget.\n\nWould people who do AI prefer a 5090 32GB for $2,000-$2,500. Or a much slower WX9070 32GB for the same price?\n\nWhat AI people want in reality is a cheap card with a lot of VRAM and they're willing to give up core performance to get the VRAM. So a 9070 32GB at <$1,000 would sell like hot cakes to that crowd. Something costing twice as much, well then the 5090 is a better buy as it's no doubt going to be faster.",
              "score": 2,
              "created_utc": 1739869647.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mdabupy",
          "author": "daekle",
          "body": "That was my thought. Having a large vram is great for certain models, even if they run a little slower. Certain models like FLUX (a stable diffusion based image generator) just wont run on my puny little 6GB card at any speed. Need more ram.",
          "score": 13,
          "created_utc": 1739815992.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mdbbpmu",
      "author": "fliberdygibits",
      "body": "AI has made me think GPU manufactures are allergic to adding more VRAM.",
      "score": 76,
      "created_utc": 1739825876.0,
      "replies": [
        {
          "id": "mdcepbm",
          "author": "mumbler1",
          "body": "I can see them slapping an \"AI\" edition where it's literally the same card but more VRAM and upcharge for the extra \ud83d\ude44",
          "score": 30,
          "created_utc": 1739837636.0,
          "replies": [
            {
              "id": "mdciy5w",
              "author": "irregularjosh",
              "body": "As someone that's wanting to do a bit of AI at home, they might as well be saying \"No AI for you, peasant\"",
              "score": 10,
              "created_utc": 1739839036.0,
              "replies": [
                {
                  "id": "mdcli3e",
                  "author": "fliberdygibits",
                  "body": "Yep.  Give me a GPU with a bunch of RT cores and Vram.  Skip all the video encode/decode, skip the texture pipelines, etc....  But apparently I'm stuck with 12-16gb of vram unless I want to give up a limb no matter HOW many new GPU generations we get.",
                  "score": 4,
                  "created_utc": 1739839875.0,
                  "replies": [
                    {
                      "id": "mdehmdd",
                      "author": "alidan",
                      "body": "aren't you able to pool gpus into a single gpu and effectively have a large vram pool natively in windows 10+? whatever downside you would have in a game wouldnt be too much of a hurdle in a more prosumer/enterprise application, though throughput between gpus may be an issue outside of pcie 5",
                      "score": 1,
                      "created_utc": 1739869692.0,
                      "replies": [
                        {
                          "id": "mdeo8bm",
                          "author": "akeean",
                          "body": "nope. VRAM needs to be physically close to the compute cores in most of the common use cases. Going through PCIe to grab something from a differeot GPUs VRAM would kill performance. Even SLI where each GPU takes turns rendering frames, or renders only part of the same frame is pretty much dead.\n\nJoing GPUs nowadays is only server grade stuff for different workloads than gaming & even then one monolithic card that puts everything on the same silicon would be faster. It's just that you physically  can't put all of that in one place & still cool it while it's working.",
                          "score": 4,
                          "created_utc": 1739873742.0,
                          "replies": [
                            {
                              "id": "mdep549",
                              "author": "alidan",
                              "body": "but the thing is, a lot of the ai training stuff is at least to my understanding compartmentalized, kind of like how raytracing is (it effectively scaled linearly with more gpus tosses at the problem), \n\nand the throughput issue was there in prior generations of pcie due to other bottlenecks, its why nvidia had nvlink, however that same bottleneck I don't believe is there anymore in pcie5, don't get me wrong it's still latency and a performance hit, but it should scale more performance for multiple gpus, at least far more than an alternative solution if you really needed the extra space.",
                              "score": 1,
                              "created_utc": 1739874301.0,
                              "replies": [
                                {
                                  "id": "mdg1doa",
                                  "author": "akeean",
                                  "body": "Yeah you can work around some issues with more bandwidth and it certainly works for a lot of things (otherwise H100 clusters for LLM wouldn't), but with memory interlink you always run into physical limits on how fast electrons transmit information that will cause your cores to run dry, or require massive caches (cache is kind of big on a die and doesn't miniaturize so well with smaller architectures) that require a lot of overhead logic to be filled, tested and still be discarded if cache misses. And then you also get interference and signal integrity issues. Every centimeter counts.\n\nAfaik for raytracing the BVH is a central bottleneck that each GPU in your cluster needs to somehow handle.",
                                  "score": 2,
                                  "created_utc": 1739893686.0,
                                  "replies": [
                                    {
                                      "id": "mdldy48",
                                      "author": "alidan",
                                      "body": "i'm thinking of it more along the lines of any single ray bounce gives data that every other ray bounce can take advantage of in compositing a scene and doesn't need to necessarily be interacted with to do it. \n\nunless im mistaken in the way ai works, it essentially process a data set against other datasets at least my thinking is you could split the data set up in ways that allow the chunk on gpu to be processed, then swapped out and put up against another data set without needing to constantly cross reference with the data across 2 gpus, that way the data is loaded in in a large chunk and worked with if the data had to constantly go between gpus to get processed that... kind of feels like a failure on people who are designing the processing method.",
                                      "score": 1,
                                      "created_utc": 1739961939.0,
                                      "replies": []
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        },
                        {
                          "id": "mdh9wfe",
                          "author": "fliberdygibits",
                          "body": "You could in the past with NVLink but Nvidia got rid of it.",
                          "score": 1,
                          "created_utc": 1739906158.0,
                          "replies": [
                            {
                              "id": "mdlexwg",
                              "author": "alidan",
                              "body": "no, windows 10 brought about the ability to pool gpus like this through pcie only, sli always required the memory to be mirrored, I don't know off other systems/custom solutions, but even professional crap that required gpu acceleration still mirrored, win 10 at its base feature set had the ability to pool both amd and nvidia cards to use them at the same time (granted this was shown off for games) and pool the memory for a larger dataset to be loaded onto it.\n\nthe problem was pcie 3 and 4 bandwith was not enough to fully utilize that, however 5 kind of does have enough, nvlink was just a proprietary way to have a high speed connection that's not based on the pcie interface.",
                              "score": 1,
                              "created_utc": 1739962505.0,
                              "replies": []
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "mdbw7n7",
          "author": "Macabre215",
          "body": "That's a big reason why",
          "score": 11,
          "created_utc": 1739831728.0,
          "replies": []
        },
        {
          "id": "mdceped",
          "author": "arabidkoala",
          "body": "I wonder if we\u2019re ever going to start seeing RAM sockets in them.  Seems like it could be a more cost-effective way of making GPUs without having to predict which use cases will require what configurations when making decisions about what fixed designs to manufacture.  Then again, waste and scarcity are critical to high prices, so I guess I can see why this would never happen.",
          "score": 3,
          "created_utc": 1739837637.0,
          "replies": [
            {
              "id": "mdehtvv",
              "author": "alidan",
              "body": "can't, I have asked this before and effectively the only reason the ram in a gpu can run so fast is its proximity to the die, take a look at laptops with soldered and socked ram, the extra length of wire in causes a 40~% throughput decrease, dell has a solution for it but I don't think anyone adopted the new standard yet.",
              "score": 1,
              "created_utc": 1739869819.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mddsle7",
          "author": "chizburger999",
          "body": "Are AMD GPUs even good at AI related tasks?",
          "score": 1,
          "created_utc": 1739855853.0,
          "replies": [
            {
              "id": "mddynqt",
              "author": "-Badger3-",
              "body": "They\u2019re generally not as good as Nvidia cards, but they\u2019re viable if you\u2019re dicking around with homelab stuff.",
              "score": 4,
              "created_utc": 1739858786.0,
              "replies": []
            },
            {
              "id": "mddyf7s",
              "author": "fliberdygibits",
              "body": "My 7800xt does pretty good.  I was more referring to GPU makers in general.",
              "score": 1,
              "created_utc": 1739858666.0,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "mdefkrk",
      "author": "systemBuilder22",
      "body": "Cistomers will go to the store with $1000 saved and will tell the retailer, \"I'd like a 5080 please!\" and the retailer will say, \n\nWE ARE ALL SOLD OUT AND THERE IS A 4 MONTH WAITING LIST BUT I HAVE THESE FANCY LOOKING 5070TI MODELS ON SALE FOR $950 WHATDOYA THINK? \". And Jensen gets a new jacket ..",
      "score": 7,
      "created_utc": 1739868430.0,
      "replies": []
    },
    {
      "id": "mda0m59",
      "author": "Eokokok",
      "body": "And Reddit kids wept...",
      "score": 13,
      "created_utc": 1739812911.0,
      "replies": []
    },
    {
      "id": "mda774a",
      "author": "cagriuluc",
      "body": "In just a couple more years, 32 GB vram will be able to get you so much ai\u2026 I am not sure whether it is a good move for now, though\u2026",
      "score": 3,
      "created_utc": 1739814722.0,
      "replies": [
        {
          "id": "mdbm3qw",
          "author": "paradoxbound",
          "body": "Possibly in future you could see 32GB or Even 64GB cards. With compute being split between traditional graphics and NPUs. There will be some interesting games if that happens.",
          "score": 1,
          "created_utc": 1739828789.0,
          "replies": [
            {
              "id": "mdnm5wm",
              "author": "Kajega",
              "body": "We have 32GB cards as of last month",
              "score": 2,
              "created_utc": 1739988902.0,
              "replies": [
                {
                  "id": "mdnucr9",
                  "author": "paradoxbound",
                  "body": "Sorry out of the loop a little with graphics cards. I simply buy the best every 5 years or so. I am even further behind this cycle as I spend money on fixing up an old RV with Starlink and bunch of solar, batteries and networking. My card is an ancient 1080. Won\u2019t make much difference to my next build which is a AMD 9800x3D paired with a 5090 in a 10 litre case. Perfect for the RV that I am full timing in for much of the year.\n\nA 32GB card sounds cool though I could pop it in a box in the house and access it remotely for au art and LLLMs. I would like to see benchmarks again Nvidia and price.",
                  "score": 1,
                  "created_utc": 1739991092.0,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "mda7ufy",
      "author": "Diciestaking",
      "body": "What is even the point in 32gb in the first place? I can't think of almost any game that would get close to that. If you are using it for design reasons, then that's another story.",
      "score": 9,
      "created_utc": 1739814898.0,
      "replies": [
        {
          "id": "mda9rc3",
          "author": "YourNightmar31",
          "body": "AMD would make stacks selling these to AI enthusiasts and hobbyists.",
          "score": 58,
          "created_utc": 1739815421.0,
          "replies": [
            {
              "id": "mdab621",
              "author": "law_dweeb",
              "body": "Doesn't the AI software need CUDA?",
              "score": 11,
              "created_utc": 1739815806.0,
              "replies": [
                {
                  "id": "mdabxoz",
                  "author": "echae",
                  "body": "Fair question! AMD has a CUDA alternative called ROCm that is fully open-source.",
                  "score": 25,
                  "created_utc": 1739816015.0,
                  "replies": [
                    {
                      "id": "mdavn5r",
                      "author": "QuickQuirk",
                      "body": "It's a bit of a pain to get running, but I got the lowly 780m running it with 16GB dedicated vram, on a large LLM. was funny seeing it actually work, albiet somewhat slow.",
                      "score": 16,
                      "created_utc": 1739821380.0,
                      "replies": [
                        {
                          "id": "mdb31sm",
                          "author": "MmmmMorphine",
                          "body": "Been hearing different opinions on that, especially with the worst case scenario of mixing nvidia and amd cards.\n\nSupposedly llama.cpp and many engines can handle it relatively natively without much set up. Though I personally don't know.\n\nThink you're right the performance is worse though, not a huge amount but certainly not in line with what the hardware should be able to do",
                          "score": 3,
                          "created_utc": 1739823449.0,
                          "replies": [
                            {
                              "id": "mdbmmfr",
                              "author": "QuickQuirk",
                              "body": "In this case the performance was worse because it was the integrated 780m GPU on the miniPC I'm running as a server.\n\nIt was actually quite impressive given that it was a miniPC running a 13GB model.\n\nBut just getting ROCm to even recognise the GPU took some command line magic I found somewhere. ROPCm seems to work on most of the AMD GPUs, but they only officially support a subset.\n\nIt's definitely not as easy and simple as the nVidia setup (and even that is a bit clunky)",
                              "score": 1,
                              "created_utc": 1739828939.0,
                              "replies": [
                                {
                                  "id": "mdcc4mj",
                                  "author": "Sporebattyl",
                                  "body": "Can you point me to a guide how to do this? I have a mini with a 780m and I\u2019ve been looking to get into local LLMs. \n\nHow slow is it?",
                                  "score": 1,
                                  "created_utc": 1739836791.0,
                                  "replies": []
                                },
                                {
                                  "id": "mdfg6j7",
                                  "author": "danielv123",
                                  "body": "Is it significantly faster to run it on the 780m vs just the CPU directly?",
                                  "score": 1,
                                  "created_utc": 1739886843.0,
                                  "replies": [
                                    {
                                      "id": "mdhbzwq",
                                      "author": "QuickQuirk",
                                      "body": "It depended on the task. The difference wasn't orders of magnitude, but it was improved. I can't remember by how much. I was mostly just doing it to play, rather than for serious work. Should have written down my results.\n\nIt's a weak GPU, but it still represents a collosal amount of compute compared to the CPU.",
                                      "score": 1,
                                      "created_utc": 1739906736.0,
                                      "replies": [
                                        {
                                          "id": "mdhxphd",
                                          "author": "danielv123",
                                          "body": "Yeah, but usually LLMs are memory bandwidth bound, and I assumed that as long as you use system memory it's pretty similar in bandwidth",
                                          "score": 1,
                                          "created_utc": 1739912677.0,
                                          "replies": []
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": "mdbugda",
                      "author": "nooneisback",
                      "body": "Which just boils down to the same issue as CUDA vs OpenCL. Just because something is open source, doesn't mean it will be more popular. CUDA can be set up by a lobotomite. Go to hugging face, pick a model, feed it content you definitely own, and voila. We still don't even have PyTorch for ROCm on Windows, let alone more than a handful of models that support it. CUDA is so simple that I know physics professors that use it on a daily basis, but can't share their screen on Zoom meetings.",
                      "score": 4,
                      "created_utc": 1739831205.0,
                      "replies": []
                    }
                  ]
                },
                {
                  "id": "mdabuc3",
                  "author": "YouDoNotKnowMeSir",
                  "body": "CUDA is the most supported in many applications for AI, CAD, etc. \n\nWe are seeing more support for AMD and there are some open source libraries to help with this compatibility as well. \n\nThat being said, it isn\u2019t perfect and probably won\u2019t be for another couple years. But we are moving in the right direction.",
                  "score": 12,
                  "created_utc": 1739815989.0,
                  "replies": [
                    {
                      "id": "mdfm6vb",
                      "author": "tastyratz",
                      "body": "I know Rocm and zluda exists but I think you're right. I have yet to actually get metastable running on a 5070 or 6080. Alternatives all fall apart for me too.",
                      "score": 2,
                      "created_utc": 1739888927.0,
                      "replies": [
                        {
                          "id": "mdhhgic",
                          "author": "YouDoNotKnowMeSir",
                          "body": "Yeah\u2026 but hey. It\u2019s good that there\u2019s finally competition and demand for it. Things will change somewhat quick.",
                          "score": 1,
                          "created_utc": 1739908238.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": "mdcgwjf",
                  "author": "Blue-Thunder",
                  "body": "It would create great incentive to turn that around.",
                  "score": 2,
                  "created_utc": 1739838361.0,
                  "replies": []
                }
              ]
            },
            {
              "id": "mdabej3",
              "author": "kazuviking",
              "body": "Only deepseek would run good as anything else is optimized for CUDA.",
              "score": -6,
              "created_utc": 1739815870.0,
              "replies": []
            },
            {
              "id": "mde3uvg",
              "author": "anonymousbopper767",
              "body": "Nah.  Same shit 10 years ago when every kid was suddenly a twitch tuber pro and needed 32 core cpus. \n\nNo one is really doing anything that needs vram.  Just don\u2019t want to admit they see a bigger number and think it\u2019s better.  Now go back to the 8GB you only use 6GB from.",
              "score": 0,
              "created_utc": 1739861533.0,
              "replies": [
                {
                  "id": "mdedsha",
                  "author": "inagy",
                  "body": "If you play 4+ year old titles on a 1080p monitor, then what you say is true.\n\n32 core CPUs made sense for streaming because you can dedicate a set of cores for the game, and another set for the video encoding.",
                  "score": 1,
                  "created_utc": 1739867338.0,
                  "replies": [
                    {
                      "id": "mdegqzy",
                      "author": "bearybrown",
                      "body": "Most logical setup was offload video encoding to GPU using NVENC  or AMD equivalent.",
                      "score": 1,
                      "created_utc": 1739869152.0,
                      "replies": [
                        {
                          "id": "mdehbot",
                          "author": "inagy",
                          "body": "Afaik the general consensus back then (in the Pascal era) was that x264 fast was still better image quality wise than what NVENC could do. I think this changed with the new encoder introduced in Turing.",
                          "score": 1,
                          "created_utc": 1739869506.0,
                          "replies": [
                            {
                              "id": "mdei1c5",
                              "author": "bearybrown",
                              "body": "Yes that is correct but if you're start up streamer and i3,16-32gb ram with 1070 is more than enough for pre 2018-2020 games and still good enough for esports title.",
                              "score": 1,
                              "created_utc": 1739869949.0,
                              "replies": []
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "mdb323n",
          "author": "nicman24",
          "body": "ai tiddies. Yes you can and should judge me",
          "score": 6,
          "created_utc": 1739823451.0,
          "replies": []
        },
        {
          "id": "mdc1tqz",
          "author": "herroh7",
          "body": "Could probably find a way with Skyrim mods!",
          "score": 3,
          "created_utc": 1739833434.0,
          "replies": []
        },
        {
          "id": "mdad4sz",
          "author": "CrankoWanko",
          "body": "Also 3D rendering. It would still be a lot but it can be useful for certain things (lots of / big textures, certain simulations, etc).",
          "score": 1,
          "created_utc": 1739816338.0,
          "replies": []
        },
        {
          "id": "mdef97v",
          "author": "systemBuilder22",
          "body": "GTA V has a memory requirements predictor.  When i max everything out @4K with raytracing it says, \"25GB\".",
          "score": 1,
          "created_utc": 1739868234.0,
          "replies": []
        },
        {
          "id": "mdap0lu",
          "author": "Arctiiq",
          "body": "I haven\u2019t found a game that uses up all 23 gb in the xtx yet.",
          "score": 1,
          "created_utc": 1739819556.0,
          "replies": []
        },
        {
          "id": "mdanetn",
          "author": "Oober3",
          "body": "It doesn't make sense. It was just AMD subreddit copium, a nice escapade from their usual ''nvidia bad'', ''i definitely don't regret my AMD card guyz seriously'' into ''please tell me I made the right choice buying a 7900xtx'' posts that make up like 90% of the sub.",
          "score": -10,
          "created_utc": 1739819117.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mdef1bm",
      "author": "systemBuilder22",
      "body": "NVidia 5070 Ti is $900\nNvidia 5080 is $1200.\nI No longer think the 9070xt is $600.\nCharging 2/3rds of those prices for equal perf leaves money ON THE TABLE. \n\nI Now think best case the 9070xt is $700.\nNVidia price hikes and performance are just TOO DISAPPOINTING.  AMD only has to be A LITTLE DISAPPOINTING and they will clean up ...",
      "score": 2,
      "created_utc": 1739868103.0,
      "replies": [
        {
          "id": "mdeipev",
          "author": "alidan",
          "body": "it isnt even close to equal where it matters for nvidia, ai and their extras, nvidia nvenc, dlss, and other crap they offer adds more than enough value to the card to make up for its cost if amd is anywhere close to nvidia. \n\nif amd makes their own ai gpu specific upscaling, gg, amd is forever dead because now you get to choose between the shttier card or the better card, where as amd always had raw performance usually at better values than nvidia, if you remove that and play in the area that nvidia is kicking damn near everyone's ass in what value do you add other than making nvidia cards maybe a few dollars cheaper? \n\nand before anyone says they can get better, amd and nvidia have had video encode and decode for about as long as eachother and amd isnt even fucking close to the same quality, I have a 7900xt, I got it because it was a hell of a deal when I got it and I was on a 1060. amd cant even make a competitive video encoder/decoder, something that would make them a must have for streamers or people who want to try streaming, effectively a marketing segment that they could have had advertise their gpus, but no, they just can't. what would make you think that them chasing nvidia would do anything else then be nvidias lesser?",
          "score": 1,
          "created_utc": 1739870362.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mde6y5w",
      "author": "Den710nuggets",
      "body": "Is frank azor not a car accident attorney",
      "score": 1,
      "created_utc": 1739863258.0,
      "replies": []
    },
    {
      "id": "mdjr1gz",
      "author": "mrblaze1357",
      "body": "Everyone kept saying that they can't wait to see this on the shelves. But I'm just sitting here like if anything that's going to be a professional GPU, in marketed as like a W9070. You'd only see it in like OEM builds, similar to the Quadro series",
      "score": 1,
      "created_utc": 1739933107.0,
      "replies": []
    },
    {
      "id": "mdka076",
      "author": "oldmanjenkins51",
      "body": "I\u2019m tired",
      "score": 1,
      "created_utc": 1739940205.0,
      "replies": []
    },
    {
      "id": "mdbbrli",
      "author": "No_Camel7011",
      "body": "You don\u2019t say",
      "score": 1,
      "created_utc": 1739825891.0,
      "replies": []
    },
    {
      "id": "mda9gmp",
      "author": "MinimumArmadillo2394",
      "body": "It kinda doesnt. Unless youre doing heavy ML stuff, you arent going to even use 16GB in most games at max settings in 4k. \n\nIf you are doing ML stuff, youre likely buying whoever GPU has the most memory anyway, regardless of cost.\n\nEdit: you guys are completely missing the point. Most gamers play in 1080p. Most gamers only care about a reachable 60 fps. Most gpus can hit that. Anyone who wants 4k ultra settings with max ray tracing, VR, and DLSS are not buying a mid range gpu lmao.\n\nHop off my dick about \"what about this game?\" When talking about whether or not 16 GB of VRAM is enough. Your extreme outliars dont even matter when discussing most games.",
      "score": -7,
      "created_utc": 1739815338.0,
      "replies": [
        {
          "id": "mdak6mn",
          "author": "Fredasa",
          "body": "Yeah that's certainly true for \"most games.\"\n\nBut the important, red letter games tend also to be the ones that 1) have high VRAM demands by default, and 2) are a modder's paradise\u2014and the most popular mods for such games tend to be the ones that improve texture detail.\n\nWhen people express concerns over VRAM limits, they really aren't thinking about most games.  They're thinking about _those_ games.  The important ones.",
          "score": 11,
          "created_utc": 1739818240.0,
          "replies": [
            {
              "id": "mdalbbh",
              "author": "MinimumArmadillo2394",
              "body": "What games are \"important\" here?\n\nEven skyrim with a huge texture overhaul in 4k with 100 mods didnt cap over 12 GB of VRAM for me. Was one of the first things I tried.\n\nThe processing power was too low on my 1080 to run some of the visual overhauls but even my 3090 ran into processing bottlenecks with some of the mods. \n\nThe only things Ive seen use anywhere close to the 24GB my 3090 FTW3 has was an ML project I worked on for a few days for image generation.",
              "score": -8,
              "created_utc": 1739818546.0,
              "replies": [
                {
                  "id": "mdaven8",
                  "author": "Fredasa",
                  "body": "> What games are \"important\" here?\n\n_Cyberpunk 2077,_ ca. 2020, can almost saturate 16GB without a single damn mod installed.  While you can't yet point to another game of its Bethesda-like scope since its release, you sure won't be future-proofing your PC if your brand new GPU has only 16GB.",
                  "score": 10,
                  "created_utc": 1739821316.0,
                  "replies": [
                    {
                      "id": "mdax4f9",
                      "author": "MinimumArmadillo2394",
                      "body": "Can it? Mine ran less than 8GB on 1440p on highest settings with a 3090. \n\nI even the official [nvidia guide](https://www.reddit.com/r/nvidia/s/Yvi8zY2Y6e) for the big new update shows you only need 8 unless you put raytracing on its highest setting, which IMO added very little to the game.",
                      "score": -3,
                      "created_utc": 1739821791.0,
                      "replies": [
                        {
                          "id": "mdayxnb",
                          "author": "Fredasa",
                          "body": "> Can it?\n\n[Of course it can.](https://www.techpowerup.com/review/cyberpunk-2077-phantom-liberty-benchmark-test-performance-analysis/7.html#:%7E:text=Enabling%20DLSS%203%20Frame%20Generation,is%20without%20any%20upscaling%20though.)\n\n_\"Enabling DLSS 3 Frame Generation at 4K, on top of max settings with RT brings the VRAM usage to a stunning 18 GB, which means you better have a RTX 4090. This is without any upscaling though.\"_\n\nNot even some kind of unique scenario, either.  I personally don't settle for anything less than 4K60 and haven't since like 2019, and if I turned RT on I would definitely hit the cap.",
                          "score": 8,
                          "created_utc": 1739822292.0,
                          "replies": [
                            {
                              "id": "mdb4vxv",
                              "author": "Cry_Wolff",
                              "body": "How many people targeting 4K and RT will buy a mid range GPU?",
                              "score": 2,
                              "created_utc": 1739823969.0,
                              "replies": [
                                {
                                  "id": "mdb7ahz",
                                  "author": "None",
                                  "body": "[deleted]",
                                  "score": 0,
                                  "created_utc": 1739824651.0,
                                  "replies": [
                                    {
                                      "id": "mdbgh8x",
                                      "author": "MinimumArmadillo2394",
                                      "body": ">OP of this spinoff thread stipulated 4K unambiguously and that's what the conversation has been about.\n\nAnd its right in most cases which is what I said. Most games can run 4k at max settings. Your specific example of cyberpunk isnt \"most games\". Most people buying a mid range GPU arent future proofing either.",
                                      "score": 3,
                                      "created_utc": 1739827207.0,
                                      "replies": []
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "id": "mdb14s0",
                              "author": "MinimumArmadillo2394",
                              "body": "Why do you need a 4090? A 3080 will do just fine here...\n\nThe point is, most people dont need or want to play at the highest of high settings and having all the VRAM for a mid level gaming system wont help anyone.\n\nMid level gamers dont care about ray tracing or 4k or anything like that. Most mid level gamers just want mid to high settings at 1080p.\n\nYou dont buy a mid level card if youre wanting 60+fps at 4k with max settings a ray tracing with dlss.",
                              "score": -4,
                              "created_utc": 1739822907.0,
                              "replies": [
                                {
                                  "id": "mdb8pzp",
                                  "author": "Fredasa",
                                  "body": "> Why do you need a 4090? A 3080 will do just fine here...\n\nSpeaking as a 3080 owner, no, it really won't.  _Cyberpunk 2077_ in 4K (DLSS Quality) on a 3080 hits the cap with almost complete reliability when I open the map.  That literally dictates how I play the game.  (The game didn't do this in its 2020 launch iteration, I have to say.)  That's with every other app on the PC closed and every scrap of VRAM I can spare devoted to the game.\n\n> Mid level gamers dont care about ray tracing or 4k or anything like that.\n\nWe're having a conversation about \"16GB in most games at max settings in 4k\" per OP a few parents up.",
                                  "score": 2,
                                  "created_utc": 1739825054.0,
                                  "replies": [
                                    {
                                      "id": "mdbsp5x",
                                      "author": "MinimumArmadillo2394",
                                      "body": ">We're having a conversation about \"16GB in most games at max settings in 4k\" per OP a few parents up.\n\nI am the OP. And yes, that statement is still true. Apart from one setting with DLSS and ultra extreme raytracing or whatever, it can still run cyberpunk at ultra settings at 60+ fps. \n\n\"Most games\" is true. \n\nEven still, this is a mid level GPU. Cyberpunk with ray tracing and the graphics on the update are clearly an outliar. Acting like it cant play \"most games\" at 4k 60fps because it cant run ray tracing, something most amd gpus struggle with anyway, is fascious at best and outright misinformation at worst.",
                                      "score": 0,
                                      "created_utc": 1739830687.0,
                                      "replies": []
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": "mdawidj",
                  "author": "alexanderpas",
                  "body": "> Even skyrim with a huge texture overhaul in 4k with 100 mods didnt cap over 12 GB of VRAM for me.\n\n\nSkyrim is over 10 years old.\n\n\nModern games have more than twice the VRAM usage compared to games from 10 years ago.",
                  "score": 5,
                  "created_utc": 1739821621.0,
                  "replies": [
                    {
                      "id": "mdaxedh",
                      "author": "MinimumArmadillo2394",
                      "body": "The mods is the point lol. Those things are huge and have absolutely massive 4k textures constantly in vram. \n\nHave you used them before? Just one vfx overhaul was able to cripple my 1080 from 200 fps to less than 30 due to VRAM requirements.",
                      "score": 4,
                      "created_utc": 1739821867.0,
                      "replies": [
                        {
                          "id": "mddmoj9",
                          "author": "FarSolar",
                          "body": "The mods have definitely scaled with the release of more powerful GPUs. I remember running tons of lighting mods and HD texture replacers on my old Skyrim install and they ran on my 970's 3.5ish GB of VRAM without too much trouble.",
                          "score": 1,
                          "created_utc": 1739853244.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": "mdbfhsh",
                  "author": "dsmiles",
                  "body": ">Even skyrim with a huge texture overhaul in 4k with 100 mods didnt cap over 12 GB of VRAM for me.\u00a0\n\nIt would in VR. \n\nThere are several modpacks in VR that I cannot play with a 3080 12gb. In fact, the modpack I want to use (Mad God's Overhaul) cannot hit 90fps (best VR framerate for a Quest 2/3) with even a 4090.",
                  "score": 2,
                  "created_utc": 1739826931.0,
                  "replies": [
                    {
                      "id": "mdbrubi",
                      "author": "MinimumArmadillo2394",
                      "body": "Most people arent playing in VR lmao. People with mid range GPUs are not playing in VR either. You expect people to buy a $1000 headset and a $600 gpu? No lol",
                      "score": -2,
                      "created_utc": 1739830436.0,
                      "replies": []
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "mdc907j",
          "author": "TanmanG",
          "body": "As a student who's working with ML stuff, I think I can speak on behalf of a lot of people who would very much would like to have access to greater amounts of VRAM locally without having to rely on IaaS or slow & unstable techniques that repurpose RAM. Ideally without breaking the bank on cards that cost more than a motorcycle or used car.",
          "score": 1,
          "created_utc": 1739835760.0,
          "replies": [
            {
              "id": "mdcap24",
              "author": "MinimumArmadillo2394",
              "body": "Yes, I also have done quite a bit of ML stuff. I have 3090s on a rig doing ML stuff right now. I underclocked my gpu because ML stuff uses less processing power and more VRAM than most things. Each one was only $700 for 24 gigs of VRAM which was a great deal.\n\nUnless you're doing ML stuff, a shit ton of RAM wont do you any good as a gamer. And anyone doing ML stuff isnt going to buy a mid tier card. They will buy whatever has the most VRAM most of the time regardless of price as generally a company would pay for it anyway. If a company won't pay for it, there is no reason to go AMD at this point because 3090s are $700 for a refurbished FE at microcenter. \n\nAnyone making money doing ML stuff is buying beefier cards and anyone not making money is buying 3090s wherever they can get them for cheap.",
              "score": -1,
              "created_utc": 1739836322.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mdcikdu",
          "author": "Xendrus",
          "body": "a reachable 60 fps was the standard maybe 10 years ago. I'd say a good amount of people are now aware how much better 120 looks and feels.",
          "score": 0,
          "created_utc": 1739838910.0,
          "replies": [
            {
              "id": "mdcj8cw",
              "author": "MinimumArmadillo2394",
              "body": "Most people cant afford a monitor that does higher refresh rates or simply dont care",
              "score": 1,
              "created_utc": 1739839128.0,
              "replies": [
                {
                  "id": "mdcjo4h",
                  "author": "Xendrus",
                  "body": "Maybe 10 years ago. 120 is incredibly affordable now, and anyone who sees/uses 120@120 for even 1 second instantly see how amazing it is, anyone not using it just isn't aware of how awful their experience actually is in comparison, or they're literally 11 and can't afford a $150 monitor. ALSO even if you have a 60hz monitor getting 120fps on it will still feel better than 60 because of frame pacing, so still good.",
                  "score": -1,
                  "created_utc": 1739839272.0,
                  "replies": [
                    {
                      "id": "mdcqu1u",
                      "author": "MinimumArmadillo2394",
                      "body": "I chose a 4k monitor. 4k 120 fps monitors with gsync are expensive. Thats what I meant when I said most people cant afford them. Adding gsync on a monitor now a days practically 2x's the price.\n\nIm barely able to find a 4k 60 frame monitor thats 27+ inches for less than $200 lol\n\nEdit: Just checked pcpartpicker. Cheapest monitor that's 4k, 26-28 inches, G-Sync compatible, and has a 144+ refresh rate is $400. There are a total of 8 monitors with the average (median) price being $558 ($480). \n\nSo yeah, it's not just 11 year olds who don't have $150. It's a lot of people who don't have $400 when you can go get a 1080p, 27 inch monitor that has 4+ star reviews for $120. Most don't care enough and it doesn't matter to them when playing cyberpunk that they have 60 or 120 FPS. It matters to them that they can see the world in more detail",
                      "score": 3,
                      "created_utc": 1739841652.0,
                      "replies": []
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "md9y5ls",
      "author": "dan_Qs",
      "body": "But my future proofing!!! I can\u2019t use ultra max 2 texture settings on my mid range grafix card in 3 years any more 111!!!!",
      "score": -28,
      "created_utc": 1739812216.0,
      "replies": []
    },
    {
      "id": "mdajg13",
      "author": "1leggeddog",
      "body": "Someone is going to make on and put up a video onYoutube.",
      "score": -1,
      "created_utc": 1739818041.0,
      "replies": []
    },
    {
      "id": "mda4k7b",
      "author": "Practical_Orchid_568",
      "body": "I read Anne frank",
      "score": -13,
      "created_utc": 1739814002.0,
      "replies": []
    },
    {
      "id": "mdddshx",
      "author": "Vandorol",
      "body": "Why do you need 32GB? My 5080 shares memory with the system so it's like having 32? Games seem to run fine, Indiana Jones maxed out runsnatb130 fps\nhttps://i.imgur.com/DKi1sNc.jpeg",
      "score": -6,
      "created_utc": 1739849703.0,
      "replies": []
    }
  ]
}