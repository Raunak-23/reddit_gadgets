{
  "post": {
    "title": "AMD is allegedly cooking up an RX 9070 XT with 32GB VRAM",
    "author": "a_Ninja_b0y",
    "id": "1ion9zy",
    "score": 1056,
    "created_utc": 1739464875.0,
    "selftext": "",
    "num_comments": 197,
    "subreddit": "gadgets",
    "url": "https://www.reddit.com/r/gadgets/comments/1ion9zy/amd_is_allegedly_cooking_up_an_rx_9070_xt_with/"
  },
  "comments": [
    {
      "id": "mckravx",
      "author": "IvaNoxx",
      "body": "GPU segment needs healthy competition or nvidia will ruin us with these half baked gpus with AI oriented cards..",
      "score": 551,
      "created_utc": 1739465580.0,
      "replies": [
        {
          "id": "mckyzyb",
          "author": "AdmiralTassles",
          "body": "I'm glad Intel is in the mix now too.",
          "score": 173,
          "created_utc": 1739467734.0,
          "replies": [
            {
              "id": "mclvz56",
              "author": "Responsible-Juice397",
              "body": "Hopefully Intel will catch up in a few years",
              "score": 96,
              "created_utc": 1739476982.0,
              "replies": [
                {
                  "id": "mcm2y4l",
                  "author": "None",
                  "body": "[deleted]",
                  "score": 66,
                  "created_utc": 1739478925.0,
                  "replies": [
                    {
                      "id": "mcm4lmf",
                      "author": "NorysStorys",
                      "body": "That\u2019s still a whole two generations behind and while granted generational improvements are not as great as they used to be, it\u2019s still significant how far behind Intel is.",
                      "score": 28,
                      "created_utc": 1739479388.0,
                      "replies": [
                        {
                          "id": "mcmbc0s",
                          "author": "N0M0REG00DNAMES",
                          "body": "On the bright side, being able to buy an extremely budget card with proper dp 2.1 is a pretty nifty productivity solution.",
                          "score": 35,
                          "created_utc": 1739481273.0,
                          "replies": [
                            {
                              "id": "mcn0vl1",
                              "author": "AdmiralTassles",
                              "body": "Always pissed me off that cheaper cards don't come with DisplayPort (the superior standard)",
                              "score": 8,
                              "created_utc": 1739488838.0,
                              "replies": [
                                {
                                  "id": "mcn5dfk",
                                  "author": "N0M0REG00DNAMES",
                                  "body": "I think it still shocks me as to just how expensive all of this stuff has gotten. I remember being able to buy a Gtx 670 or 7950 for around $300 when I built my first pc \ud83d\ude2d the prices don\u2019t bother me as much for work use (eg ai), but the 4090 not being able to natively power a g9 57 was ridiculous to me",
                                  "score": 6,
                                  "created_utc": 1739490316.0,
                                  "replies": [
                                    {
                                      "id": "mcp19ke",
                                      "author": "None",
                                      "body": "[deleted]",
                                      "score": 1,
                                      "created_utc": 1739516947.0,
                                      "replies": [
                                        {
                                          "id": "mcp1ile",
                                          "author": "N0M0REG00DNAMES",
                                          "body": "Eh I paid $400 for it open box and use it for code, so it friend really matter much (I had dual 32 4ks before). Honestly even my M1 Max drives it fine, but it\u2019s a green too old to bf able to handle it natively either",
                                          "score": 1,
                                          "created_utc": 1739517089.0,
                                          "replies": []
                                        }
                                      ]
                                    }
                                  ]
                                },
                                {
                                  "id": "mcp0y81",
                                  "author": "None",
                                  "body": "[deleted]",
                                  "score": 2,
                                  "created_utc": 1739516769.0,
                                  "replies": [
                                    {
                                      "id": "mcptjqn",
                                      "author": "rpkarma",
                                      "body": "Eh an HDMI dummy plug as you pointed out is a great solve for it IME.",
                                      "score": 2,
                                      "created_utc": 1739534003.0,
                                      "replies": [
                                        {
                                          "id": "mcpuvju",
                                          "author": "None",
                                          "body": "[deleted]",
                                          "score": 1,
                                          "created_utc": 1739534670.0,
                                          "replies": [
                                            {
                                              "id": "mcsn1b5",
                                              "author": "rpkarma",
                                              "body": "That wasn\u2019t really a problem for me: I\u2019m never getting much over 60FPS when using game streaming outside of the house, and if I\u2019m in the house then the dummy plug wasn\u2019t needed anyway. YMMV I guess!",
                                              "score": 1,
                                              "created_utc": 1739566717.0,
                                              "replies": []
                                            }
                                          ]
                                        }
                                      ]
                                    },
                                    {
                                      "id": "mcp2v4q",
                                      "author": "Noctudeit",
                                      "body": "Remote Desktop does not rely on local displays. It uses as many displays as you have on the remote machine. Are you using some third party remote access like LogMeIn?",
                                      "score": 1,
                                      "created_utc": 1739517862.0,
                                      "replies": []
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        },
                        {
                          "id": "mcm5vfv",
                          "author": "nametaken_thisonetoo",
                          "body": "Sadly true",
                          "score": 1,
                          "created_utc": 1739479741.0,
                          "replies": []
                        },
                        {
                          "id": "mcoo0mz",
                          "author": "Ajreil",
                          "body": "We desperately need a budget card even if it's a few generations behind. I'm still running a 1660 Super.",
                          "score": 1,
                          "created_utc": 1739510162.0,
                          "replies": []
                        },
                        {
                          "id": "mcp5jqn",
                          "author": "inescapableburrito",
                          "body": "For the price it's pretty great. 220 for a b570 Vs 300 or more for a 3060, or 250ish for a b580 Vs 320 to 400 for a 4060. That's solid value and at a price point that has basically vanished from Nvidia's lineup.",
                          "score": 1,
                          "created_utc": 1739519460.0,
                          "replies": []
                        },
                        {
                          "id": "mcpzawc",
                          "author": "ACanadianNoob",
                          "body": "I think he means 4060. The B580 is ahead of the 4060 in most benchmarks, aside from games that have driver issues with Intel graphics which is admittedly too many for me to ever buy an Intel graphics card.",
                          "score": 1,
                          "created_utc": 1739536728.0,
                          "replies": []
                        },
                        {
                          "id": "mcpsbul",
                          "author": "Ghostrider215",
                          "body": "Because they don\u2019t have the knowledge that nvidia and amd have. AMD have always made CPU\u2019s and GPU\u2019s. nvidia has only ever made GPU\u2019s and intel has dominated the CPU market for years. Give Intel a chance to catch up and soon they\u2019ll be on top I feel.",
                          "score": 0,
                          "created_utc": 1739533364.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "id": "mcpg3j6",
              "author": "tempnew",
              "body": "Apparently Intel is giving up on discrete GPUs:\n\n> Unfortunately for PC graphics enthusiasts, it seems like Intel\u2019s discrete GPU efforts are similarly seen as a failed experiment now. Gelsinger says he\u2019s focused on simplifying the company\u2019s consumer products now, and dedicated graphics cards / chips are apparently on the chopping block.\n\nhttps://www.theverge.com/2024/11/1/24285513/intel-ceo-lunar-lake-one-off-memory-package-discrete-gpu",
              "score": -4,
              "created_utc": 1739526048.0,
              "replies": []
            },
            {
              "id": "mcoybn2",
              "author": "WarriYahTruth",
              "body": "Intel is garbage and has been for years since 2020.\n\nAMD ripped them to pieces while also developing GPUs....Intel doesn't even do GPUs which is even more embarrassing",
              "score": -14,
              "created_utc": 1739515317.0,
              "replies": [
                {
                  "id": "mcp94o7",
                  "author": "seek-ye-first-kether",
                  "body": "Thinking should be something you should develop",
                  "score": 4,
                  "created_utc": 1739521621.0,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "mcm76q3",
          "author": "CMDR_omnicognate",
          "body": "this AMD card is likely also an AI card though. it's still just a 9070xt underneath it just has twice the vram, which is likely to help with AI tasks",
          "score": 25,
          "created_utc": 1739480106.0,
          "replies": []
        },
        {
          "id": "mcmba2h",
          "author": "MileZero17",
          "body": "Nvidia\u2019s been sneaky for a while. Remember the 970 with 3.5gb and 0.5gb ram sections?",
          "score": 14,
          "created_utc": 1739481258.0,
          "replies": [
            {
              "id": "mco9vzd",
              "author": "LizardFishLZF",
              "body": "1060 3gb too",
              "score": 7,
              "created_utc": 1739504318.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mcm3v1t",
          "author": "N7even",
          "body": "And their stupidly designed RTX Melty90's",
          "score": 8,
          "created_utc": 1739479180.0,
          "replies": [
            {
              "id": "mcnrb1t",
              "author": "bonesnaps",
              "body": "melty80s now too.",
              "score": 7,
              "created_utc": 1739497685.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mcr7irw",
          "author": "dark_sylinc",
          "body": "Not to ruin the mood, but any GPU with 32GB of VRAM is not gaming oriented, but AI oriented.\n\nThe main requirement of AI is memory. Lots of memory. That's way more important than raw performance, and a 9070 w/ 32GB (i.e. cheap/weak processor with lots of VRAM) is the perfect AI card.",
          "score": 5,
          "created_utc": 1739551581.0,
          "replies": [
            {
              "id": "mcrocai",
              "author": "lostinspaz",
              "body": "i wouldn\u2019t say \u201cway more important\u201d. \n\ni would like more vram AND the equivalent of more CUDA cores. it\u2019s no fun when training runs take days.",
              "score": 1,
              "created_utc": 1739556504.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mcnhuti",
          "author": "chadhindsley",
          "body": "I just want them to be as smoothly compatible with Adobe and other software as nvidia's",
          "score": 4,
          "created_utc": 1739494463.0,
          "replies": []
        },
        {
          "id": "mclyp4y",
          "author": "Lancestrike",
          "body": "Pun intended?",
          "score": 3,
          "created_utc": 1739477737.0,
          "replies": [
            {
              "id": "mcmhwzc",
              "author": "twigboy",
              "body": "Well, the 5090 uses about half the wattage of a small grill so yeah checks out",
              "score": 3,
              "created_utc": 1739483133.0,
              "replies": [
                {
                  "id": "mcml6km",
                  "author": "daCampa",
                  "body": "And the connectors can be used as a one use small grill as well",
                  "score": 2,
                  "created_utc": 1739484061.0,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "mco0kdt",
          "author": "_kusa",
          "body": "AMD will release a GPU with 32GB VRAM and barely compete with a 5070.",
          "score": 4,
          "created_utc": 1739500951.0,
          "replies": [
            {
              "id": "mcp2ger",
              "author": "None",
              "body": "[deleted]",
              "score": 4,
              "created_utc": 1739517628.0,
              "replies": [
                {
                  "id": "mcpuv9p",
                  "author": "Sushigami",
                  "body": "So if you don't use frame generation you're claiming AMD is much better performance per price?",
                  "score": 3,
                  "created_utc": 1739534666.0,
                  "replies": []
                }
              ]
            },
            {
              "id": "mcp2ral",
              "author": "Brad1895",
              "body": "It might also not catch fire, use 500+ watts, and not cost as much as the rest of the pc.",
              "score": 3,
              "created_utc": 1739517800.0,
              "replies": [
                {
                  "id": "mdifklt",
                  "author": "_kusa",
                  "body": "The thing about nVidia is they sell a lot more GPUs, so even rare issues appear to be massive problems.\n\nAs long as they're honouring warranties I don't have a problem with that.\n\nThe cost is outrageous but it's more AMD refusing or being unable to put up even a small amount of competition that has led to this level of price gouging.",
                  "score": 1,
                  "created_utc": 1739917781.0,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "mcm8594",
          "author": "ricktor67",
          "body": "Meanwhile people keep buying the half baked GPUs from nvidia because a decade ago AMD has some shitty drivers for some cards no one bought anyway.",
          "score": 7,
          "created_utc": 1739480377.0,
          "replies": [
            {
              "id": "mcprkyy",
              "author": "4514919",
              "body": "A decade ago? This generation released with idle power draw of over 100W if you were using more than one display because the drivers couldn't correctly downclock the memory modules.\n\nIt took AMD months to *mostly* fix it.",
              "score": 9,
              "created_utc": 1739532965.0,
              "replies": []
            },
            {
              "id": "mcqdh4c",
              "author": "-Badger3-",
              "body": "I mean, nvidia\u2019s \u201chalf baked\u201d cards are still outperforming AMD\u2019s cards.",
              "score": 8,
              "created_utc": 1739542230.0,
              "replies": [
                {
                  "id": "mcqdswh",
                  "author": "ricktor67",
                  "body": "Does the fire and wiring melting help performance or is it ONLY the price tag of a used car that makes them so good?",
                  "score": -2,
                  "created_utc": 1739542344.0,
                  "replies": [
                    {
                      "id": "mcsq0ef",
                      "author": "lxs0713",
                      "body": "I mean while it's definitely a problem, it's only really happening to the high wattage cards selling for 4 figures. So it's not really something that will affect most people. My 4070 Super used like a third of the power the 5090 does. I'm not worried about it in the slightest.",
                      "score": 2,
                      "created_utc": 1739567606.0,
                      "replies": []
                    }
                  ]
                },
                {
                  "id": "mcqz0dw",
                  "author": "always_farting_",
                  "body": "as if 5% more performance will be a dealbreaker...",
                  "score": -2,
                  "created_utc": 1739549091.0,
                  "replies": [
                    {
                      "id": "mcqzjfz",
                      "author": "-Badger3-",
                      "body": "Nvidia's Ray tracing performance is still leagues better than AMD's, despite this subreddit's copium about true gamers not caring whether their graphics card has...better graphics.",
                      "score": 5,
                      "created_utc": 1739549248.0,
                      "replies": [
                        {
                          "id": "mcr9z5v",
                          "author": "always_farting_",
                          "body": "Lots of people dont want to eat the big hit that the enabling of  Ray tracing offers. Lots of people play games that Ray tracing doesnt matter. Lots of people dont want to spend the extra money that nvidia charges for their products\n\nYou decided whats important for you and since you believe what you think is more importan t than other people's beliefs now you think everyone else is coping.",
                          "score": 0,
                          "created_utc": 1739552294.0,
                          "replies": [
                            {
                              "id": "mcrwvnd",
                              "author": "-Badger3-",
                              "body": "I\u2019m old enough to remember when people would say this about shaders and tessellation\n\nWe\u2019re at a point where games are starting to come out that *require* ray-tracing capable hardware, and soon enough it\u2019s going to be the norm. I bought my AMD card with the knowledge that\u2019s it\u2019s a little cheaper, but it\u2019s not going to last me as long as the equivalent Nvidia card.\n\nAnd it absolutely *is* copium. If AMD announced today they found a way to get ray-tracing working as well as Nvidia, the \u201cpssh, who cares about ray-tracing?\u201d narrative in this subreddit would die overnight. It's insane that people here treat wanting a graphics card that provides better graphics isn't a valid enough reason for buying Nvidia.",
                              "score": 4,
                              "created_utc": 1739559006.0,
                              "replies": [
                                {
                                  "id": "mcwafj9",
                                  "author": "Jacek3k",
                                  "body": "Because it is about price-performance ratio. \nIf you have unlimitted budget, then sure, getting better product makes more sense. \n\nIn real-world scenarios tho, you need to make compromises. And having to pay much much more, but only having few percent better card + better raytracing (or raytracing at all vs none), yeah...\nI can see why some people wont choose the \"best option\"",
                                  "score": 0,
                                  "created_utc": 1739625582.0,
                                  "replies": []
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "id": "mco0s1z",
              "author": "_kusa",
              "body": "I've never had an AMD card that hasn't had issues, even the last gen of intel based Apple laptops with the AMD GPUs overheated when you plugged them into external monitors.",
              "score": -5,
              "created_utc": 1739501027.0,
              "replies": [
                {
                  "id": "mcqdem8",
                  "author": "ricktor67",
                  "body": "Meanwhile nvidia cards are literally catching on fire.",
                  "score": 1,
                  "created_utc": 1739542206.0,
                  "replies": [
                    {
                      "id": "mdif96n",
                      "author": "_kusa",
                      "body": "Mine isn't \ud83e\udd37\ud83c\udffe\u200d\u2642\ufe0f",
                      "score": 1,
                      "created_utc": 1739917684.0,
                      "replies": []
                    }
                  ]
                },
                {
                  "id": "mcp2jis",
                  "author": "Brad1895",
                  "body": "That was Apple doing their esthetic over function BS. Lack of cooling will do that to any device.",
                  "score": 1,
                  "created_utc": 1739517677.0,
                  "replies": [
                    {
                      "id": "mcp4aj3",
                      "author": "_kusa",
                      "body": "Nope, well documented issue where the gpu drew 20w of power when plugged into an external monitor.",
                      "score": -1,
                      "created_utc": 1739518705.0,
                      "replies": []
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "mcsah1s",
          "author": "None",
          "body": "nvidia doesn't give a fuck about gamers and never will",
          "score": 2,
          "created_utc": 1739562991.0,
          "replies": []
        },
        {
          "id": "mcmsmzl",
          "author": "None",
          "body": "lol baked\nTurns out it may be an accurate statement",
          "score": 1,
          "created_utc": 1739486277.0,
          "replies": []
        },
        {
          "id": "mcp0n55",
          "author": "None",
          "body": "Currently Nvidia has a monopoly over the market. It's not looking good.",
          "score": 1,
          "created_utc": 1739516595.0,
          "replies": []
        },
        {
          "id": "mcv15b8",
          "author": "rtyrty100",
          "body": "Half baked? These are quality ass products. AMD is a solid company and can\u2019t produce the quality and performance that Nvidia is.",
          "score": 1,
          "created_utc": 1739598995.0,
          "replies": []
        },
        {
          "id": "mcn3yr5",
          "author": "As7ro_",
          "body": "The problem is AMD has shown time and time again that their gpus deteriorate much faster than nvidia. They\u2019ve been competing for years and can\u2019t break past them.",
          "score": -3,
          "created_utc": 1739489843.0,
          "replies": [
            {
              "id": "mcnvio0",
              "author": "CrunchingTackle3000",
              "body": "Agree I\u2019ve been buying video cards since 1998 and the Nvidia cards last usually twice as long as the AMD card",
              "score": -1,
              "created_utc": 1739499149.0,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "mckqqy5",
      "author": "Whatworksbetter",
      "body": "the worst time for AMD to not compete with Nvidia.5080-5090 are really underwhelming. I hope they change their tune and this seems like they will. 32 GB will be incredible on a 600 dollar card.",
      "score": 269,
      "created_utc": 1739465423.0,
      "replies": [
        {
          "id": "mcl35q7",
          "author": "akeean",
          "body": "The 32gb likely won't be priced at 600. More like \"performs like a 4080, but 4090 VRAM\"-prices to get a nice extra margin on  the higher bill of materials.",
          "score": 115,
          "created_utc": 1739468906.0,
          "replies": [
            {
              "id": "mclcq6l",
              "author": "dilbert_fennel",
              "body": "Like 800 plus",
              "score": 38,
              "created_utc": 1739471634.0,
              "replies": [
                {
                  "id": "mclvsay",
                  "author": "akeean",
                  "body": "Easily. A GB of GDDR6 still adds $2-3 to the bill of materials wich means it'll add ~4-8x of that (per GB) to the assembled product sales price. Plus if there is nothing with a comparable spec in that price category, they can add more.\n\nIntel is rumored to be making a Battlemage model with extra VRAM for the same reason, but that won't be as fast or as much VRAM as a 9070XT 32GB, leaving AMD with a cozy spot to price in between that Battlemage card and second hand 4090(D) and new 5090(D).",
                  "score": 16,
                  "created_utc": 1739476928.0,
                  "replies": [
                    {
                      "id": "mcp39ut",
                      "author": "None",
                      "body": "[deleted]",
                      "score": 7,
                      "created_utc": 1739518099.0,
                      "replies": [
                        {
                          "id": "mcuogq0",
                          "author": "akeean",
                          "body": "Oh absolutely. Though I did mean it more about compute performance between the models, but you are 100% right about capacity and how frustrating it is.\n\nI have a nagging feeling that when NVIDIA releases their \"RTX 2.0 Neural Textures\", that it could save a huge chunk of VRAM at no quality loss, or look ridiculously better at no memory savings. But older RTX cards, especially those that were gimped by low VRAM assignments paired with \"too much\" compute for that buffer size, will take a high performance hit - not because of their VRAM but because of how that stuff will likely take more compute, or some type of compute that older cards can't do as well as the cards this feature is supposed to be selling.",
                          "score": 2,
                          "created_utc": 1739592836.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": "mcml86g",
                  "author": "Eteel",
                  "body": "If the $700 leak is accurate (which wouldn't be surprising), we're probably looking at $900 minimum.",
                  "score": 7,
                  "created_utc": 1739484074.0,
                  "replies": []
                }
              ]
            },
            {
              "id": "mcp303v",
              "author": "None",
              "body": "[deleted]",
              "score": 0,
              "created_utc": 1739517942.0,
              "replies": [
                {
                  "id": "mcpcbbm",
                  "author": "kazuviking",
                  "body": "That chinese leak in mhw was with frame generation. The score generated in mhw benchmark is calculated from the real fps. Some did the math and the 9070xt got 102 real fps in ultra.",
                  "score": 1,
                  "created_utc": 1739523641.0,
                  "replies": [
                    {
                      "id": "mcpdl58",
                      "author": "None",
                      "body": "[deleted]",
                      "score": 1,
                      "created_utc": 1739524443.0,
                      "replies": [
                        {
                          "id": "mcpfh6a",
                          "author": "kazuviking",
                          "body": "Not reall accurate as daniel owens tested it and he got 2 fps less on a 7900xtx.",
                          "score": 1,
                          "created_utc": 1739525651.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "mckvfqm",
          "author": "None",
          "body": "[deleted]",
          "score": 44,
          "created_utc": 1739466734.0,
          "replies": [
            {
              "id": "mcm1qiz",
              "author": "dsmiles",
              "body": "Man I really hope that was a price error. AMD can't be that stupid to blow such an obvious opportunity to expand their presence and competition in the market, right?\n\n  \n.... Right?",
              "score": 11,
              "created_utc": 1739478584.0,
              "replies": [
                {
                  "id": "mcmbfuq",
                  "author": "BurninNuts",
                  "body": "Green fan boys never go red, no point in trying to appeal to them.",
                  "score": 7,
                  "created_utc": 1739481303.0,
                  "replies": [
                    {
                      "id": "mcop79f",
                      "author": "tardis0",
                      "body": "I'm willing to. Once my 3070 kicks the bucket I'm probably going with AMD",
                      "score": 3,
                      "created_utc": 1739510709.0,
                      "replies": []
                    },
                    {
                      "id": "mcv7ipg",
                      "author": "Eisegetical",
                      "body": "I am a massive cuda green fanboy and 32gb vram at ~$1000 will 100% sway me.\u00a0",
                      "score": 2,
                      "created_utc": 1739602571.0,
                      "replies": []
                    },
                    {
                      "id": "mcmi5hp",
                      "author": "highfalutinjargon",
                      "body": "Me and a few of my friends did! Between the Intel CPU issues and the insane prices for Nvidia GPUS where I\u2019m based I went full team red for my build and some of my friends upgraded from their old GTX cards to 7800/7900 cards!",
                      "score": 3,
                      "created_utc": 1739483200.0,
                      "replies": [
                        {
                          "id": "mcml2s9",
                          "author": "HallucinatoryFrog",
                          "body": "Been building with AMD/Radeon since 2002, driver issues at times, but by and far a much bigger bang for my bucks.",
                          "score": 1,
                          "created_utc": 1739484032.0,
                          "replies": []
                        }
                      ]
                    },
                    {
                      "id": "md4giv4",
                      "author": "FerricDonkey",
                      "body": "I wouldn't say never.\n\n\nI was planning on sticking with Intel cpus pretty much forever, based purely on the fact that when some program has problems with one or the other, it usually has problems with amd.\u00a0\n\n\nBut then Intel sucked at 14th and 15th in a row, and my 8th wasn't cutting it - and the 9800x3d came out. So now I use that.\u00a0\n\n\nMy 3070 is starting to struggle. I'm asking too much of it - I could turn some settings down, or reduce resolution, but I don't want to. So in some games, I'll get the occasional stutter, or other issues.\u00a0\n\n\nThe 5080 is disappointing. I mean, it'd be great at $300 cheaper. But the price plus specs pisses me off. I could buy a 5090, but the idea of spending that much money on a graphics card just makes me angry. So I'm just annoyed at Nvidia.\u00a0\n\n\nPlus their cards don't actually exist for sale.\u00a0\n\n\nIf amd releases something good for a reasonable price, I might make the switch.\u00a0",
                      "score": 1,
                      "created_utc": 1739734628.0,
                      "replies": []
                    },
                    {
                      "id": "mcq7yem",
                      "author": "IamChwisss",
                      "body": "Why do you say that? I'd happily switch over considering the prices to move from a 4070 to a 5080",
                      "score": 1,
                      "created_utc": 1739540241.0,
                      "replies": []
                    }
                  ]
                },
                {
                  "id": "mcslyf9",
                  "author": "_kusa",
                  "body": "They fully gave up after the 10 series",
                  "score": 1,
                  "created_utc": 1739566396.0,
                  "replies": []
                }
              ]
            },
            {
              "id": "mckvmx5",
              "author": "None",
              "body": "[deleted]",
              "score": 18,
              "created_utc": 1739466791.0,
              "replies": [
                {
                  "id": "mckwfee",
                  "author": "None",
                  "body": "[deleted]",
                  "score": 18,
                  "created_utc": 1739467012.0,
                  "replies": [
                    {
                      "id": "mcm8l99",
                      "author": "Tudar87",
                      "body": "Sold and shipped by Amazon doesnt mean what it used to.",
                      "score": 19,
                      "created_utc": 1739480502.0,
                      "replies": [
                        {
                          "id": "mcohngf",
                          "author": "sarhoshamiral",
                          "body": "What? Sold by Amazon means it is not a 3rd party seller or a scalper. It is Amazon purchasing directly from the manufacturer and selling.",
                          "score": 2,
                          "created_utc": 1739507358.0,
                          "replies": []
                        },
                        {
                          "id": "mcockw4",
                          "author": "kscountryboy85",
                          "body": "Really? Have any sources to share? I am truly curious as to why you assert that? I only buy from items stocked by amazon.",
                          "score": 2,
                          "created_utc": 1739505330.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": "mckyrby",
                  "author": "AdmiralTassles",
                  "body": "Scalper? I hardly know her. Plus that would be kind of mean.",
                  "score": -16,
                  "created_utc": 1739467667.0,
                  "replies": []
                }
              ]
            },
            {
              "id": "mcmh3wn",
              "author": "Kerrigore",
              "body": "I saw a price leak saying MSRP was $1000CAD so $1350 seems possible for some high end variants.",
              "score": 3,
              "created_utc": 1739482904.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mcmcvih",
          "author": "AtomicSymphonic_2nd",
          "body": "Haven\u2019t seen this many frustrated PC folks in a long time. It was widely expected that Nvidia would go above and beyond the capabilities of the 4090 with this new generation.\n\nInstead the 7900 XTX maintains most of its lead or only loses by less than 10% with a damned 5080. And the 5080 has less RAM than the 7900 XTX!!!\n\nSo much of the community was making fun of AMD\u2019s 7900 XTX last year\u2026 now they are pissed at Nvidia for going the way of Intel\u2019s CPU division and stalling on progress and overly relying on AI to boost frame rates.",
          "score": 11,
          "created_utc": 1739481707.0,
          "replies": []
        },
        {
          "id": "mcl2dr6",
          "author": "akeean",
          "body": "5080&90 are underwhelming because NVIDIA knew there wasn't competition in this tier this generation as AMD had failed to bring a Zen 1 like breakthrough moment with GPU chiplets to GPUs.\n\n4090 had so much more juice compared to 3090 because they had expected the 7900xtx to be faster, but RDNA3 didn't quite pan out as expected.\n\nSo this gen they'll juice it with just driver locked bullshit & definig industy APIs (Neural texture & mega geometry) so AMD will have to play catch up with more things than just RT. \n\n5000 super series will just come with 50% more VRAM thanks to bigger modules scheduled to become avaliable. Hopefully XDNA will arrive with a bang in 2 years.",
          "score": 9,
          "created_utc": 1739468691.0,
          "replies": [
            {
              "id": "mcl5rpi",
              "author": "peppersge",
              "body": "How much of that is the symptom of the situation? That it was just hard to make a particular jump if both AMD and NVIDIA had problems making a leap? They both have to deal with the same technical obstacles required to advance the tech and are operating on roughly similar time frames between generations. NVIDIA is probably about 1 release cycle ahead of AMD.\n\nI am not sure how much of their philosophies differ (such as AMD tending to have more CPU cores while Intel has faster clock speed) that can really change how quickly they can make better hardware.",
              "score": 6,
              "created_utc": 1739469637.0,
              "replies": [
                {
                  "id": "mcl9w7b",
                  "author": "akeean",
                  "body": "A lot!\n\nSemiconductors is a gamble of betting on something 5 years down the line and then running with the things that panned out well while downplaying those that didn't (AMD's thing that panned out were cores thanks to TSMC chiplet packaging and weakness was the inter chiplet latency, while Intel being stuck on their node due to fabbing issues so they really optimizd their monolithic dies, even switching fabs didn't pan out for them wich is why they likely won't make a profit this year). \n\nCrypto & AI just distorted the markets so much that everyone got so confused with what to provision & now they are trapped by older products that were so overpowerd for what the market needs.\n\nPricing (and thus consumer value) is just something they can tweak in the last moment with consideration on how many wafers they had ordered years ago (and now need to sell) and how much money they can squeeze out of each, distributed between wthe different designs they can print on them, choosing the designs that make the most with some weeks lead time to swap between finished designs based on market acceptance.",
                  "score": 3,
                  "created_utc": 1739470820.0,
                  "replies": [
                    {
                      "id": "mcm3v87",
                      "author": "peppersge",
                      "body": "How does that work for GPUs? With CPUs, we know that there are some similarities since every company tries to go faster. Intel has just run into the 5 GHz wall faster.\n\nFor GPUs, how does the balance between parallel process and clock speed work?\n\nDoes that also fundamentally change how certain things such as hardware ray tracing can function?",
                      "score": 1,
                      "created_utc": 1739479182.0,
                      "replies": [
                        {
                          "id": "mcmp229",
                          "author": "akeean",
                          "body": "Modern GPUs already offer between thousands to tens of thousands of \"cores\". I  \n  \nThese cores support fewer functions compared to a CPU, instead they do a few tasks but accelerated by hardware (i.e. vector math) thanks to the many cores can do parallelizable tasks (of types they CAN process) way faster than a comparable (in terms of complexity) CPU. See Hashing in Cryptomining. That many more cores mean more of the die area is used by interconnects to all of those cores and maybe even between each other.  \n  \nIn Silicon these many cores (of varying types) are already grouped in dozens of clusters that handle various tasks and building an effective GPU in part depends on getting the ratio right for whatever the market most needs and to connect them all together so that data can make its way to all of them fast enough and that the work of splitting up work tasks keeps all of them fed with bites of just the right size and complexity. Part of this is a software (driver and application specific) task, not just a hardware design. Intel in particular is still struggling a lot with that last part in their driver and maybe even task managing hardware after changing underutilized hardware arrangement that Alchemist had. That's why we see Battlemage cards losing a lot more performance when paired with weaker CPUs than similarly powerful competitor GPUs and why Intel GPUs have some very large and expensive dies for their actual performance. They are probably not using their silicon very well most of the time.\n\nMaybe RT workload is a big reason why AMD gave up on their chiplet based approach for their GPUs as it didn't scale that well in terms of performance per die size (and thus cost) used.\n\nAt the moment Raytracing workload still has some centralized bottlenecks, that's also why the performance hit for enabling it is so high. For example a common technique in computer graphics to limit how many polygons needs shading in a scene, is to give all objects \"level of detail\" states, so that the objects that make up the most pixels on the screen will have more polygons and that something that is only 20 square pixels on the screen won't require up 60% of the shading cores of your chip. This means some games can switch between LODs a lot, especially Unreal 5 with their Nanite that automates LOD states and can cause loads of LOD switching per second. In Raytracing until now, chaning a LOD state of a moving object required rebuilding a key Datastructure. Nanite's dynamism can cause LOD switching on every single frame. That's why Unreal 5 can have serious performance issues.\n\nSome of that can be fixed via drivers and application optimization, see the performance boost that NVIDIA Megameshes offer on older RTX cards (see latest Allan Wake 2 update), by optimizing how it processes RT workload and how certain Datastructures are updated.\n\nMaybe API changes in new versions of Vulcan and DirectX will allow GPU makers to overcome bottlenecks and embrace chiplets with future GPU generations. This would allow for better utilization of the silicon wafers and less waste which helps the potential price floor.",
                          "score": 1,
                          "created_utc": 1739485203.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "mcl1yj7",
          "author": "DonArgueWithMe",
          "body": "I've been saying for months there's a big difference between \"not competing with the 5090\" and \"not making any better cards\" but nobody around here wanted to hear that. \n\nThey always make more than just 2 models in a generation, so it's wild people were so adamant there wouldn't be anything better than the 9070.",
          "score": 9,
          "created_utc": 1739468571.0,
          "replies": [
            {
              "id": "mcm1pv4",
              "author": "PM_YOUR_BOOBS_PLS_",
              "body": "> They always make more than just 2 models in a generation, so it's wild people were so adamant there wouldn't be anything better than the 9070.\n\nAre you dense?  AMD themselves have announced, many times, to the public, that they will NOT be making high end GPUs this generation.  That means there MIGHT be a 9080 eventually, but definitely NOT a 9090.\n\nAll leaks point to there being a 9070 and 9070 XT at launch.  That's it.  Those are literally the only cards that have been leaked for AMD so far, and leaks get very accurate around launch times.  If any other models are on the way, they are many months out, and will like be 9060 level cards or below.  You're high on copium if you really expect anything better than a 9070 XT this generation.\n\nEdit:  And to be clear, a 9070 with extra VRAM is NOT a better card for 99% of users.  Most games don't hit 16 GB of VRAM usage, and if you aren't hitting that limit, having more VRAM literally provides NO benefit.  The 32 GB leaker has said that specific card is deliberately targeting AI workloads, and will have a much higher price to reflect that.",
              "score": 7,
              "created_utc": 1739478578.0,
              "replies": [
                {
                  "id": "mcmhtyu",
                  "author": "DonArgueWithMe",
                  "body": "Are you dense? I'm talking about people who said the 9070 would be the top card AMD offered for the generation and you (along with most people here) are misquoting AMD. The 9070xt already proves I was right and you were wrong. \n\nThey never said they won't have a mid tier card. They never said specifically what models they will or won't compete against. They said they won't compete against the top end, which they never really have. They will not have a 5090 level card. \n\nBut that doesn't mean they won't be competitive at the $800-1000 range. Nvidia raised the \"top end\" through the stratosphere so 500-1000 is mid tier and under 500 is budget.\n\n\nEdit to add: if you guys think amd picked the name \"9070\" without intending to make a \"9080\" you are insane.",
                  "score": 1,
                  "created_utc": 1739483109.0,
                  "replies": [
                    {
                      "id": "mtzb4eq",
                      "author": "Both-Election3382",
                      "body": "Its exactly the same as a 5070 and 5070ti, its not beating a 5080.\n\n\nAmd always snatches defeat from the jaws of victory.\n\n\nThey had a real opportunity with the 9070xt to give it 20 or 24gb of vram at launch to really offer something more futureproof nvidia didnt but dropped the ball. Its weird because the generation before it they did offer those amounts.",
                      "score": 0,
                      "created_utc": 1748077298.0,
                      "replies": [
                        {
                          "id": "mu0y612",
                          "author": "DonArgueWithMe",
                          "body": "Because the 7900xtx was a higher tier card. You don't need 24gb of vram on what is intended to he a midterm card. It's trying to compete in price not compete with a 5080 or 5080...\n\nAnd with what they're currently offering everyone has been slashing Nvidia prices because nobody wants them. Amd prices have remained high because people are buying them.\n\nThat shows amd is winning this generation which will set them up to continue winning if they choose.",
                          "score": 1,
                          "created_utc": 1748101990.0,
                          "replies": [
                            {
                              "id": "mu1ohfo",
                              "author": "Both-Election3382",
                              "body": "The 7900 xtx costs the same as a 9070xt.\n\n\nThen there was the cheaper 20gb variant too.",
                              "score": 0,
                              "created_utc": 1748110357.0,
                              "replies": [
                                {
                                  "id": "mu26a0x",
                                  "author": "DonArgueWithMe",
                                  "body": "You're just proving my point. The xtx has been around for 2.5 years, is on older architecture, has much lower rt performance, no fsr 4, and had an msrp of $999 when it launched. \n\nThe fact that it's msrp was double the 9070xt's shows they weren't intended to be the same market segment.\n\nYou're basically saying \"why isn't the 5070 faster than the 4090?\" Because one was competing at the top tier and the other isn't.",
                                  "score": 1,
                                  "created_utc": 1748116370.0,
                                  "replies": []
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": "mcmkezg",
                      "author": "PM_YOUR_BOOBS_PLS_",
                      "body": "Man, I'm just going to follow your misspelled username at this point.  I don't want to distract you from that copium.",
                      "score": -5,
                      "created_utc": 1739483843.0,
                      "replies": []
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "mclf485",
          "author": "Paweron",
          "body": "Lol keep dreaming about this costing anywhere near 600$",
          "score": 3,
          "created_utc": 1739472327.0,
          "replies": []
        },
        {
          "id": "mclc26h",
          "author": "GrayDaysGoAway",
          "body": "> 32 GB will be incredible on a 600 dollar card.\n\nI don't see how that will even be useful, let alone incredible. This card won't be powerful enough to run games at high enough resolutions and detail levels to use anywhere near that much VRAM. This is just marketing bullshit to make it seem like AMD is competing when they're not.",
          "score": 8,
          "created_utc": 1739471442.0,
          "replies": [
            {
              "id": "mclcxac",
              "author": "dilbert_fennel",
              "body": "It will be a price point that draws down the higher cost cards. It will be a bugdet ai card that makes 1800 cards worth 1000.",
              "score": 2,
              "created_utc": 1739471691.0,
              "replies": [
                {
                  "id": "mcle619",
                  "author": "GrayDaysGoAway",
                  "body": "Wishful thinking. Practically everybody doing so-called \"AI\" work will have plenty of budget to buy those higher end cards and will be more than happy to do so for the extra performance they bring.\n\nIF this card actually comes in at $600 (which is very doubtful in and of itself), it may take a chunk out of Nvidia's midrange offerings. But it will have no effect on the upper tiers.",
                  "score": 2,
                  "created_utc": 1739472053.0,
                  "replies": [
                    {
                      "id": "mcm0273",
                      "author": "5160_carbon_steel",
                      "body": "> Practically everybody doing so-called \"AI\" work will have plenty of budget to buy those higher end cards\n\nNot necessarily. Sure, anyone who fine tunes or trains LLMs professionally will want the best of the best because time is money, but there are plenty of budget conscious hobbyists who are just looking to inference or maybe train some LORAs. I've seen plenty of people over at /r/LocalLLaMA talk about buying a used 3090 or an 7900 XTX because it's the cheapest way to run models as large as 32B parameters and not everyone is willing to cough up an extra grand or two for a 4090/5090.\n\nAnd realistically, even stuff like the 4090/5090 is still hobbyist level hardware for AI. If you're doing *real* AI work, you're going to a whole different price tier with stuff like the A100 or H100.\n\nI'm not gonna pretend that AMD's performance is anywhere near as good as Nvidia's. ROCm is far behind CUDA (that said, it is catching up, and it still does see use in [AI research](https://www.reddit.com/r/LocalLLaMA/comments/1inch7r/a_new_paper_demonstrates_that_llms_could_think_in/)), but if you're just a hobbyist inferencing its performance will be just fine.\n\nEven if this has the MSRP of the XTX ($1000), it's still going to be the cheapest way to get 32 GB of VRAM on a single card by a mile, and that's going to make it very appealing for people wanting to run AI locally. VRAM is king when it comes to AI, and with 8 GB more than a 3090/4090 you now have the headroom for larger models and context windows.\n\nAgain, CUDA is still the standard, so maybe you're right and we won't see this bite into 5090 sales too much, but I wouldn't completely count out that possibility.",
                      "score": 7,
                      "created_utc": 1739478117.0,
                      "replies": [
                        {
                          "id": "mcmits7",
                          "author": "throwawaycontainer",
                          "body": "> Again, CUDA is still the standard, so maybe you're right and we won't see this bite into 5090 sales too much, but I wouldn't completely count out that possibility.\n\nPushing up the VRAM is probably one of the best ways of trying to break the CUDA monopoly.",
                          "score": 2,
                          "created_utc": 1739483392.0,
                          "replies": [
                            {
                              "id": "mcncnuh",
                              "author": "AuryGlenz",
                              "body": "Yep, though they should go bigger. People *will* figure out how to make shit work on AMD cards if they have 48 or even 64GB available at a decent price point.",
                              "score": 1,
                              "created_utc": 1739492740.0,
                              "replies": []
                            }
                          ]
                        },
                        {
                          "id": "mcmi48q",
                          "author": "GrayDaysGoAway",
                          "body": "Only time will tell I suppose. But I think the number of people doing any sort of hobbyist machine learning stuff is an incredibly small segment of the market and won't move the needle at all.",
                          "score": 1,
                          "created_utc": 1739483190.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "mclw5ni",
          "author": "Responsible-Juice397",
          "body": "Looks like they focused on cpus this time, intel got crushed.",
          "score": 0,
          "created_utc": 1739477033.0,
          "replies": []
        },
        {
          "id": "mtzanhb",
          "author": "Both-Election3382",
          "body": "The 9070xt 16gb has been 800+ always, msrp is just a fake subsidized marketing stunt and you know it.\n\n\nThis card is gonna cost at least 900 euros if not 1k",
          "score": 0,
          "created_utc": 1748077006.0,
          "replies": []
        },
        {
          "id": "mckwbmv",
          "author": "i_am_Misha",
          "body": "You don't own a card otherwise you would knew for 1400E you get close to 4090 performance and rtx 5 smoothness. I pulled the trigger when I saw all influencers got binned cards for testing.",
          "score": -14,
          "created_utc": 1739466982.0,
          "replies": [
            {
              "id": "mckwxhg",
              "author": "Biohead66",
              "body": "it's a new GPU generation , performance uplift should be 30-50% . Well it is 10%. Unacceptable and you supporting them is bad for consumers.",
              "score": 6,
              "created_utc": 1739467153.0,
              "replies": [
                {
                  "id": "mcl0rn5",
                  "author": "i_am_Misha",
                  "body": "30-50% compared with? I get 250+ in mmorpg with 50 players around, 100+ in 500+ players  zerg fights and 50+ when 500+ players cast things near me on 49\" monitor. what are you talking about? Performance in benchmarks with binned cards or reality check when gamers present the product ON THE GAMES THEY PLAY?",
                  "score": -13,
                  "created_utc": 1739468237.0,
                  "replies": [
                    {
                      "id": "mcl3r7n",
                      "author": "archive_anon",
                      "body": "The fact that you equate framerate with the size of your monitor in inches really takes the steam out of your arguments faster than I've ever seen, ngl.",
                      "score": 13,
                      "created_utc": 1739469071.0,
                      "replies": []
                    },
                    {
                      "id": "mclewwi",
                      "author": "Paweron",
                      "body": "Mmorpgs are known to run on potatoes, how is that a GPU showcase? \n\nAnd yeah you get high FPS with it... 10% higher than with a 4080 super which is from last gen and cost a lot less.\n\nIt makes absolutely no difference if your monitor is 49'' or 2'' big, all that matters is the resolution",
                      "score": 6,
                      "created_utc": 1739472267.0,
                      "replies": [
                        {
                          "id": "mclicjr",
                          "author": "i_am_Misha",
                          "body": "False Information: Smooth Motion from RTX 5000 series IN GAME not on paper delivers 50+ fps. You do the math in %. My old 3080 on my monitor 51400x1440, max 60 fps on low, 25-30 on siege on Low Settings. I play Max High now Unreal 4 mmo.",
                          "score": -3,
                          "created_utc": 1739473232.0,
                          "replies": [
                            {
                              "id": "mcmi5jx",
                              "author": "Paweron",
                              "body": "Smooth motion had nothing to do with the 50 series, it will be available on the 40 series soon as well. So yet again, the 5080 performed barley better than the 4080",
                              "score": 1,
                              "created_utc": 1739483201.0,
                              "replies": []
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "mckqrr2",
      "author": "lokicramer",
      "body": "Ill stick with my 3080 until the 100980 XT TI drops next year.",
      "score": 61,
      "created_utc": 1739465430.0,
      "replies": [
        {
          "id": "mckyc1i",
          "author": "nokinship",
          "body": "Same. It's just not worth the investment with such low VRAM.",
          "score": 3,
          "created_utc": 1739467547.0,
          "replies": []
        },
        {
          "id": "mcktwux",
          "author": "Gorbashsan",
          "body": "Ditto, it more than gets the job done for me. I stuck with a 980 till the 1660 came out, and rode that till the 3080. Probably gonna keep this thing till at least 2026.",
          "score": 4,
          "created_utc": 1739466308.0,
          "replies": [
            {
              "id": "mcmi3ng",
              "author": "scr33ner",
              "body": "Yup doing same with my EVGA 3080ti",
              "score": 3,
              "created_utc": 1739483186.0,
              "replies": []
            },
            {
              "id": "mcmwvdu",
              "author": "kindbutblind",
              "body": "Still on 980ti \ud83e\udd72",
              "score": 2,
              "created_utc": 1739487567.0,
              "replies": [
                {
                  "id": "mcn0a05",
                  "author": "Gorbashsan",
                  "body": "awww, my 980 is still in my livingroom micro atx case powering my retro station and doing its best. They were good cards, it's nice to know others still have one going!",
                  "score": 1,
                  "created_utc": 1739488644.0,
                  "replies": []
                }
              ]
            },
            {
              "id": "mcmdhoc",
              "author": "AHungryManIAM",
              "body": "I\u2019m still using a 1080ti I got when it released. It still plays any game that comes out just fine.",
              "score": 0,
              "created_utc": 1739481879.0,
              "replies": [
                {
                  "id": "mcn2q63",
                  "author": "Optimus_Prime_Day",
                  "body": "Im on 1070 ti... its showing its age. like a lot.",
                  "score": 4,
                  "created_utc": 1739489438.0,
                  "replies": []
                },
                {
                  "id": "mcmjg4f",
                  "author": "Gorbashsan",
                  "body": "Honestly I might not have bothered, but my work involves some GPU heavy tasks at times so I have to get at least reaonably more recent cards regularly. Maya and Blender take a loooot longer to crap out a render on older hardware.",
                  "score": 1,
                  "created_utc": 1739483569.0,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "mcn2zgc",
          "author": "rmunoz1994",
          "body": "I\u2019ll stick. With my 1080ti",
          "score": 6,
          "created_utc": 1739489524.0,
          "replies": []
        },
        {
          "id": "mcqp5mk",
          "author": "Creator13",
          "body": "For the time being I'm very happy with my second hand 2070 super for my needs",
          "score": 1,
          "created_utc": 1739546123.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mckq5te",
      "author": "breastfedtil12",
      "body": "Hell yeah",
      "score": 21,
      "created_utc": 1739465260.0,
      "replies": []
    },
    {
      "id": "mcnli36",
      "author": "MrEmouse",
      "body": "Finally, a GPU with enough vram to play Star Citizen.",
      "score": 19,
      "created_utc": 1739495694.0,
      "replies": [
        {
          "id": "mcuef9f",
          "author": "FaveDave85",
          "body": "Yea but why would you want to play star citizen",
          "score": 3,
          "created_utc": 1739588684.0,
          "replies": [
            {
              "id": "mcv7ork",
              "author": "Eisegetical",
              "body": "They don't play star citizen. They only throw money into it for the idea of playing it",
              "score": 2,
              "created_utc": 1739602672.0,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "mckqv7x",
      "author": "renome",
      "body": "Not sure that's going to be a consumer-grade card.",
      "score": 23,
      "created_utc": 1739465457.0,
      "replies": []
    },
    {
      "id": "mcohna5",
      "author": "androidDude0923",
      "body": "It's been disproven by AMD.",
      "score": 5,
      "created_utc": 1739507356.0,
      "replies": []
    },
    {
      "id": "mcl0i0g",
      "author": "akeean",
      "body": "It's gonna be underpowered in gaming for that much VRAM, kinda like a RX570 8GB used to be. At least it won't bottleneck there until neural textures (80% VRAM reduction at higher quality) become industry standart. Nice for people that want to run a little bigger LLMs locally (i.e Deepseek <40b distilled), but not want to spend on an Epic plattform to run it on CPU or pro grade workstation cards instead if this will be priced around $1000.",
      "score": 9,
      "created_utc": 1739468162.0,
      "replies": []
    },
    {
      "id": "mcl4q7f",
      "author": "ednerjn",
      "body": "Depending of the price, it would be nice to play with AI without bankrupt yourself.",
      "score": 10,
      "created_utc": 1739469343.0,
      "replies": [
        {
          "id": "mcmwwyt",
          "author": "Forte69",
          "body": "I think the Mac Mini could be a few generations away from becoming a great option for local AI. Low power is a big deal.",
          "score": 5,
          "created_utc": 1739487580.0,
          "replies": [
            {
              "id": "mcnba4z",
              "author": "Brisslayer333",
              "body": "Why is low power a big deal? Like, you need to run the things 24/7 so the power savings are worth it, you mean? That's my guess",
              "score": 2,
              "created_utc": 1739492276.0,
              "replies": [
                {
                  "id": "mcndfih",
                  "author": "Forte69",
                  "body": "Yeah, if you\u2019re running it a lot then it adds up, similar to the economics of crypto mining back when people did it on gaming GPUs. Heat/noise/size matters to some people too.\n\nYou can already run AI models on sub-$1k macs, and while it\u2019s not great, nothing else compares in that price bracket. Their ARM chips are very promising",
                  "score": 1,
                  "created_utc": 1739492994.0,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "mckwa57",
      "author": "None",
      "body": "[deleted]",
      "score": 16,
      "created_utc": 1739466970.0,
      "replies": [
        {
          "id": "mclnfr2",
          "author": "Pendarric",
          "body": "i7-920 here, and the 970 :-)\nquite old, but my pile of shame is so high any expense to upgrade to a current gen build isnt worth it.",
          "score": 5,
          "created_utc": 1739474641.0,
          "replies": [
            {
              "id": "mcnn7rj",
              "author": "db_admin",
              "body": "That was a great chip.",
              "score": 2,
              "created_utc": 1739496276.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mcn03wp",
          "author": "JavaShipped",
          "body": "I just found a brand new 3060 12gb for \u00a3240 and picked that up for my partner to upgrade them from my old 970.\n\nShe's seen a doubling in frame rate and thinks I'm some kind of pc god.",
          "score": 2,
          "created_utc": 1739488590.0,
          "replies": []
        },
        {
          "id": "mcw49ob",
          "author": "Kevin2355",
          "body": "I've been milking that card out for like 10 years playing at 1080p. I haven't found a game i couldn't play yet. Maybe next year I'll get one... but probably not haha.",
          "score": 1,
          "created_utc": 1739622736.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mco5rg0",
      "author": "BlastFX2",
      "body": "[No they're not.](https://twitter.com/AzorFrank/status/1890123243090235407)",
      "score": 9,
      "created_utc": 1739502818.0,
      "replies": []
    },
    {
      "id": "mcpno5e",
      "author": "zomgasquirrel",
      "body": "This was actually proved false just a little while ago",
      "score": 3,
      "created_utc": 1739530732.0,
      "replies": []
    },
    {
      "id": "mclgehp",
      "author": "stockinheritance",
      "body": "ring absorbed retire sink roof sense spotted lock flag smart\n\n *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*",
      "score": 5,
      "created_utc": 1739472691.0,
      "replies": []
    },
    {
      "id": "mcmsua5",
      "author": "NotTooDistantFuture",
      "body": "All that ram would be compelling for running AI models locally, but unfortunately it\u2019s a nightmare getting most models running on AMD hardware. \n\nIf you\u2019re able to be picky about the model, there\u2019s choices that\u2019ll work, but there\u2019s whole types (img2img, for example) which don\u2019t seem to work at all, or not easily.",
      "score": 5,
      "created_utc": 1739486339.0,
      "replies": []
    },
    {
      "id": "mclbxvb",
      "author": "easant-Role-3170Pl",
      "body": "AMD has no problem with 24GB, the problem is in their software. I hope they take on matab and ray tracing technologies in all seriousness, because objectively they are far behind Nvidia. If they improve their software, then this will be real competition.",
      "score": 6,
      "created_utc": 1739471405.0,
      "replies": [
        {
          "id": "mcnc9ek",
          "author": "Brisslayer333",
          "body": "They were only behind by a single generation in RT last time, no? I believe the XTX performs better than the 5080 in some games with RT enabled, simply due to the insufficient frame buffer size on the 5080.",
          "score": 1,
          "created_utc": 1739492606.0,
          "replies": [
            {
              "id": "mcpezkt",
              "author": "easant-Role-3170Pl",
              "body": "it gives 20% less fps and has much worse picture quality. I hope that with the announcement of the new series of cards they will announce new versions of FSR that will really be able to compete with DLSS",
              "score": 1,
              "created_utc": 1739525335.0,
              "replies": [
                {
                  "id": "mcpjgyc",
                  "author": "Brisslayer333",
                  "body": "Are you conflating RT and upscaling, or are you just treating them as the same because you sorta need one for the other?\n\nAlso, FSR4 has already been announced and we already know how good it's going to be.",
                  "score": 1,
                  "created_utc": 1739528211.0,
                  "replies": [
                    {
                      "id": "mcpsf3b",
                      "author": "easant-Role-3170Pl",
                      "body": "Yes, but they presented it on live stands and on prototype cards. Still, there was no full release and real tests. So we are waiting.",
                      "score": 1,
                      "created_utc": 1739533411.0,
                      "replies": []
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "mcku1pj",
      "author": "Homewra",
      "body": "What about a midrange for 1440p with affordable prices?",
      "score": 2,
      "created_utc": 1739466345.0,
      "replies": [
        {
          "id": "mcmcnaf",
          "author": "Solo_Wing__Pixy",
          "body": "7800 XT is treating me great on 1440p 144hz, think I spent around $400 on it",
          "score": 3,
          "created_utc": 1739481644.0,
          "replies": []
        },
        {
          "id": "mcm31fb",
          "author": "MetalstepTNG",
          "body": "\"Sorry, $1200 Rx 9080 GRE on cutdown silicon is the best we can do.\"",
          "score": 1,
          "created_utc": 1739478951.0,
          "replies": [
            {
              "id": "mcm3f6d",
              "author": "Homewra",
              "body": "Thank you leather jacket man. I'll be waiting for the RX 9070 or 9060 XT then...",
              "score": 2,
              "created_utc": 1739479059.0,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "mcnvbnw",
      "author": "akgis",
      "body": "to be 32GB it will at have to be 512bit bus, I call BS on this.",
      "score": 2,
      "created_utc": 1739499079.0,
      "replies": [
        {
          "id": "mcps2fv",
          "author": "4514919",
          "body": "It's just a clamshell design like they have already done with the W7x00 Pro GPUs lineup.",
          "score": 1,
          "created_utc": 1739533225.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mcof38h",
      "author": "crimxxx",
      "body": "So probably something around a 3090, but more ram and much cheaper. This will probably be great for those people trying to run local large language models, except for the lack of cuda, since it\u2019s not nvidia. All things considered seems like a good thing if priced right.",
      "score": 2,
      "created_utc": 1739506315.0,
      "replies": []
    },
    {
      "id": "mcq6zaz",
      "author": "GhostDan",
      "body": "It is probably focused on gamers who are also AI hobbies. That extra memory would come in very helpful for running LLM.",
      "score": 2,
      "created_utc": 1739539874.0,
      "replies": []
    },
    {
      "id": "mckxsqd",
      "author": "Xero_id",
      "body": "20gb vram is enough, just do that on a good card at a price lower than rtx5080 and you'll be good. 32gb is overkill right now and will force them to price to high for anyone thinking of switching to them.",
      "score": 6,
      "created_utc": 1739467397.0,
      "replies": [
        {
          "id": "mcltd23",
          "author": "th3davinci",
          "body": "This isn't aimed at the standard market, but probably \"prosumers\" who game as a hobby but also do shit like rendering, video editing and such.",
          "score": 10,
          "created_utc": 1739476260.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mclyqch",
      "author": "3good5you",
      "body": "What the hell is the naming scheme?! I just bought a 7900XT. How can this be followed by for example 9070XT?!",
      "score": 2,
      "created_utc": 1739477746.0,
      "replies": [
        {
          "id": "mcmjlhh",
          "author": "saints21",
          "body": "And Nvidia is only on 50's! AMD is so far ahead.",
          "score": 4,
          "created_utc": 1739483611.0,
          "replies": []
        },
        {
          "id": "mcm39u6",
          "author": "MetalstepTNG",
          "body": "First time?",
          "score": 7,
          "created_utc": 1739479017.0,
          "replies": [
            {
              "id": "mcmli7i",
              "author": "3good5you",
              "body": "Apparently\u2026",
              "score": 1,
              "created_utc": 1739484156.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mcncfc9",
          "author": "Brisslayer333",
          "body": "The 9070XT isn't following the 7900XT, it's following the 7700XT presumably if the names are any indication.",
          "score": 2,
          "created_utc": 1739492662.0,
          "replies": [
            {
              "id": "mco41a7",
              "author": "0ccdmd7",
              "body": "So theoretically is the 7900XT expected to be superior to the 9070XT? or maybe marginally the opposite?",
              "score": 1,
              "created_utc": 1739502203.0,
              "replies": [
                {
                  "id": "mcprhfv",
                  "author": "Brisslayer333",
                  "body": "AMD's promotional material, which we shouldn't rely on because these companies can't be trusted to tell it straight, suggests that the 9070XT and 7900XT have similar/identical performance.\n\nFor your reference I'm one of those \"let's shut up and wait for the actual benchmarks\" kind of people. In early March we'll see what's what.",
                  "score": 1,
                  "created_utc": 1739532913.0,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "mcmc65g",
      "author": "AtomicSymphonic_2nd",
      "body": "And even after repeatedly claiming in public that they don\u2019t want to make high-end GPUs anymore\u2026 rumors like this exist.\n\nAnd now I wonder if they\u2019re gonna make a \u201c9070 XTX\u201d since Nvidia is screwing their chance to get ahead with their 5080 cards.",
      "score": 2,
      "created_utc": 1739481508.0,
      "replies": []
    },
    {
      "id": "mcl04nj",
      "author": "Samwellikki",
      "body": "Will it arrive FULLY cooked, is what everyone wants to know",
      "score": 1,
      "created_utc": 1739468057.0,
      "replies": []
    },
    {
      "id": "mcmajxp",
      "author": "Guzeno",
      "body": "Gosh I remember when my GPU had 4 GB of ram. back then you'd conquer the world on that! It's mad how far we got haha",
      "score": 1,
      "created_utc": 1739481054.0,
      "replies": []
    },
    {
      "id": "mcmj4ak",
      "author": "LaxLogik",
      "body": "If this is the case, I will be switching to team red!!",
      "score": 1,
      "created_utc": 1739483475.0,
      "replies": []
    },
    {
      "id": "mcn2jn6",
      "author": "Optimus_Prime_Day",
      "body": "Just want them to catch up on rtx capabilities. Then they can truly take nvidia for a ride.",
      "score": 1,
      "created_utc": 1739489379.0,
      "replies": []
    },
    {
      "id": "mcn5rr4",
      "author": "OscarDivine",
      "body": "Add an X to the name tack on some more RAM and punch up some clocks, Boom 9070XTX is Born",
      "score": 1,
      "created_utc": 1739490447.0,
      "replies": []
    },
    {
      "id": "mcn9r86",
      "author": "colin_colout",
      "body": "My 1070 from almost 10 years ago was 8gb.  My 4060 is 8gb. \n\nI get why they are stingy with the vram, but it kinda hurts.",
      "score": 1,
      "created_utc": 1739491768.0,
      "replies": []
    },
    {
      "id": "mcnyde3",
      "author": "bcredeur97",
      "body": "AMD going at it with the VRAM since everyone is complaining about Nvidia\u2019s VRAM.\n\nGreat move!",
      "score": 1,
      "created_utc": 1739500167.0,
      "replies": []
    },
    {
      "id": "mcp5pek",
      "author": "zorrodood",
      "body": "Damn, that's like 4000 more than Nvidia. Impressive!",
      "score": 1,
      "created_utc": 1739519554.0,
      "replies": []
    },
    {
      "id": "mcpjtx5",
      "author": "Kiahra",
      "body": "As a VR Player i would absolutely love a decent mid range card with as much VRAM as possible. But yes that is one very specific use-case.",
      "score": 1,
      "created_utc": 1739528434.0,
      "replies": []
    },
    {
      "id": "mclgyhu",
      "author": "zandadoum",
      "body": "I sadly will stick to nvidia coz amd drivers are usually garbage.",
      "score": 1,
      "created_utc": 1739472846.0,
      "replies": []
    },
    {
      "id": "mckxz70",
      "author": "MyGoldfishGotLoose",
      "body": "That'd be big for running llms at home.",
      "score": 1,
      "created_utc": 1739467447.0,
      "replies": []
    },
    {
      "id": "mckxpns",
      "author": "anothersnappyname",
      "body": "Let\u2019s see the blender, redshift, resolve, and nuke benchmarks. AMD has had a lot of trouble competing with nvidia on the pro front. But man fingers crossed this steps up somehow",
      "score": 1,
      "created_utc": 1739467372.0,
      "replies": []
    },
    {
      "id": "mclmt05",
      "author": "uselessmindset",
      "body": "For the low price of $4,999.99 USD\u2026.",
      "score": 0,
      "created_utc": 1739474466.0,
      "replies": [
        {
          "id": "mcminz6",
          "author": "Kerrigore",
          "body": "*Before tariffs",
          "score": 1,
          "created_utc": 1739483345.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mcks6vs",
      "author": "pragmatic84",
      "body": "Until AMD can sort out proper ray tracing and DLSS it doesn't matter how much vram they throw on their cards and Nvidia knows this.",
      "score": -5,
      "created_utc": 1739465825.0,
      "replies": [
        {
          "id": "mcktvvm",
          "author": "dirtinyoureye",
          "body": "Sounds like ray tracing on these cards are similar to high end 30 series. Improvement at least. And FSR 4 looks like a big step up.",
          "score": 5,
          "created_utc": 1739466300.0,
          "replies": [
            {
              "id": "mckud9z",
              "author": "pragmatic84",
              "body": "I hope so, AMD have done a great job at providing value over the last few years but if you want something high end you're basically stuck with Nvidia's extortionate prices atm\n\nWe all benefit from legit competition.",
              "score": 2,
              "created_utc": 1739466435.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mckvy6l",
          "author": "cpuguy83",
          "body": "Sorry, Radeon ray traces just fine except for path tracing... and nvidia is shit at it too.. just amd is more shit.",
          "score": 0,
          "created_utc": 1739466878.0,
          "replies": []
        },
        {
          "id": "mckur28",
          "author": "DutchDevil",
          "body": "There is a non-gamer market for these cpu\u2019s that wants to run ai models at home or play with them in a prof. environment without breaking the bank. With 2 of these in a system you can run a fairly large model, it won\u2019t be blazing fast but probably fast enough to get some decent use out of it.",
          "score": -1,
          "created_utc": 1739466543.0,
          "replies": [
            {
              "id": "mckya1t",
              "author": "TechnoRedneck",
              "body": "As someone who does mess with AI models in a homelab environment, the vram sounds great, but that doesn't make these good for AI. An absolute ton of AI applications are CUDA based and so are locked out of AMD due to CUDA being CUDA. The rest that don't rely on CUDA still are being built with Nvidia in mind.\n\nThe two most prominent GUI frontend options for Stable Diffusion(the most popular open source image generation AI) are A1111 and ComfyUI. Both of which only fully support AMD cards via Linux, both of their Windows versions are missing features for AMD.\n\nNvidia leads in AI because they were so early to the ai market. Being so early let them create the standards(CUDA) and sell everyone hardware, now all the software needs the hardware lock in that is keeping AMD out. OpenCL (vendor independent) and ROCm(amd) were simply to late to the party.",
              "score": 2,
              "created_utc": 1739467531.0,
              "replies": [
                {
                  "id": "mcl4tza",
                  "author": "DutchDevil",
                  "body": "Sure, but Ollama works fine, I could absolutely make a case for these gpu\u2019s in an AI setting.",
                  "score": -1,
                  "created_utc": 1739469372.0,
                  "replies": []
                }
              ]
            },
            {
              "id": "mcmodab",
              "author": "Kike328",
              "body": "<0.001% of the userbase lol.\n\nI don\u2019t think people is going to spend 1000$ to run a mediocre 32b parameter distilled model when paid ultra cheap APIs are available",
              "score": 1,
              "created_utc": 1739484996.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mcnj41v",
          "author": "slippery_hemorrhoids",
          "body": "Incorrect ray tracing is just fine.\n\nTheir problem is stability and support in general. I've got a 9700xtx and handles everything I throw at it on ultra, but it crashes at least once every other night in various games. I'll likely never buy an amd gpu again until that improves.",
          "score": 0,
          "created_utc": 1739494884.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mcmnjx3",
      "author": "ChimkenNumggets",
      "body": "If it outperforms my 7900XTX I will buy it immediately just to support AMD",
      "score": 0,
      "created_utc": 1739484754.0,
      "replies": []
    },
    {
      "id": "mcl5pcc",
      "author": "zmunky",
      "body": "I mean it's too little too late. No one is going to care. Nvidia and Intel already have offerings that are better deals dollar for dollar. Tarrifs are going to make this card make zero sense on the perform per dollar.",
      "score": -3,
      "created_utc": 1739469618.0,
      "replies": [
        {
          "id": "mcqndu2",
          "author": "BlastFX2",
          "body": "If you believe Jensen's \u201c4090 performance for $550\u201d bullshit, sure. \n\nLeaked benchmarks show the 9070 XT to be roughly equivalent to the 7900 XTX, which would put it somewhere around 5070ti or 5080, depending on the workload. Leaked prices put the MSRP at $700, slightly undercutting the 5070ti. Not that MSRP means anything these days\u2026",
          "score": 1,
          "created_utc": 1739545560.0,
          "replies": [
            {
              "id": "mcqy08a",
              "author": "zmunky",
              "body": "There is no chance it's going to be close to the 7900xtx. They even said that much. It's going to be competing in the 70 class but they banked on Nvidia hosing everyone and for once they didn't and they are scrambling to price it.",
              "score": 1,
              "created_utc": 1739548791.0,
              "replies": [
                {
                  "id": "mcsiz6t",
                  "author": "BlastFX2",
                  "body": "According to this [leaked Chinese benchmark](https://videocardz.com/newz/amd-radeon-rx-9070-xt-gpu-z-specs-leak-4096-cores-16gb-g6-20-gbps-memory-and-3-1-ghz-oc-boost), it gets 212 FPS in Monster Hunter Wilds on the ultra preset (incorrectly translated in the article as high) at 1080p with framegen. That's equivalent to a 7900 XTX.",
                  "score": 1,
                  "created_utc": 1739565514.0,
                  "replies": []
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "mclnzev",
      "author": "Discobastard",
      "body": "PS5 Pro is fine",
      "score": -3,
      "created_utc": 1739474791.0,
      "replies": []
    },
    {
      "id": "mcl5w0y",
      "author": "Prodigy_of_Bobo",
      "body": "Make it 64gb and I'm sold... Need to ai my LLMs asap fr my GPU can't churn out enough waifu hentai Kwik enuf for my insta ngl",
      "score": -6,
      "created_utc": 1739469671.0,
      "replies": []
    },
    {
      "id": "mckzn5c",
      "author": "nicman24",
      "body": "I will buy this regardless of price. Fuck nvidia for going 2k and them twice for the paper launch",
      "score": -5,
      "created_utc": 1739467914.0,
      "replies": []
    },
    {
      "id": "mcmlqnx",
      "author": "InfinityTortellino",
      "body": "They need to implement better ai compatibility or no one will care",
      "score": -5,
      "created_utc": 1739484224.0,
      "replies": []
    }
  ]
}