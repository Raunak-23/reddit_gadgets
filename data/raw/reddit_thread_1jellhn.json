{
  "post": {
    "title": "Nvidia announces DGX desktop \u201cpersonal AI supercomputers\u201d | Asus, Dell, HP, and others to produce powerful desktop machines that run AI models locally.",
    "author": "chrisdh79",
    "id": "1jellhn",
    "score": 869,
    "created_utc": 1742347328.0,
    "selftext": "",
    "num_comments": 258,
    "subreddit": "gadgets",
    "url": "https://www.reddit.com/r/gadgets/comments/1jellhn/nvidia_announces_dgx_desktop_personal_ai/"
  },
  "comments": [
    {
      "id": "mijm3kd",
      "author": "zirky",
      "body": "can i just buy a regular ass graphics card at a reasonable price?",
      "score": 837,
      "created_utc": 1742347577.0,
      "replies": [
        {
          "id": "mijmamg",
          "author": "legendov",
          "body": "No",
          "score": 392,
          "created_utc": 1742347646.0,
          "replies": [
            {
              "id": "mil054y",
              "author": "spudddly",
              "body": "you must pay $15,000 so you can run a chatbot on your desktop because we invested $15 trillion into it and would reallly like some money back",
              "score": 110,
              "created_utc": 1742371268.0,
              "replies": [
                {
                  "id": "mimfr5i",
                  "author": "thirteennineteen",
                  "body": "To be fair I do really want Chat GPT running locally \u00af\\_(\u30c4)_/\u00af",
                  "score": 6,
                  "created_utc": 1742394146.0,
                  "replies": [
                    {
                      "id": "mimjnew",
                      "author": "Cthulhar",
                      "body": "Just get Jan ai",
                      "score": 1,
                      "created_utc": 1742395362.0,
                      "replies": [
                        {
                          "id": "mimwsv0",
                          "author": "None",
                          "body": "[deleted]",
                          "score": 7,
                          "created_utc": 1742399314.0,
                          "replies": [
                            {
                              "id": "miqizqm",
                              "author": "TheSpecialApple",
                              "body": "i host models locally, theres a few good ones, depending on your hardware you can scale back for a more distilled version or grab something better. regardless its easily doable already",
                              "score": 1,
                              "created_utc": 1742439953.0,
                              "replies": []
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "id": "mikqznh",
              "author": "half-baked_axx",
              "body": "why waste chip on 32gb $2000+ consumer card when you can sell 96gb workstation monsters for $10000\n\nwe're fucked",
              "score": 64,
              "created_utc": 1742365430.0,
              "replies": [
                {
                  "id": "mim0508",
                  "author": "hbdgas",
                  "body": "That's OK, just wait 5-10 years and you'll be able to get one of those workstations used for $20000.",
                  "score": 11,
                  "created_utc": 1742388819.0,
                  "replies": [
                    {
                      "id": "mim5vc2",
                      "author": "None",
                      "body": "[deleted]",
                      "score": 2,
                      "created_utc": 1742390879.0,
                      "replies": [
                        {
                          "id": "mim9ui4",
                          "author": "shadrap",
                          "body": "Eggs or meme coins?",
                          "score": 1,
                          "created_utc": 1742392235.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": "mily7j7",
                  "author": "alidan",
                  "body": "you can keep everything you want at the 3nm or whatever the fuck node it is, go back to the last node you used and older and just make gpus on those and sell them at whatever price makes sense, hell you could make 700+mm dies or multiple smaller dies and no one would care because instead of 20k+ per wafer, they would be 1-5k per wafer on mature nodes that no one really has a demand for. \n\nstop screwing over everyone with fusing crap off your dies and just make a unified architecture you can use for high end workstations and normal gpus, your bread and butter market is all stupidly high end headless crap anyway, why segment a market anymore.",
                  "score": 12,
                  "created_utc": 1742388098.0,
                  "replies": [
                    {
                      "id": "mink8ot",
                      "author": "parisidiot",
                      "body": "but it doesn't work that way. tsmc or whoever has limited capacity.\n\nsay they can only make 10,000 chips in a month or whatever. nvidia has customers for 10,000 datacenter chips. why would they bother making any of the lower end chips that earn them less money if they can sell all of the more expensive ones?",
                      "score": 4,
                      "created_utc": 1742406068.0,
                      "replies": [
                        {
                          "id": "mixtbaf",
                          "author": "zzazzzz",
                          "body": "fab capacity is on a per node basis. you completely missed the commenters point. they wouldnt be trading any capacity of their dataceter chips. they would buy additional capacity of a much cheaper node.",
                          "score": 1,
                          "created_utc": 1742543363.0,
                          "replies": [
                            {
                              "id": "miyxdmj",
                              "author": "parisidiot",
                              "body": "does TSMC not have a maximum number of wafers they can make regardless? they're not building a new factory for each node, no? my assumption, and it is an assumption, is based on conjecture that one node would take capacity from the others. i'm wrong on this, i'm guessing?",
                              "score": 1,
                              "created_utc": 1742563429.0,
                              "replies": [
                                {
                                  "id": "miz99pd",
                                  "author": "zzazzzz",
                                  "body": "i mean do you think they just throw away the whole production line every time they go to a new node?\nand if you dont need cutting edge node you dont need TSMC. samsung produces more chips than TSMCeach year for example",
                                  "score": 1,
                                  "created_utc": 1742567263.0,
                                  "replies": [
                                    {
                                      "id": "mizwwws",
                                      "author": "parisidiot",
                                      "body": "yes, generally production lines in other industries are re-tooled. when ford starts producing the 2-26 f-150, they don't usually build a whole new factory -- they re-tool an older production line.\n\ntsmc can only produce n wafers per year. they don't just add on new fab capacity non-stop -- i mean, i'm sure they do, but they aren't necessarily keeping older lines producing at the same capacity as they were before. production space, machinery, raw materials -- these are all finite. if they can dedicate them exclusively to higher margin units, they will. no?",
                                      "score": 1,
                                      "created_utc": 1742574208.0,
                                      "replies": []
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": "miqzcah",
                      "author": "Moscato359",
                      "body": "Nvidia doesn't make any products, at all.\n\nRemember this.",
                      "score": 1,
                      "created_utc": 1742447434.0,
                      "replies": []
                    }
                  ]
                }
              ]
            },
            {
              "id": "mijwnqs",
              "author": "sargonas",
              "body": "Why? There\u2019s no profit in it for them to do that. :(",
              "score": 12,
              "created_utc": 1742351252.0,
              "replies": [
                {
                  "id": "milxcro",
                  "author": "None",
                  "body": "[deleted]",
                  "score": 3,
                  "created_utc": 1742387765.0,
                  "replies": [
                    {
                      "id": "mir7gcy",
                      "author": "Domascot",
                      "body": "Thats what *you* think, but Jensen not so much, apparently \u00af\\\\\\_(\u30c4)\\_/\u00af",
                      "score": 1,
                      "created_utc": 1742452172.0,
                      "replies": []
                    }
                  ]
                },
                {
                  "id": "ming081",
                  "author": "Hour_Reindeer834",
                  "body": "I think they could sell enough small, cool, quiet, reliable GPUs easy.",
                  "score": 1,
                  "created_utc": 1742404863.0,
                  "replies": [
                    {
                      "id": "minym4r",
                      "author": "sargonas",
                      "body": "Yeah but for every silicon chip that comes out of TSMC\u2019s fab on their hardware, they can sell as a consumer GPU for $1k, or put that same chip on an enterprise AI card for $25-100k",
                      "score": 2,
                      "created_utc": 1742410214.0,
                      "replies": []
                    }
                  ]
                },
                {
                  "id": "mikjw9p",
                  "author": "trashacount12345",
                  "body": "Indeed. Why should they bust their ass for you?",
                  "score": -33,
                  "created_utc": 1742361319.0,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "mijnb39",
          "author": "Bangaladore",
          "body": "I get the frustration on the GPU side, but to be clear, the highest end consumer GPU has like 32 GB of usable memory for AI models. \n\nThese systems go up to 784GB of unified memory for AI models.",
          "score": 56,
          "created_utc": 1742347995.0,
          "replies": [
            {
              "id": "mijq2xa",
              "author": "ericmoon",
              "body": "Can I use it while microwaving something without tripping a breaker?",
              "score": 60,
              "created_utc": 1742348951.0,
              "replies": [
                {
                  "id": "mik3f4i",
                  "author": "StaticFanatic3",
                  "body": "I\u2019m guessing it\u2019s going to be something like the AMD \u201cStrix halo\u201d chips in which case it\u2019ll probably use less power than a typical desktop PC with a discrete graphics card",
                  "score": 8,
                  "created_utc": 1742353792.0,
                  "replies": []
                },
                {
                  "id": "mime78g",
                  "author": "sprucenoose",
                  "body": "Depends. Do you have any friends at your local power company? With some mild rolling brownouts they can probably throw enough juice your way.",
                  "score": 3,
                  "created_utc": 1742393664.0,
                  "replies": []
                },
                {
                  "id": "mijs62p",
                  "author": "None",
                  "body": "[deleted]",
                  "score": -14,
                  "created_utc": 1742349664.0,
                  "replies": [
                    {
                      "id": "mijy12g",
                      "author": "AccomplishedBother12",
                      "body": "I can turn on every light in my house and it will still be less than 1 kilowatt, so no",
                      "score": 9,
                      "created_utc": 1742351750.0,
                      "replies": []
                    },
                    {
                      "id": "mijy2d9",
                      "author": "Giantmidget1914",
                      "body": "I have a power meter on two fridges.  It takes about 120w when running.",
                      "score": 8,
                      "created_utc": 1742351763.0,
                      "replies": []
                    },
                    {
                      "id": "mijszfa",
                      "author": "ericmoon",
                      "body": "lol no it does not",
                      "score": 12,
                      "created_utc": 1742349947.0,
                      "replies": [
                        {
                          "id": "mijzv22",
                          "author": "None",
                          "body": "[removed]",
                          "score": -9,
                          "created_utc": 1742352422.0,
                          "replies": [
                            {
                              "id": "mik30zx",
                              "author": "QuaternionsRoll",
                              "body": "/s? I hope? Unified memory has relatively little to do with the power efficiency of Macs",
                              "score": 10,
                              "created_utc": 1742353637.0,
                              "replies": [
                                {
                                  "id": "mim0von",
                                  "author": "None",
                                  "body": "[removed]",
                                  "score": 0,
                                  "created_utc": 1742389088.0,
                                  "replies": [
                                    {
                                      "id": "mine6bp",
                                      "author": "QuaternionsRoll",
                                      "body": "That\u2019s great, but Macs don\u2019t have nearly the same capabilities\u2026 good luck running Llama 3.1 405B without quantization on a Mac. What point are you trying to make, exactly?\n\nYes, if you\u2019re just trying to run a dinky little 7B parameter model, a custom PC probably isn\u2019t worth it, but that\u2019s no secret.",
                                      "score": 0,
                                      "created_utc": 1742404346.0,
                                      "replies": [
                                        {
                                          "id": "mio0t2v",
                                          "author": "None",
                                          "body": "[removed]",
                                          "score": 0,
                                          "created_utc": 1742410842.0,
                                          "replies": [
                                            {
                                              "id": "mio46gg",
                                              "author": "QuaternionsRoll",
                                              "body": "But it doesn\u2019t make sense. The memory bandwidth of the Mac mini tops out at 273 GB/s, while the 5090 hits 1792 GB/s. Macs may use less power, but they don\u2019t even come close to matching the capabilities of this hardware.\n\nIf the point is that you can do less with a less powerful machine, then sure\u2026 I could say the same about a Ti-84. Did you know it can run models with up to 256 parameters?",
                                              "score": 0,
                                              "created_utc": 1742411827.0,
                                              "replies": [
                                                {
                                                  "id": "mio5emk",
                                                  "author": "None",
                                                  "body": "[removed]",
                                                  "score": 1,
                                                  "created_utc": 1742412188.0,
                                                  "replies": [
                                                    {
                                                      "id": "mio605k",
                                                      "author": "QuaternionsRoll",
                                                      "body": "The Ti-84 hits the same metrics running a 256 parameter model as the Mac mini hits running a 7B parameter model as the DGX station hits running a 405B parameter model. What\u2019s your point?",
                                                      "score": 1,
                                                      "created_utc": 1742412361.0,
                                                      "replies": [
                                                        {
                                                          "id": "miopqgp",
                                                          "author": "None",
                                                          "body": "[removed]",
                                                          "score": 1,
                                                          "created_utc": 1742418239.0,
                                                          "replies": [
                                                            {
                                                              "id": "mipg51t",
                                                              "author": "QuaternionsRoll",
                                                              "body": "No, it definitely will\u2026 I\u2019m saying that Nvidia chips use as much power as they do for a reason.",
                                                              "score": 1,
                                                              "created_utc": 1742426473.0,
                                                              "replies": []
                                                            }
                                                          ]
                                                        }
                                                      ]
                                                    }
                                                  ]
                                                }
                                              ]
                                            }
                                          ]
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": "mijtlrw",
                      "author": "Astroloan",
                      "body": "I think a refrigerator might use more power (watt hours) in the long run because it runs all day everyday, but it probably uses less wattage than a 1000w gpu. Probably only half as much.",
                      "score": -4,
                      "created_utc": 1742350168.0,
                      "replies": [
                        {
                          "id": "mikl9l2",
                          "author": "Dudeonyx",
                          "body": "Much less than half, usually 120 to 200w. 5 to 8 times less power.\n\nThere's a power draw spike for a second or so when it's first turned on but that doesn't really matter",
                          "score": 1,
                          "created_utc": 1742362071.0,
                          "replies": []
                        },
                        {
                          "id": "mikw3af",
                          "author": "_Dreamer_Deceiver_",
                          "body": "Do you think they're just going to be modelling for 2 minutes or something? If someone is buying a dedicated machine for modelling it's going to be running most of the time",
                          "score": 1,
                          "created_utc": 1742368622.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "id": "mijvfbn",
              "author": "renome",
              "body": "Damn, how much electricity will these bad boys suck up?",
              "score": 9,
              "created_utc": 1742350811.0,
              "replies": [
                {
                  "id": "mik3zxh",
                  "author": "xiodeman",
                  "body": "Don\u2019t worry, the expansion port has a dangling nuclear reactor",
                  "score": 20,
                  "created_utc": 1742354020.0,
                  "replies": []
                },
                {
                  "id": "mil8471",
                  "author": "worotan",
                  "body": "But don\u2019t worry, we\u2019re taking climate change seriously, so you don\u2019t need to reduce consumption, just keep buying more stuff and hope the problem disappears magically. Whatever you do, don\u2019t feel any responsibility to do anything but keep buying stuff, despite that being specifically what climate scientists are now shouting at us is disastrous.",
                  "score": 12,
                  "created_utc": 1742376472.0,
                  "replies": [
                    {
                      "id": "mil95wf",
                      "author": "Anduin1357",
                      "body": "All that this proves is that the only way to scale eco-friendly energy is up, not sideways. Besides, the reason why AI is energy intensive at the moment is because of how immature AI is today. When hardware is ready to fossilize mainstream inferencing, we'll have ASIC NPUs take over and drive down the kW/prompt token/inference token.\n\nNPUs are specifically the hardware that we will all be obligated to buy and replace GPUs for AI use, which would probably be like 5 years away (2 hardware cycles).",
                      "score": -6,
                      "created_utc": 1742377131.0,
                      "replies": [
                        {
                          "id": "milqq6l",
                          "author": "worotan",
                          "body": "So we just tell climate change to pause while we sort things out?\n\nDoesn\u2019t work like that. How about you deal with it in the real world, rather than repeating marketing pr?\n\nWe need to reduce consumption of resources, not increase them. \n\nThat\u2019s called serious climate science.",
                          "score": 2,
                          "created_utc": 1742385054.0,
                          "replies": [
                            {
                              "id": "milu7kt",
                              "author": "Anduin1357",
                              "body": "And why should I reduce my consumption if my life can only get better by increasing consumption? It has been more than a decade of climate policies and while there was some good, a lot of it shows that even the climate activists aren't serious about the movement.\n\nAt this rate, I would much rather that humanity fix our governance so that we are prepared for legitimate change, not the current 'shame the individuals' campaign going on right now. You want to reduce my climate impact? Go nuclear. Fusion, or fission; I don't care and you can't make me not consume.\n\nEdit: By the way, those weren't marketing PR. They were facts.",
                              "score": 0,
                              "created_utc": 1742386506.0,
                              "replies": []
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": "mikr5qe",
                  "author": "Spirited-Pause",
                  "body": "Yes",
                  "score": 4,
                  "created_utc": 1742365534.0,
                  "replies": []
                },
                {
                  "id": "mimm6ij",
                  "author": "econpol",
                  "body": "However much it is, it'll be worth it for some furry romance novel brain storming sessions.",
                  "score": 2,
                  "created_utc": 1742396140.0,
                  "replies": []
                },
                {
                  "id": "mil5ibo",
                  "author": "Fairuse",
                  "body": "Unless there are multiple GPU dies, it sounds like it will basically use as much power as a typical GPU. The main thing with these devices is larger fast RAM, which doesn\u2019t take that much power to run.",
                  "score": 3,
                  "created_utc": 1742374818.0,
                  "replies": []
                }
              ]
            },
            {
              "id": "mijvi9a",
              "author": "Optimus_Prime_Day",
              "body": "At what cost though? 10k?",
              "score": 5,
              "created_utc": 1742350840.0,
              "replies": [
                {
                  "id": "mik6kop",
                  "author": "ye_olde_green_eyes",
                  "body": ">Since the systems will be manufactured by different companies, Nvidia did not mention pricing for the units. However, in January, Nvidia mentioned that the base-level configuration for a DGX Spark-like computer would retail for around $3,000.",
                  "score": 8,
                  "created_utc": 1742355067.0,
                  "replies": [
                    {
                      "id": "mik8j34",
                      "author": "Bangaladore",
                      "body": "Key word base level.",
                      "score": 10,
                      "created_utc": 1742355880.0,
                      "replies": []
                    },
                    {
                      "id": "mikxrq5",
                      "author": "AndersDreth",
                      "body": "In this day and age I wouldn't pay that kind of money, but if A.I keeps getting smarter I'll bet we're all scrambling to get the best A.I just like we do with graphic cards.",
                      "score": -5,
                      "created_utc": 1742369719.0,
                      "replies": [
                        {
                          "id": "milhm2u",
                          "author": "VampireFrown",
                          "body": "I wouldn't, because I don't have room temperature IQ, and can adequately research and create stuff for myself.",
                          "score": 0,
                          "created_utc": 1742380612.0,
                          "replies": [
                            {
                              "id": "milwvsu",
                              "author": "Primary_Opal_6597",
                              "body": "Okay but\u2026 have you ever tried finding a recipe online?",
                              "score": 1,
                              "created_utc": 1742387579.0,
                              "replies": [
                                {
                                  "id": "mily7gb",
                                  "author": "VampireFrown",
                                  "body": "...Yes? It's not hard?",
                                  "score": 2,
                                  "created_utc": 1742388097.0,
                                  "replies": []
                                }
                              ]
                            },
                            {
                              "id": "millxdm",
                              "author": "AndersDreth",
                              "body": "Because that's the only thing A.I is used for /s",
                              "score": 2,
                              "created_utc": 1742382829.0,
                              "replies": [
                                {
                                  "id": "milo733",
                                  "author": "VampireFrown",
                                  "body": "And what research applications do you envisage AI being useful for from the comfort of your own fucking bedroom, lol?",
                                  "score": -2,
                                  "created_utc": 1742383914.0,
                                  "replies": [
                                    {
                                      "id": "milppq1",
                                      "author": "AndersDreth",
                                      "body": "You don't get it do you? In a couple of decades your 'Alexa' isn't just some dumb microphone that can tell you fart jokes and order things from Amazon. Everyone is going to have an A.I that can actually do shit reliably, but how reliable depends on the solution you end up going for.",
                                      "score": 1,
                                      "created_utc": 1742384609.0,
                                      "replies": [
                                        {
                                          "id": "milur58",
                                          "author": "VampireFrown",
                                          "body": "No, I do get it, and likely to a far deeper degree of technical expertise than you, given that I've been in the mix for >10 years by now, and not merely the past couple of years like most others.\n\nYou didn't answer my question. What do you specifically envisage needing an AI model for in your own bedroom? Let's assume it does whatever the thing is perfectly accurately, sure - what do you need it for?",
                                          "score": -1,
                                          "created_utc": 1742386726.0,
                                          "replies": [
                                            {
                                              "id": "mim23y1",
                                              "author": "AndersDreth",
                                              "body": "Partly for the same reason I would prefer outright buying a house or a car instead of leasing them, and partly because I consider the things I do in my bedroom or home for that matter, private.\n\nI don't trust companies for shit when it comes to privacy, and with good reason, so why in the everliving fuck would I want to subscribe to a cloud-based A.I where there's concerns like latency due to traffic, concerns about privacy, and concerns about how well the long-term memory of the A.I is maintained?",
                                              "score": 1,
                                              "created_utc": 1742389538.0,
                                              "replies": []
                                            }
                                          ]
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            },
                            {
                              "id": "millimg",
                              "author": "684beach",
                              "body": "Says that and then says \u201cressarch\u201d. Who knows, maybe in your research you\u2019re the type of person to confuse lighting with lightning.",
                              "score": 0,
                              "created_utc": 1742382625.0,
                              "replies": []
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": "mijvnno",
                  "author": "Bangaladore",
                  "body": "Probably more. Who knows",
                  "score": 2,
                  "created_utc": 1742350894.0,
                  "replies": []
                },
                {
                  "id": "mik6ttu",
                  "author": "typo180",
                  "body": "$3k with 1TB SSD, $4k with 4TB.",
                  "score": 2,
                  "created_utc": 1742355170.0,
                  "replies": [
                    {
                      "id": "mik8nci",
                      "author": "Bangaladore",
                      "body": "That doesn\u2019t get you much gpu memory. They will have many tiers at many times higher prices",
                      "score": 2,
                      "created_utc": 1742355930.0,
                      "replies": [
                        {
                          "id": "mik98yj",
                          "author": "typo180",
                          "body": "Ah, I misread the context when I replied. Yes, I think those prices get you 128 GB of unified memory.",
                          "score": 2,
                          "created_utc": 1742356189.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": "mim0if8",
                  "author": "lostinspaz",
                  "body": "it\u2019s specifically $3999",
                  "score": 1,
                  "created_utc": 1742388953.0,
                  "replies": []
                },
                {
                  "id": "mil5ynh",
                  "author": "Fairuse",
                  "body": "At $10k for 784GB of RAM, it would beat the shit out of the newly released M3 Ultra with 512GB of RAM. The M3 Ultra only saving grace right now is that it has tons of RAM. A 5090 with same amount of RAM would run circles around the M3 Ultra.\u00a0\n\nEven at $15k it will still make the M3 Ultra obsolete.\n\nThe DGX station with 784GB of RAM would need to be like $30k to make anyone consider M3 Ultra 512GB @ $10k.",
                  "score": 1,
                  "created_utc": 1742375107.0,
                  "replies": [
                    {
                      "id": "mil9t4o",
                      "author": "Anduin1357",
                      "body": "288GB HBM3e paired with an ARM CPU\n\nYeah no. What's the use case? Be an inference and training server?",
                      "score": 0,
                      "created_utc": 1742377523.0,
                      "replies": []
                    }
                  ]
                },
                {
                  "id": "miksoa1",
                  "author": "None",
                  "body": "They\u2019ll be leased at 1k a month, but energy bills will be 9k per month, so yes, 10k per month.",
                  "score": -2,
                  "created_utc": 1742366466.0,
                  "replies": []
                }
              ]
            },
            {
              "id": "mikpixi",
              "author": "techieman33",
              "body": "The complaint isn\u2019t about the memory. It\u2019s that fab time is going to making AI chips instead of consumer GPUs. Which is understandable from a business standpoint since there is a lot more profit in AI chips. But it does suck for us consumers.",
              "score": 2,
              "created_utc": 1742364545.0,
              "replies": [
                {
                  "id": "mio19pj",
                  "author": "norbertus",
                  "body": "NVIDIA's consumer business is a side hustle at this point.\n\nLast year, NVIDIA reported $115.2 billion in data center revenue.\n\nTheir gamaing market was a mere 10% that size.",
                  "score": 1,
                  "created_utc": 1742410978.0,
                  "replies": []
                }
              ]
            },
            {
              "id": "mimr314",
              "author": "wuvonthephone",
              "body": "Isn't it sort of wild that the entire business of AAA games is dependent on two companies? Even consoles will be more expensive because of this. First crypto block chain nonsense, now this.",
              "score": 1,
              "created_utc": 1742397615.0,
              "replies": []
            },
            {
              "id": "mik4d9e",
              "author": "None",
              "body": "[deleted]",
              "score": -1,
              "created_utc": 1742354168.0,
              "replies": [
                {
                  "id": "mikqg1o",
                  "author": "frankchn",
                  "body": "I don\u2019t think the RTX Pro 6000 is \u201cprosumer\u201d given that it will probably be north of US$10,000.",
                  "score": 2,
                  "created_utc": 1742365098.0,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "miloiqu",
          "author": "NZafe",
          "body": "AMD has entered the chat",
          "score": 5,
          "created_utc": 1742384064.0,
          "replies": []
        },
        {
          "id": "milxs6m",
          "author": "garry4321",
          "body": "![gif](giphy|j2GLSp9qUpcTfv4w11|downsized)",
          "score": 3,
          "created_utc": 1742387932.0,
          "replies": []
        },
        {
          "id": "mimh3ee",
          "author": "GameOvaries18",
          "body": "No and you will have to pay $250 a year to update Chat GPT which you didn\u2019t want in the first place.",
          "score": 4,
          "created_utc": 1742394564.0,
          "replies": []
        },
        {
          "id": "mijn7gj",
          "author": "PineappleLemur",
          "body": "AMD?",
          "score": 9,
          "created_utc": 1742347960.0,
          "replies": []
        },
        {
          "id": "mijuuwd",
          "author": "MagicOrpheus310",
          "body": "You mean AMD..? Haha",
          "score": 5,
          "created_utc": 1742350608.0,
          "replies": []
        },
        {
          "id": "mijz6dp",
          "author": "Starfox-sf",
          "body": "It will need a special AI-generated \u201cass graphics\u201d card.",
          "score": 8,
          "created_utc": 1742352167.0,
          "replies": []
        },
        {
          "id": "mip18h4",
          "author": "FallingUpwardz",
          "body": "How else do you think they\u2019re paying for these",
          "score": 2,
          "created_utc": 1742421738.0,
          "replies": []
        },
        {
          "id": "miqzakm",
          "author": "Moscato359",
          "body": "Sorry, they make ai compute modules that can also be used to display video, instead",
          "score": 2,
          "created_utc": 1742447408.0,
          "replies": []
        },
        {
          "id": "mijojxu",
          "author": "brickyardjimmy",
          "body": "Never sir, never.",
          "score": 2,
          "created_utc": 1742348430.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mijupoq",
      "author": "MagicOrpheus310",
      "body": "Gaming really is just a hobby for NVIDIA at this point",
      "score": 183,
      "created_utc": 1742350558.0,
      "replies": [
        {
          "id": "mijzzau",
          "author": "None",
          "body": "its less than 10% of their revenue they could give a fuck about gaming",
          "score": 79,
          "created_utc": 1742352466.0,
          "replies": [
            {
              "id": "mikcsdf",
              "author": "santathe1",
              "body": "[David Mitchell](https://youtu.be/om7O0MFkmpw) explains.",
              "score": 43,
              "created_utc": 1742357779.0,
              "replies": [
                {
                  "id": "mikguj0",
                  "author": "bit1101",
                  "body": "How does more than half of USA get this wrong?",
                  "score": 32,
                  "created_utc": 1742359734.0,
                  "replies": [
                    {
                      "id": "mil3ia5",
                      "author": "KrtekJim",
                      "body": "I think they do it \"on accident\"",
                      "score": 17,
                      "created_utc": 1742373504.0,
                      "replies": [
                        {
                          "id": "min0ak0",
                          "author": "None",
                          "body": "I correct this every time I hear it. Also \u201cI seen a guy the other day\u201d. Get it together, America.",
                          "score": 3,
                          "created_utc": 1742400329.0,
                          "replies": [
                            {
                              "id": "miq6j8f",
                              "author": "woodcookiee",
                              "body": "I had never heard this until I took a job in a rural city for a couple years. Could be having a normal, intelligent conversation with someone, then suddenly they tell me about how they \u201cseen\u201d something or someone. wtf",
                              "score": 1,
                              "created_utc": 1742435426.0,
                              "replies": []
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": "miktnpb",
                      "author": "None",
                      "body": "honestly, I understand it its incorrect, but I only ever realize in hindsight that I said it wrong again because I only use the phrase so often now but I grew up saying it wrong a lot.  Old habits die hard",
                      "score": 3,
                      "created_utc": 1742367080.0,
                      "replies": []
                    }
                  ]
                },
                {
                  "id": "minzclq",
                  "author": "blank_isainmdom",
                  "body": "I've been watching Soapbox again lately. Good times.",
                  "score": 2,
                  "created_utc": 1742410426.0,
                  "replies": []
                }
              ]
            },
            {
              "id": "mil2bkd",
              "author": "TotoCocoAndBeaks",
              "body": "Companies do care about ten percent of their revenue.\n\nAnd thats an awful misuse of \u2018could\u2019\n\nSo its just pretty funny that through bad grammar your post ended up being correct",
              "score": 20,
              "created_utc": 1742372713.0,
              "replies": [
                {
                  "id": "milt0rk",
                  "author": "HiddenoO",
                  "body": ">Companies do care about ten percent of their revenue.\n\nThey could likely more than make up for that revenue by investing those wafers into more AI and data centre chips while saving on advertising and gaming-related development.\n\nThe main reason they still care about consumer GPUs is that 1) it's good as advertisement for Nvidia being \"the best\" in the compute market and 2) it's their fallback for when the AI bubble bursts.",
                  "score": 2,
                  "created_utc": 1742386020.0,
                  "replies": [
                    {
                      "id": "minn8d6",
                      "author": "Plebius-Maximus",
                      "body": "Gaming grade chips aren't workstation/server suitable though.\n\nEven the 5090 isn't a full die, it's a defective one which is why it has cores missing Vs the rtx pro 6000. You also cant sell all the low grade stuff like 60-class chips to data centres. They have no need for it.\n\nYou can slap some lights on any kind of GPU and sell it to gamers though. The profit margins on the gaming stuff are still massive. Even if they aren't as high as professional stuff",
                      "score": 2,
                      "created_utc": 1742406919.0,
                      "replies": [
                        {
                          "id": "minvbyp",
                          "author": "None",
                          "body": "[deleted]",
                          "score": 0,
                          "created_utc": 1742409256.0,
                          "replies": [
                            {
                              "id": "miop253",
                              "author": "Plebius-Maximus",
                              "body": ">Nobody is forcing Nvidia to allocate wafers to those consumer cards.\n\nYou diversify your portfolio \n\n>You could slap on extra VRAM and sell them for multiple times the price as workstation GPUs like they're doing with the RTX PRO 6000 now. Even a slightly weaker 4090 with double the VRAM at twice the price would sell like hot cakes.\n\n>Heck, Chinese modded 4090s with 48GB VRAM are selling for $5k+.\n\nWhy is nobody Vram modding a 60 class card then? Of course 4090 and 5090 with extra Vram are expensive and desirable. They're powerful enough to have export restrictions.\n\nNobody cares about 60 class cards as they're not that useful",
                              "score": 0,
                              "created_utc": 1742418039.0,
                              "replies": [
                                {
                                  "id": "mioue9h",
                                  "author": "None",
                                  "body": "[deleted]",
                                  "score": 0,
                                  "created_utc": 1742419649.0,
                                  "replies": [
                                    {
                                      "id": "mipbjqf",
                                      "author": "Plebius-Maximus",
                                      "body": ">So I presume you take back all the rubbish you wrote above?\n\nAre you drunk or can you not see how the points all compliment each other.\n\nNvidia isn't going to put all their eggs into one basket. And also the gaming grade stuff WILL NOT CUT IT for servers and workstations. \n\nThese are not mutually exclusive. Try to fucking comprehend  this\n\n>So you talk about the 5090, have your argument demolished and now you're suddenly talking about 60 class cards? Moving the goal post at its finest here\n\nAre you being deliberately obtuse here? No argument got demolished you ignorant individual. Genuinely have you been drinking or taking substances since your last comment?\n\nNot everything is the same silicon. A GB202 is NOT inside a 60 class card. The 60 class is much cheaper to make, and still has decent profit. While the high end chips are what goes into the top gaming cards (if they're defective) and workstation stuff (if they're not). Nobody is forcing them to do a thing that they still get a lot of profit from? Yeah no shit.\n\nWhy comment when you don't understand\n\nAnd you're the one who said you could slap extra Vram on a 60 class card. You literally quoted my text and responded that. I'm saying they couldn't, as they wouldn't sell well",
                                      "score": 0,
                                      "created_utc": 1742425014.0,
                                      "replies": []
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        },
                        {
                          "id": "miqzmja",
                          "author": "Moscato359",
                          "body": "\"\u00a0You also cant sell all the low grade stuff like 60-class chips\"  \nthey could simply make more of the larger chips, and not order the smaller chips at all\n\n\"Even the 5090 isn't a full die\"  \nThey can fuse off them, and sell them to datacenters as a stepdown model",
                          "score": 0,
                          "created_utc": 1742447589.0,
                          "replies": []
                        }
                      ]
                    },
                    {
                      "id": "min3ps5",
                      "author": "GrayDaysGoAway",
                      "body": "> They could likely more than make up for that revenue by investing those wafers into more AI and data centre chips\n\nNo, they can't. They're already producing that stuff as quickly as they possibly can. The bottleneck is in the packaging, not a lack of chips. GPUs are the only way for them to earn that 10%.",
                      "score": 0,
                      "created_utc": 1742401330.0,
                      "replies": []
                    }
                  ]
                },
                {
                  "id": "mil3pki",
                  "author": "None",
                  "body": "I am aware of the correct way to say it, it's just force of habit.  While they care about gaming, they're not catering prices to gamers because they're aware that 90% of their chips go to data centers or other businesses.  They're willing to lose that 10%, because that 10% used to be more than half of their revenue.  Nvidia has seen an unparalleled increase in valuation, due to the demand from data centers, and now, machine learning. That 10% of revenue this year will be less than 2 by 2030.  You have to be aware that Nvidia's growth is almost entirely driven by these mentioned target markets, in order to have an accurate understanding as possible regarding their attitudes toward the gaming segment.  Does Nvidia care about the gaming market? Of course.  Do they care enough to consider them in their pricing strategy? Not a chance.  Nvidia predicts its revenue to grow to 1 trillion by 2028, and research doesn't deny the possibility.  Gaming revenues are currently 11 billion.  Considering the gaming market is reaching some state of maturity, its growth rate will be nowhere near the emerging AI and cloud computing markets.  I am aware this is reddit and it's easier to make fun of someone's grammar, than it is to employ nuance. I did my best to use correct grammar. If I am wrong, please feel free to correct me.",
                  "score": -11,
                  "created_utc": 1742373639.0,
                  "replies": [
                    {
                      "id": "mil8hj6",
                      "author": "ChinoGambino",
                      "body": "Their business would have to grow by 8x to even come close to $1T in revenue. Maybe Trump will destroy the USD making that possible but it seems impossible. There's only so many wafers that can be bought and products that can be thrown at the market.",
                      "score": 2,
                      "created_utc": 1742376705.0,
                      "replies": [
                        {
                          "id": "min16ac",
                          "author": "None",
                          "body": "bud just say you don't understand the current chip market",
                          "score": 0,
                          "created_utc": 1742400587.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "id": "mimrtbu",
              "author": "_unsinkable_sam_",
              "body": "they could or couldn\u2019t?",
              "score": 3,
              "created_utc": 1742397833.0,
              "replies": []
            }
          ]
        },
        {
          "id": "milhc7p",
          "author": "NotAnADC",
          "body": "im still holding out hope for the shield tv pro 2 that they through together with what amounts to loose change for them",
          "score": 1,
          "created_utc": 1742380460.0,
          "replies": []
        },
        {
          "id": "mijyoi9",
          "author": "Johnson_N_B",
          "body": "Always has been.",
          "score": -7,
          "created_utc": 1742351987.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mijml2s",
      "author": "joestaff",
      "body": "After seeing DeepSeek, I figured home AI servers were going to eventually be a thing. Maybe not a common thing, but not so uncommon that it'd be shocking to see. Like smart lights or outlets.",
      "score": 113,
      "created_utc": 1742347745.0,
      "replies": [
        {
          "id": "mijt3yp",
          "author": "rocket-lawn-chair",
          "body": "They already exist. You can pop a pair of high-vram cards in a chassis with a mobo/processor for LLM models of moderate size. Smaller models can even run on a rasp pi 5.\n\nIt\u2019s surprising what you can already do to run local chat models. It\u2019s really the training of the model that\u2019s most intensive.\n\nThis product seems like it\u2019s built for more than just a local chat bot.",
          "score": 15,
          "created_utc": 1742349992.0,
          "replies": [
            {
              "id": "mijwat6",
              "author": "geekwonk",
              "body": "ugh i really really want to get a 16GB Pi 5 and that 26TOPS AI HAT. i\u2019ve got RAM for days around this house but i don\u2019t game, so i can load up models quickly and watch them spend a bunch of time working on Hello, World",
              "score": 5,
              "created_utc": 1742351123.0,
              "replies": []
            },
            {
              "id": "milv2ja",
              "author": "HiddenoO",
              "body": "The issue is that it's cost-effective for almost nobody.\n\nIf e.g. your average prompt has 1k tokens input and 1k tokens output (\\~2k words each), you can do 2,000 Gemini-Flash 2.0 requests per 1$. Even at 1000 requests a day (which takes heavy use, likely including agents and RAG), that's only \\~$15 a month.\n\nEven if your LLM workstation only cost $2.5k (2x used 3090 and barebones components), it'd take you 14 years until it pays off, and that's assuming cloud LLMs won't get any cheaper.\n\nFlash 2.0 also performs on par with or better than most models/quants you can use with 2x 3090, so you really need very specific reasons (fine-tuning, privacy, etc.) for the local workstation to be worth using. Those exist but the vast majority of people wouldn't pay such a hefty premium for them.",
              "score": 1,
              "created_utc": 1742386854.0,
              "replies": [
                {
                  "id": "mimncpm",
                  "author": "Tatu2",
                  "body": "Privacy I think would be the largest reason. That way the information that you're feeding and receiving, isn't shared out to the internet, and stored in some location, by some other company.",
                  "score": 6,
                  "created_utc": 1742396493.0,
                  "replies": [
                    {
                      "id": "mimph2s",
                      "author": "HiddenoO",
                      "body": "It is, but the vast majority of people don't give nearly as much of a fuck about privacy in that sense as the Reddit privacy evangelists will make you believe.",
                      "score": 5,
                      "created_utc": 1742397134.0,
                      "replies": [
                        {
                          "id": "mimrs9z",
                          "author": "Tatu2",
                          "body": "I agree, even as a security engineer. This seems like a pretty niche product, that I don't see too many use cases for. I don't imagine this will sell well. I could see businesses wanting that, especially if they working with personal health information, but that's not what this product is intended for. It's personal use.",
                          "score": 3,
                          "created_utc": 1742397825.0,
                          "replies": []
                        }
                      ]
                    },
                    {
                      "id": "mimuk1e",
                      "author": "IAMA_Madmartigan",
                      "body": "Yeah that\u2019s the biggest one for me. Being able to link into all my personal files and run things without uploading requests to a server",
                      "score": 2,
                      "created_utc": 1742398651.0,
                      "replies": []
                    }
                  ]
                },
                {
                  "id": "mj0u6b7",
                  "author": "NihilisticAngst",
                  "body": "I agree with you that it's not cost effective, and especially not for an average user.\n\nHowever, depending on what you're doing with the LLMs, you don't need anywhere near 2 3090s. I've been successfully running local LLMs on my personal data with only a 4070 and 12GB of VRAM. Lower end LLM models are also becoming more and more capable as development continues. For many people, running local LLMs is viable with minimal additional investment. Personally, I'm very interested in potentially purchasing one of these AI supercomputers in the future.",
                  "score": 3,
                  "created_utc": 1742583805.0,
                  "replies": [
                    {
                      "id": "mj0wusp",
                      "author": "HiddenoO",
                      "body": "The topic was about buying \"home AI servers\", not running it on your existing machine.\n\nAlso, frankly speaking, if you're not an enthusiast willing to spend the time to mess around with a lot of models and/or fine-tune them, the performance/power expenditure in $ will still be worse than what you get from just using Flash 2/Mistral Small.",
                      "score": 1,
                      "created_utc": 1742584596.0,
                      "replies": []
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "mijngyc",
          "author": "PM_ME_YOUR_KNEE_CAPS",
          "body": "M3 Mac Ultra",
          "score": 41,
          "created_utc": 1742348050.0,
          "replies": [
            {
              "id": "mik8piu",
              "author": "f-elon",
              "body": "My M2 Ultra runs 250GB LLM\u2019s without a hitch.",
              "score": 12,
              "created_utc": 1742355955.0,
              "replies": [
                {
                  "id": "mimysw3",
                  "author": "mdonaberger",
                  "body": "feel how you will about Apple, this shit right here is why i have been yelling to anyone who would listen about ARM servers since 2003. my first entrypoint to self-hosting was the [TonidoPlug](https://www.filecloud.com/supportdocs/display/docs/Introduction+to+TonidoPlug), which cost a total of $2 to run 24/7 for a whole year.",
                  "score": 6,
                  "created_utc": 1742399893.0,
                  "replies": [
                    {
                      "id": "mip4ejx",
                      "author": "Bluedot55",
                      "body": "While apple is making excellent hardware right now, I'm not sure how much of it is arm vs good design and being willing to spend more on the cutting edge node and go for a wider core that's lower on the v/f curve.",
                      "score": 2,
                      "created_utc": 1742422735.0,
                      "replies": []
                    }
                  ]
                },
                {
                  "id": "mip3689",
                  "author": "xxAkirhaxx",
                  "body": "Worth noting that the mac m4 max comes at a similar (albeit cheaper price point) for the same amount of Unified RAM with twice the memory bandwidth. It would be comparable to having a 3070 running 128gb of VRAM. This thing, this AI box they're making is a joke. I think it's meant for people who don't know about locally running models who want something \"that will just work and don't want to learn\" Which is fair I guess. But technically that's always been Apple's job, and I don't like that NVIDIA is outdoing them in the same dept....",
                  "score": 3,
                  "created_utc": 1742422345.0,
                  "replies": []
                },
                {
                  "id": "mimn1ac",
                  "author": "_hephaestus",
                  "body": "What quants? Doesn\u2019t the M2 max out at 192? Probably a better deal than M3 since they didn\u2019t up bandwidth",
                  "score": 2,
                  "created_utc": 1742396397.0,
                  "replies": [
                    {
                      "id": "miof9ko",
                      "author": "f-elon",
                      "body": "Mine is not maxed out..  but yeah ram caps at 192\n\n24 core CPU  \n60 core GPU  \n32 core NE  \n128 GB RAM",
                      "score": 1,
                      "created_utc": 1742415204.0,
                      "replies": [
                        {
                          "id": "mion26z",
                          "author": "_hephaestus",
                          "body": "I mean for the 250gb llms, don\u2019t you have to use some heavy quantization to fit that in 128gb ram?",
                          "score": 1,
                          "created_utc": 1742417460.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "id": "mio0fjy",
              "author": "lucellent",
              "body": "If LLMs are all that you want to run sure... but for CUDA apps it's useless",
              "score": 1,
              "created_utc": 1742410735.0,
              "replies": []
            },
            {
              "id": "mijo7x6",
              "author": "Moist_Broccoli_1821",
              "body": "20k for trash. AI super PC - $3,599",
              "score": -58,
              "created_utc": 1742348313.0,
              "replies": [
                {
                  "id": "mijoeps",
                  "author": "PM_ME_YOUR_KNEE_CAPS",
                  "body": "9.5k, 512GB fast ram, can run deepseek. Can\u2019t do that on anything cheaper",
                  "score": 28,
                  "created_utc": 1742348379.0,
                  "replies": [
                    {
                      "id": "mijzogw",
                      "author": "ndjo",
                      "body": "Quantized, not full r1.",
                      "score": 7,
                      "created_utc": 1742352354.0,
                      "replies": []
                    },
                    {
                      "id": "mijojw0",
                      "author": "Moist_Broccoli_1821",
                      "body": "Never buy apple PC products",
                      "score": -51,
                      "created_utc": 1742348429.0,
                      "replies": [
                        {
                          "id": "mijuik5",
                          "author": "MargielaFella",
                          "body": "Any actual reasoning for this? Or just blind hate?",
                          "score": 20,
                          "created_utc": 1742350489.0,
                          "replies": [
                            {
                              "id": "mijuurg",
                              "author": "NecroCannon",
                              "body": "Windows is definitely not better right now, let\u2019s be real.",
                              "score": 19,
                              "created_utc": 1742350607.0,
                              "replies": [
                                {
                                  "id": "mimlof2",
                                  "author": "Two_Shekels",
                                  "body": "Idk, I absolutely love when windows constantly forces updates on me and inject \u201cAI\u201d bs into literally every component of the system",
                                  "score": 1,
                                  "created_utc": 1742395988.0,
                                  "replies": [
                                    {
                                      "id": "min119z",
                                      "author": "NecroCannon",
                                      "body": "You\u2019re right, when I heard they were taking constant screenshots to train AI, I just thought\n\n\u201cThere\u2019s so many cameras, why not use AI to learn our facial expressions. It\u2019s just AI so, no worries on being toilet cammed\u201d\n\nI love the future where AI is a middleman for our privacy just being completely gone and on 24/7 corporate surveillance.",
                                      "score": 2,
                                      "created_utc": 1742400547.0,
                                      "replies": []
                                    }
                                  ]
                                },
                                {
                                  "id": "mijve6o",
                                  "author": "MargielaFella",
                                  "body": "You replied to the wrong person",
                                  "score": -6,
                                  "created_utc": 1742350799.0,
                                  "replies": [
                                    {
                                      "id": "mijvkh1",
                                      "author": "NecroCannon",
                                      "body": "No I\u2019m just agreeing with you lol Linux and MacOS are looking better by the day",
                                      "score": 14,
                                      "created_utc": 1742350862.0,
                                      "replies": [
                                        {
                                          "id": "mijyy1k",
                                          "author": "Webfarer",
                                          "body": "No sir, this is a space where we hate each other profusely",
                                          "score": 5,
                                          "created_utc": 1742352083.0,
                                          "replies": [
                                            {
                                              "id": "mikmgx6",
                                              "author": "hans_l",
                                              "body": "Aye. Linux users and windows users are natural enemies. Like Linux users and Mac users. Or Linux users and BSD users. Or Linux users and other Linux users.",
                                              "score": 3,
                                              "created_utc": 1742362756.0,
                                              "replies": []
                                            }
                                          ]
                                        },
                                        {
                                          "id": "mik607j",
                                          "author": "MargielaFella",
                                          "body": "True. Yeah Windows has a monopoly on PC gaming so (for now) it has a use case for me. Apples software division is increasingly becoming lazier and introduce more bugs than they fix with every release. \n\nIf Apple keeps going down this trajectory I\u2019ll leave for Linux. \nI\u2019m using an M3 Pro right now but I\u2019ll happily switch to a Thinkpad with Ubuntu or Mint.",
                                          "score": 1,
                                          "created_utc": 1742354835.0,
                                          "replies": []
                                        },
                                        {
                                          "id": "mik3zme",
                                          "author": "DarkKumane",
                                          "body": "As a certified Mac hater for years, the only thing I hate about it nowadays is the overvaluation of Apple, egregious overpricing of products, and lack of modular components/right to repair. Despite all that, I would still pick Mac over Windows right now. Imo Linux is currently king, and it's more accessible than ever right now. I highly recommend checking out a distro like mint or ubuntu to anyone reading this who's on the fence about trying a new OS.",
                                          "score": 0,
                                          "created_utc": 1742354017.0,
                                          "replies": [
                                            {
                                              "id": "mikk3xq",
                                              "author": "None",
                                              "body": "[deleted]",
                                              "score": 1,
                                              "created_utc": 1742361435.0,
                                              "replies": [
                                                {
                                                  "id": "miklukj",
                                                  "author": "MargielaFella",
                                                  "body": "Linux you need to add extensions to make it feel good imo. Scrolling on every distro I used is horrible. But you can tweak it to feel better. Battery management is also terrible so think twice before using it as your main OS on a laptop. \n\nIt\u2019s getting there, but I\u2019m still not sure Linux is viable for a regular person yet.",
                                                  "score": 1,
                                                  "created_utc": 1742362406.0,
                                                  "replies": []
                                                }
                                              ]
                                            }
                                          ]
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    },
                    {
                      "id": "mik30fr",
                      "author": "_RADIANTSUN_",
                      "body": "GPU compute capacity less than a 4080 on a $30k computer lol",
                      "score": -12,
                      "created_utc": 1742353631.0,
                      "replies": [
                        {
                          "id": "mikea7y",
                          "author": "FightOnForUsc",
                          "body": "With what 20x the RAM?",
                          "score": 10,
                          "created_utc": 1742358484.0,
                          "replies": [
                            {
                              "id": "mikej8u",
                              "author": "_RADIANTSUN_",
                              "body": "Ya exactly. Imagine how dogshit the performance of a 4080 would be on 512GB scale model. Apparently the ultra performant R1 4bit quant produces a measly 18tk/s. Lol.",
                              "score": -12,
                              "created_utc": 1742358605.0,
                              "replies": [
                                {
                                  "id": "mikesbp",
                                  "author": "FightOnForUsc",
                                  "body": "What\u2019s your point? If nvidia put 512 GB of 800GB/s RAM on a 4080 it would be ridiculously more expensive than it is. So you\u2019re saying, this thing is better (but it\u2019s not) and then say oh but that\u2019s too expensive (but it provides more).",
                                  "score": 10,
                                  "created_utc": 1742358727.0,
                                  "replies": [
                                    {
                                      "id": "mikey36",
                                      "author": "_RADIANTSUN_",
                                      "body": "The point is that the compute performance will be worse than a $1K GPU for a $30k machine lmao",
                                      "score": -10,
                                      "created_utc": 1742358806.0,
                                      "replies": [
                                        {
                                          "id": "mikf64i",
                                          "author": "FightOnForUsc",
                                          "body": "Except that it costs $9,499. Shop me another GPU with access to 512 GB of RAM or similar for less.",
                                          "score": 7,
                                          "created_utc": 1742358915.0,
                                          "replies": [
                                            {
                                              "id": "miki4ex",
                                              "author": "_RADIANTSUN_",
                                              "body": "Why do you keep trying to change the subject from the objectively horrible compute performance for a $10k machine?",
                                              "score": -2,
                                              "created_utc": 1742360382.0,
                                              "replies": [
                                                {
                                                  "id": "mimqx42",
                                                  "author": "rnobgyn",
                                                  "body": "Man ngl you\u2019re all over the place on this one",
                                                  "score": 1,
                                                  "created_utc": 1742397566.0,
                                                  "replies": [
                                                    {
                                                      "id": "miovhgn",
                                                      "author": "_RADIANTSUN_",
                                                      "body": "How so? It's pretty straightforward. Point out what your problem is specifically. \n\nIt's a GPU weaker than a 5080 so it will have horrible performance for running things at such large scales e.g. Deepseek R1 4bit quant runs at 18tk/s.",
                                                      "score": 0,
                                                      "created_utc": 1742419982.0,
                                                      "replies": []
                                                    }
                                                  ]
                                                }
                                              ]
                                            }
                                          ]
                                        }
                                      ]
                                    }
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": "mijvrc5",
                  "author": "geekwonk",
                  "body": "i think it tops out at around $14K",
                  "score": 1,
                  "created_utc": 1742350930.0,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "miltdi7",
          "author": "smulfragPL",
          "body": "they arleady were for a long time",
          "score": 2,
          "created_utc": 1742386166.0,
          "replies": []
        },
        {
          "id": "mimc9tq",
          "author": "roamingandy",
          "body": "I'd love to properly train one on my writing style, from all the documents i've ever written, and have it answer all emails and such for me, then send to me for editing and approval. \n\nDone well and that could save so much time as the majority of our online communications are a rehash of things we've said or written in the past anyway.",
          "score": 2,
          "created_utc": 1742393049.0,
          "replies": [
            {
              "id": "minrq6c",
              "author": "ilyich_commies",
              "body": "Training it on the stuff you\u2019ve written won\u2019t get it to match your style very well unless you\u2019ve written enough to fill multiple libraries. Unfortunately AI just doesn\u2019t work like that. You\u2019d have better luck training it on all the text you\u2019ve ever read and audio you\u2019ve ever listened to, but it would be impossible to compile a data set like that",
              "score": 2,
              "created_utc": 1742408206.0,
              "replies": []
            },
            {
              "id": "minhd7x",
              "author": "lkn240",
              "body": "My work laptop just got apple intelligence... which will respond to e-mails for you.  It's pretty unimpressive so far.\n\nLike I'd be embarrassed to send out some of the things it comes up with.",
              "score": 1,
              "created_utc": 1742405249.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mik76bv",
          "author": "GregmundFloyd",
          "body": "Ultrahouse 3000",
          "score": 1,
          "created_utc": 1742355315.0,
          "replies": []
        },
        {
          "id": "miwe02s",
          "author": "BluudLust",
          "body": "Already have one. You can offload some layers to the GPU and run the rest on the CPU. It's fast enough.",
          "score": 1,
          "created_utc": 1742519668.0,
          "replies": []
        },
        {
          "id": "mikizjs",
          "author": "ResponsibleTruck4717",
          "body": "I don't know if we will get home servers, at least not just for llm.\n\nThis technology as whole is still in alpha / beta state at best, it's unstable can give wrong answers, sometimes it can't perform simple tasks.\n\nAs the technology mature (if it will survive) the hardware requirements will change and better optimization will be developed.",
          "score": -1,
          "created_utc": 1742360833.0,
          "replies": [
            {
              "id": "miksu5u",
              "author": "Brasou",
              "body": "There's already home LLM available. its slow as tits and they are far perfect. But yeah, its already here just slow.",
              "score": 4,
              "created_utc": 1742366565.0,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "mijwq6k",
      "author": "Spara-Extreme",
      "body": "What's the use case for this outside of researchers and hobbyists?  I can understand a few of these machines hitting the market but can't imagine there's a huge customer base.",
      "score": 37,
      "created_utc": 1742351278.0,
      "replies": [
        {
          "id": "mik94ae",
          "author": "GrandmaPoses",
          "body": "Porn.",
          "score": 44,
          "created_utc": 1742356132.0,
          "replies": [
            {
              "id": "mil7u3q",
              "author": "Bokbreath",
              "body": "There it is",
              "score": 13,
              "created_utc": 1742376296.0,
              "replies": []
            },
            {
              "id": "minvxnr",
              "author": "BevansDesign",
              "body": "You know how you go to a porn site and it's full of awful weird stuff you don't want to see? Imagine if you could go to one and it showed you exactly what you wanted. Or even *created* it automatically. Then you just set up a few Amazon delivery subscriptions and never have to leave your house again.",
              "score": 3,
              "created_utc": 1742409434.0,
              "replies": []
            }
          ]
        },
        {
          "id": "miksz5h",
          "author": "plissk3n",
          "body": "Put in all my documents, mails, browsing history etc. Than it do my taxes, remind me of things which are overdue, give me a tip where I might have seen a product online etc.\n\nAll things I never would want in a cloud service.",
          "score": 18,
          "created_utc": 1742366653.0,
          "replies": [
            {
              "id": "milvkqh",
              "author": "HiddenoO",
              "body": "Half of those you wouldn't want to do locally with current models either (taxes), or you're better off not using LLMs (remind you of things which are overdue).",
              "score": 11,
              "created_utc": 1742387059.0,
              "replies": [
                {
                  "id": "mimq6iz",
                  "author": "plissk3n",
                  "body": "It was more of a thought about the future, and I do think that certain AIs will develop in some kind of personal assistants or even companions.",
                  "score": -1,
                  "created_utc": 1742397345.0,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "mik0v2o",
          "author": "CosmicCreeperz",
          "body": "It doesn\u2019t mean \u201chome AI PC\u201d.  Those many thousands of AI companies (actually, way more than that as everyone is getting into it)  \nhave many tens or hundreds of thousands of data scientists and ML engineers, etc.\n\nI knew a few DS who would kill to run large models locally.",
          "score": 14,
          "created_utc": 1742352800.0,
          "replies": [
            {
              "id": "mik2dky",
              "author": "Spara-Extreme",
              "body": "Sure but those companies also have access to cloud H100's.  That being said, thats a good use case: local development for companies building AI models for their products.",
              "score": 11,
              "created_utc": 1742353380.0,
              "replies": [
                {
                  "id": "mik4lat",
                  "author": "CosmicCreeperz",
                  "body": "Heh, reliable access to cloud H100s is very expensive, since you have to reserve them or you may lose spot instances.  The cheapest instance is $30 an hour.",
                  "score": 6,
                  "created_utc": 1742354257.0,
                  "replies": []
                },
                {
                  "id": "mil0g5t",
                  "author": "AgencyBasic3003",
                  "body": "Local development is not the main use case. Sometimes you have customers which want your product, but they want to run it on premise. In this case you want to run all your models locally so that the data doesn\u2019t leave the network. This can be especially useful if it is really sensitive company data that you don\u2019t want to run on third party infrastructure.",
                  "score": -1,
                  "created_utc": 1742371467.0,
                  "replies": [
                    {
                      "id": "milwnkg",
                      "author": "clumsynuts",
                      "body": "Are you referring to commercial use?",
                      "score": 1,
                      "created_utc": 1742387489.0,
                      "replies": []
                    }
                  ]
                }
              ]
            },
            {
              "id": "mikfaql",
              "author": "FightOnForUsc",
              "body": "No company has 100,000s of data scientists and ML engineers. I don\u2019t know if any have 10,000s. The most you would see would be at google or meta I think and they\u2019re likely in the 1000s",
              "score": 2,
              "created_utc": 1742358978.0,
              "replies": [
                {
                  "id": "mikgk8b",
                  "author": "CosmicCreeperz",
                  "body": "That was across the industry of course, not per company :)\n\nThese computers aren\u2019t going to sell millions but they could sell hundreds of thousands.  Certainly as much or more of a market as the Mac Pro\u2026",
                  "score": 1,
                  "created_utc": 1742359594.0,
                  "replies": [
                    {
                      "id": "mimkpok",
                      "author": "FightOnForUsc",
                      "body": "Oh yes across the industry absolutely!",
                      "score": 1,
                      "created_utc": 1742395691.0,
                      "replies": []
                    }
                  ]
                }
              ]
            }
          ]
        },
        {
          "id": "mikmad5",
          "author": "habitual_viking",
          "body": "I work in a financial institution, we can\u2019t use LLMs because of security risk with sending data to foreign clouds.\n\nHaving AI machines on premise is a huge deal - and at a starting price point of $3000 they could easily compete with cloud subscriptions, if we were using those.",
          "score": 4,
          "created_utc": 1742362654.0,
          "replies": [
            {
              "id": "milwxm6",
              "author": "clumsynuts",
              "body": "They\u2019d more likely setup some on-prem server that could service the entire org rather than buy everyone their own desktop.",
              "score": 5,
              "created_utc": 1742387599.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mikrsej",
          "author": "GuerrillaRodeo",
          "body": "Researchers is the most probable answer. Just feed them textbooks and papers and let them generate answers real quick. I already tried that with medical books on my old-ass 2080 Ti and it's surprisingly good even at this level.",
          "score": 1,
          "created_utc": 1742365917.0,
          "replies": []
        },
        {
          "id": "milwrv6",
          "author": "nicman24",
          "body": "People running deepseek without an Internet connection",
          "score": 1,
          "created_utc": 1742387536.0,
          "replies": []
        },
        {
          "id": "mim4b16",
          "author": "the_tethered",
          "body": "Trading for sure.",
          "score": 1,
          "created_utc": 1742390324.0,
          "replies": []
        },
        {
          "id": "miu06xh",
          "author": "NotAHost",
          "body": "It sounds like 3d printers in a different way. They see one in every house but we\u2019re not going to get there for 30 years.",
          "score": 1,
          "created_utc": 1742493180.0,
          "replies": []
        },
        {
          "id": "mik06ft",
          "author": "shrimel",
          "body": "I imagine some businesses that need to keep their data on prem?",
          "score": 1,
          "created_utc": 1742352541.0,
          "replies": [
            {
              "id": "mik0cjt",
              "author": "Spara-Extreme",
              "body": "Maybe - but nvidia already offers rack servers for that.  This seems like a workstation.",
              "score": 3,
              "created_utc": 1742352605.0,
              "replies": []
            }
          ]
        },
        {
          "id": "mim91py",
          "author": "User1539",
          "body": "Star Trek computers. \n\nWhat LLMs are really good at is understanding commands and forming a plan, then carrying it out.\n\nComputers have been 'hard' to use. You can't just say 'Print this out', you have to know what printer you want to use and all that. \n\nI think the idea is that they want you to feel like your computer is the 1980's cartoon character we all imagined. You'll be able to talk to it, it'll help you come up with ideas, and collaborate with realizing those ideas. \n\nNo more learning Photoshop, or Autodesk. You can just tell your computer you want to 3D print something, and it'll help you design it, figure out how to connect to the printer, and then print it out for you. \n\nThat's what they want. A computer that will tell you when to use Excel, and how to use Excel, then use it for you, to get your report done as fast as possible. \n\nIf things keep moving forward, we'll have appliances from the Jetsons eventually. \n\nI think that's the idea anyway.",
          "score": 0,
          "created_utc": 1742391967.0,
          "replies": []
        },
        {
          "id": "mikxvi1",
          "author": "DirectStreamDVR",
          "body": "Personal assistant \n\nIdeally its paired with a mobile app\n\nImagine chatgpt with an unlimited memory, you could feed it your entire life instead of just 100 memories\n\nYou could connect it with every other LLM and when the thing you ask it is outside of its capabilities it can outsource to chat gpt or grok or whatever. \n\nPair it with your home security system, allowing it to actually watch your cameras and say hey a man is outside with a package, it could learn that the person walking by your house is just your neighbor who walks their dog everyday at this time, it could say \u201chey, there\u2019s a guy outside breaking into your car\u201d it wouldn\u2019t just be a bleep on your phone while you\u2019re sleeping, it could literally yell at you until you\u2019re awake. Or pair it with a speaker outside and have it attempt to scare the intruder away.\n\nPair it with your smart home, you could say hey its kinda getting dark, or literally anything to the regards, you wouldn\u2019t have you memorize the phrase, the system could lookup exactly when the sun will set and turn the lights on at the perfect time\n\nTell it to add things to it grocery list, order it to be delivered\n\nConnect it to your front door bell / let it talk with visitors, tell it how to handle things like deliveries, ie place at back door, go away, whatever \n\nPair it with your cable box, hey do I have any shows on tonight? Yes, in 5 minutes a new episode of lost is on, do you want me to put it on? Nah, just set it to record. Ok\n\nObviously a lot of this is far off, but having the brains inside your home is the first step. Modules that connect with our products will come later.",
          "score": 0,
          "created_utc": 1742369787.0,
          "replies": []
        },
        {
          "id": "milajm9",
          "author": "weid_flex_but_OK",
          "body": "Not now, but in the nearish future, I imagine being able to have one of these machines running my home and helping me in parts of my life. I'd LOVE a Jarvis-type system in my house that I can talk to, quickly jot ideas to or make lists for, bounce those ideas around with,help me organize my projects and calendar, maybe do my taxes, tell me where I can save money, keep check of my house and provide warnings of thing going wrong, answering the door, etc etc etc. \n\nIn my mind, it'll be like having a 24/7 assistant.",
          "score": -1,
          "created_utc": 1742377962.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mijyhay",
      "author": "None",
      "body": "Oh yeah that\u2019s what I want in my life, more fucking AI.",
      "score": 44,
      "created_utc": 1742351915.0,
      "replies": [
        {
          "id": "miks3xr",
          "author": "PGMetal",
          "body": "Do you know what this even is? It seems like you don't understand what anything in the title means.",
          "score": -31,
          "created_utc": 1742366117.0,
          "replies": [
            {
              "id": "mikweff",
              "author": "PoshInBoost",
              "body": "Not the OP, but I turn off AI features in all my devices. If the selling point of there is having AI then why get one? The technology isn't reliable enough yet, and there's more effort going into monetising the users than improving accuracy, so it's not going to be good any time soon.",
              "score": 21,
              "created_utc": 1742368825.0,
              "replies": [
                {
                  "id": "mikzm79",
                  "author": "Dereklewis930",
                  "body": "You\u2019re not the target, not everything is made for you",
                  "score": -14,
                  "created_utc": 1742370921.0,
                  "replies": []
                },
                {
                  "id": "miltla9",
                  "author": "smulfragPL",
                  "body": "because it is reliable and incredibly useful and you either don't have a use case or don't know how to use it",
                  "score": -12,
                  "created_utc": 1742386255.0,
                  "replies": [
                    {
                      "id": "mirgfxb",
                      "author": "PoshInBoost",
                      "body": "If it could be trusted I'd have plenty of use cases \n\nWhen it fails obviously it's fine, you can reject the output. It's the subtle fails that cause the problems.",
                      "score": 1,
                      "created_utc": 1742457960.0,
                      "replies": []
                    }
                  ]
                }
              ]
            }
          ]
        }
      ]
    },
    {
      "id": "mijnpd0",
      "author": "Ok_Transition9957",
      "body": "I just want video games",
      "score": 42,
      "created_utc": 1742348132.0,
      "replies": [
        {
          "id": "mijpk5z",
          "author": "themikker",
          "body": "A big barrier for modern AI in video games is that they rely on the slow, online models. Been theorycrafting a few ways AI could be used for story crafting and game mastering, but to run stuff like that offline (at the same time as the game, mind) requires more oomph than most PC gamers have.\n\nIf offline support like this becomes more popular, it could have a significant impact... if used correctly, of course.",
          "score": -44,
          "created_utc": 1742348775.0,
          "replies": [
            {
              "id": "mikf31a",
              "author": "renaissance_man__",
              "body": "Practically no game uses neural nets for AI. They all use state machines / behavior trees.",
              "score": 19,
              "created_utc": 1742358874.0,
              "replies": [
                {
                  "id": "mil66t5",
                  "author": "themikker",
                  "body": "Obviously. That's why I said modern AI. Having a game able to use a LLM to produce on the fly scenarios for generative storylines is very different from how AI is implemented in most games. Even how opponents plan in strategy games could have an LLM, but I'm thinking more in terms of \"generate random quests\" for an RPG or something. There would be some value in that if it could be done live and local. That wouldn't apply to most games ofc.",
                  "score": -12,
                  "created_utc": 1742375251.0,
                  "replies": [
                    {
                      "id": "mimmhzh",
                      "author": "renaissance_man__",
                      "body": "An llm outputting mountains of infinite slop vs handcrafted quests, and you choose the llm...",
                      "score": 4,
                      "created_utc": 1742396237.0,
                      "replies": [
                        {
                          "id": "mimqjf9",
                          "author": "Bluedot55",
                          "body": "There's some interesting use cases for this already, take a look at the llm Skyrim mods. One of them is called CHIM, afaik. \n\nIt allows for some interesting stuff, like you could walk up to a random NPC, and ask them what's in the cave near the village, and they might tell you. Then ask if they want to help clear out the cave, but then they reply that they want something for it, so you have to barter with them for a price or an item.",
                          "score": 1,
                          "created_utc": 1742397453.0,
                          "replies": []
                        },
                        {
                          "id": "mimo18x",
                          "author": "themikker",
                          "body": "I don't understand the downvotes.\n\nI'm not saying \"stop designing games and just make chatGPT generate it\", it's about how to leverage AI in a way that supports game development. A lot of games use random content generation, and not just for Roguelikes that are build via an algorithm. Skyrim and Left 4 Dead are other examples. I imagine that a lot of the content would still be hand crafted in those scenarios. It would be a minor segment of a larger whole.\n\nGetting it to not spit out slop is a huge challenge, which was why I also added \"if used correctly\". Not even going into the mess of generating art assets with it either. You would have to do a lot of manipulations and context for prompting before it could be useful. It won't work at all out of the box, it would require a lot of fine tuning, or a lot of pre-work, depending on what Nvidia provides in terms of software.",
                          "score": -5,
                          "created_utc": 1742396699.0,
                          "replies": []
                        }
                      ]
                    },
                    {
                      "id": "milh1lb",
                      "author": "Emadec",
                      "body": "AI would have to stop hallucinating things that don't exist first",
                      "score": 3,
                      "created_utc": 1742380299.0,
                      "replies": [
                        {
                          "id": "mimqc1t",
                          "author": "Bluedot55",
                          "body": "There's some interesting use cases for this already, take a look at the llm Skyrim mods. One of them is called CHIM, afaik. \n\nIt allows for some interesting stuff, like you could walk up to a random NPC, and ask them what's in the cave near the village, and they might tell you. Then ask if they want to help clear out the cave, but then they reply that they want something for it, so you have to barter with them for a price or an item. \n\nIt can get far more complex than anything that could be manually done, although as you said, it can occasionally mess up. But I think games are one of the situations where if it's right 90% of the time or more, we can work with that.",
                          "score": 1,
                          "created_utc": 1742397391.0,
                          "replies": []
                        }
                      ]
                    }
                  ]
                }
              ]
            },
            {
              "id": "mik7nlx",
              "author": "TanmanG",
              "body": "From my boots on the ground market research, most developers just use FSMs or behavior trees.",
              "score": 4,
              "created_utc": 1742355513.0,
              "replies": []
            }
          ]
        }
      ]
    },
    {
      "id": "mik9b9j",
      "author": "Adrian-The-Great",
      "body": "I am utterly confused about the direction on nvidia over the next couple of years. It\u2019s like they have outgrown graphics cards and now everything is focused on ai, ai developments, ai project management and now desktop pcs.",
      "score": 4,
      "created_utc": 1742356217.0,
      "replies": [
        {
          "id": "mikcbn5",
          "author": "agitatedprisoner",
          "body": "Sounds like you've got it.  Nvidia is planning to provide the compute for the dawning age of AI and AI robots.",
          "score": 13,
          "created_utc": 1742357567.0,
          "replies": []
        },
        {
          "id": "mikrlm7",
          "author": "Dragonasaur",
          "body": "AI as the current fad, but quantum computing as the next fad (to analyze/compute data extrapolated thanks to AI)",
          "score": 0,
          "created_utc": 1742365803.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mijn326",
      "author": "Books_for_Steven",
      "body": "I was looking for a way to accelerate the collection of my personal data",
      "score": 55,
      "created_utc": 1742347917.0,
      "replies": [
        {
          "id": "mik01dh",
          "author": "None",
          "body": "if its local and offline how can they collect your data?",
          "score": 28,
          "created_utc": 1742352488.0,
          "replies": [
            {
              "id": "mikhyuu",
              "author": "None",
              "body": "[deleted]",
              "score": -14,
              "created_utc": 1742360301.0,
              "replies": [
                {
                  "id": "mikxwuq",
                  "author": "Gaeus_",
                  "body": "You can run a local AI on a consumer PC right now, and it's fully offline.",
                  "score": 21,
                  "created_utc": 1742369812.0,
                  "replies": []
                },
                {
                  "id": "mikjb90",
                  "author": "None",
                  "body": "if its not connected to the internet how is that possible? This is a genuine question I dont know much about AI",
                  "score": 18,
                  "created_utc": 1742361005.0,
                  "replies": [
                    {
                      "id": "mikxdn7",
                      "author": "whatnowwproductions",
                      "body": "It's not, it's fear mongering.",
                      "score": 13,
                      "created_utc": 1742369463.0,
                      "replies": [
                        {
                          "id": "mimnram",
                          "author": "Tatu2",
                          "body": "There's always a networking/security joke in the industry. How do you make a secure network? Don't connect it. It's funny, because it's true.",
                          "score": 1,
                          "created_utc": 1742396616.0,
                          "replies": []
                        }
                      ]
                    },
                    {
                      "id": "miveaib",
                      "author": "almond5",
                      "body": "No one answered your question so I can. You can make your own models, LLMs, image detector, etc., without being online. If you have vast amounts of training data, you'll want a GPU that can process the data quickly JUST for training. PyTorch and Tensorflow are popular APIs for doing this locally. \n\nFor many models, except maybe LLMs like DeepSeek, you don't need much processing power once the model is trained. You can just use the CPU on a Raspberry Pi to do the image detection once a model is trained. The whole process is a basic way of using layers of weights for neural networks or least mean square calculations for prediction algorithms",
                      "score": 1,
                      "created_utc": 1742507771.0,
                      "replies": [
                        {
                          "id": "mivigt2",
                          "author": "None",
                          "body": "So when these devices release, will we have to train them or will they come pre-trained?  Will we be able to integrate these machines into our homes and allow us to just tell our home into a live in LLM?",
                          "score": 1,
                          "created_utc": 1742509084.0,
                          "replies": [
                            {
                              "id": "mivljrz",
                              "author": "almond5",
                              "body": "I probably should of read the article \ud83d\ude05. Forget what i said about training. These computers simplify using large models like LLMs (chatgpt, grok), vision models, and diffusion models (text to image) to run very large (millions to billions of parameters) pre-trained models quickly.\n\nI bet they will be good in medical fields and such for trying to identify illnesses from image scans, etc., on an efficient basis.",
                              "score": 1,
                              "created_utc": 1742510066.0,
                              "replies": [
                                {
                                  "id": "mivnbpr",
                                  "author": "None",
                                  "body": "absent of corruption, I would love this for law enforcement and prosecution.  Feed a transcript of a testimony to search for inconsistencies or events that don't add up.  Could also go through unsolved cases and connect dots.  The medical field is where I hope it shines because my dad was diagnosed with diabetes in august 2020 and he died of stage 4 pancreatic cancer 3 months later.  The doctor didnt want to waste his time, but an llm would be able to do it faster",
                                  "score": 1,
                                  "created_utc": 1742510641.0,
                                  "replies": []
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                },
                {
                  "id": "mikjgfl",
                  "author": "None",
                  "body": "[deleted]",
                  "score": -13,
                  "created_utc": 1742361082.0,
                  "replies": []
                }
              ]
            }
          ]
        },
        {
          "id": "mijsjvz",
          "author": "screamtracker",
          "body": "cant they harvest any faster? Huge disappointment so far \ud83d\ude44",
          "score": 3,
          "created_utc": 1742349796.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mil99mf",
      "author": "Semen_K",
      "body": "oh and as a bonus they will become obsolete only in 2 years, not three",
      "score": 7,
      "created_utc": 1742377196.0,
      "replies": []
    },
    {
      "id": "mik4nh7",
      "author": "Deepwebexplorer",
      "body": "IF\u2026I could trust it to manage my data and security locally\u2026.IF, it would be incredible. But I\u2019m not sure what is going to make me want to trust it fully. Maybe AGI happens when it can convince us to trust it.",
      "score": 4,
      "created_utc": 1742354282.0,
      "replies": []
    },
    {
      "id": "milgwug",
      "author": "Emadec",
      "body": "Now why would I want a random hallucinated sentence generator at home that can't count fingers",
      "score": 4,
      "created_utc": 1742380228.0,
      "replies": []
    },
    {
      "id": "mimzphp",
      "author": "Pantim",
      "body": "Can they please stop cranking out more AI devices and focus on the hallucinating problem?\u00a0\n\n\nYet another study just came out showing that they are typically wrong 60% of the time. Which mind you, is the case for general internet searches anyway... But still,AI needs to be held to a higher standard.",
      "score": 4,
      "created_utc": 1742400157.0,
      "replies": [
        {
          "id": "miotoz5",
          "author": "IFunkymonkey",
          "body": "Can you please link one of those studies?",
          "score": 1,
          "created_utc": 1742419435.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mijqjls",
      "author": "_dactor_",
      "body": "Hard pass lol",
      "score": 7,
      "created_utc": 1742349109.0,
      "replies": []
    },
    {
      "id": "mijqrbm",
      "author": "ArseBurner",
      "body": "Reminds me of the old SGI workstations. Cool stuff.",
      "score": 2,
      "created_utc": 1742349184.0,
      "replies": []
    },
    {
      "id": "milxh99",
      "author": "alidan",
      "body": "show me the good use case for this and I may be ok with it. \n\nI want ai to type what I say, I want local queries of things, not sending it off the the cloud to do what can be done with no ai and 5gb of ram.",
      "score": 2,
      "created_utc": 1742387814.0,
      "replies": [
        {
          "id": "mj14owo",
          "author": "NihilisticAngst",
          "body": "You can get the AI models to make arbitrary decisions with workflow automation software like n8n. You can set this up right now locally and have the AI make workflow decisions based on your programmed inputs or other triggers. The benefit of this is that you can potentially avoid needing to hardcode a rigid decision tree, and allow the AI to have some agency in making decisions for you. This does have to be tweaked and refined to get consistent output, but consistent output is achievable even with lightweight local models, depending on the complexity of the decision you are wanting the LLM to make. And thus, at least as far as the use case I'm suggesting, the only benefit I could really see this AI desktop computer bringing is a more highly performant environment that can potentially do more powerful things with AI than current consumer desktops can. The thing is though is that even with a moderately powerful GPU, you can already run successful models that are able to run local queries and such. I'm not exactly sure what type of processes you might need a more powerful AI desktop PC to do that a typical system that exists today can't. Maybe advanced analysis of large local datasets?",
          "score": 1,
          "created_utc": 1742586912.0,
          "replies": []
        }
      ]
    },
    {
      "id": "milzcos",
      "author": "Dorraemon",
      "body": "Who asked for this",
      "score": 2,
      "created_utc": 1742388530.0,
      "replies": []
    },
    {
      "id": "mimf9gu",
      "author": "DLiltsadwj",
      "body": "What\u2019s the advantage of running it locally?",
      "score": 2,
      "created_utc": 1742393994.0,
      "replies": [
        {
          "id": "mj154kv",
          "author": "NihilisticAngst",
          "body": "Security of your data not being trusted with a third-party, cloud-based software. Also, continued ability to use your LLM models even during an ongoing Internet outage (you could run your system off a electric generator and still be able to use it).",
          "score": 1,
          "created_utc": 1742587039.0,
          "replies": []
        }
      ]
    },
    {
      "id": "min3kcw",
      "author": "Macqt",
      "body": "Hard pass.",
      "score": 2,
      "created_utc": 1742401286.0,
      "replies": []
    },
    {
      "id": "mikming",
      "author": "Leetter",
      "body": "\"These desktop systems, first previewed as \"Project DIGITS\" in January, aim to bring AI capabilities to developers, researchers, and data scientists who need to prototype, fine-tune, and run large AI models locally\"",
      "score": 1,
      "created_utc": 1742362784.0,
      "replies": []
    },
    {
      "id": "milvk9g",
      "author": "RiderLibertas",
      "body": "Looks like I'll be continuing to build my own super computers for the forseeable future.",
      "score": 1,
      "created_utc": 1742387054.0,
      "replies": []
    },
    {
      "id": "mim7o2f",
      "author": "T1mely_P1neapple",
      "body": "conversational search only $15k!",
      "score": 1,
      "created_utc": 1742391502.0,
      "replies": []
    },
    {
      "id": "mimc91p",
      "author": "cmoz226",
      "body": "I will pay anything for a chatbot!  Sign me up",
      "score": 1,
      "created_utc": 1742393041.0,
      "replies": []
    },
    {
      "id": "mipkxb1",
      "author": "drdailey",
      "body": "My bet is $30k",
      "score": 1,
      "created_utc": 1742428024.0,
      "replies": []
    },
    {
      "id": "mir4wsp",
      "author": "BrokkelPiloot",
      "body": "No thanks. I don't need crappy AI.",
      "score": 1,
      "created_utc": 1742450621.0,
      "replies": []
    },
    {
      "id": "mis3h76",
      "author": "giomancr",
      "body": "Am I the only one here who just wants a gaming pc or a lifelike sex robot and nothing in between?",
      "score": 1,
      "created_utc": 1742471261.0,
      "replies": []
    },
    {
      "id": "mikj8hf",
      "author": "Itsatinyplanet",
      "body": "***Beware of sweaty-five-head zuckerberg lizards*** offering AI models that \"run locally\".",
      "score": 1,
      "created_utc": 1742360964.0,
      "replies": [
        {
          "id": "milylh3",
          "author": "dch528",
          "body": "I don\u2019t understand the joke. You can already run lots of models locally, and with consumer hardware. At no extra cost. From Zuckerberg, too.",
          "score": -1,
          "created_utc": 1742388245.0,
          "replies": []
        }
      ]
    },
    {
      "id": "mijt612",
      "author": "ohiocodernumerouno",
      "body": "not enough vram",
      "score": 1,
      "created_utc": 1742350011.0,
      "replies": []
    },
    {
      "id": "mijy5xh",
      "author": "Johnson_N_B",
      "body": "What does it all mean, Basil?",
      "score": 1,
      "created_utc": 1742351800.0,
      "replies": []
    },
    {
      "id": "mim8yyw",
      "author": "Classic_Cream_4792",
      "body": "Last ditch effort to sell ai\u2026 literally using the personal computer which is the oldest of technology and overly mature in the marketplace. Is that a tower and not laptop too. Wow. They are praying for stock price to go up it seems",
      "score": 1,
      "created_utc": 1742391941.0,
      "replies": []
    },
    {
      "id": "mimyikg",
      "author": "xRockTripodx",
      "body": "I don't fucking want AI locally, or anywhere else, for that matter. All it does is replace human intelligence, ingenuity, and jobs with a fucking algorithm.",
      "score": 1,
      "created_utc": 1742399810.0,
      "replies": []
    },
    {
      "id": "mikjrzq",
      "author": "The_Pandalorian",
      "body": "Awesome! I can't wait to not buy this piece of shit that nobody will want.",
      "score": -1,
      "created_utc": 1742361254.0,
      "replies": []
    },
    {
      "id": "mijrynw",
      "author": "Hoggel123",
      "body": "I don't know if I want local ai yet.",
      "score": 0,
      "created_utc": 1742349594.0,
      "replies": []
    },
    {
      "id": "mijyfg5",
      "author": "NotAPreppie",
      "body": "I already do this on my Mac and gaming PC...",
      "score": 0,
      "created_utc": 1742351896.0,
      "replies": []
    },
    {
      "id": "mikx530",
      "author": "powerexcess",
      "body": "Are these good for training models?",
      "score": 0,
      "created_utc": 1742369306.0,
      "replies": []
    },
    {
      "id": "mil5b6t",
      "author": "ChowAreUs",
      "body": "Im actually excited for these but not the fucking prices.",
      "score": 0,
      "created_utc": 1742374689.0,
      "replies": []
    },
    {
      "id": "mikkzx9",
      "author": "Rfksemperfi",
      "body": "My m1 Mac has done this for quite a while. Why is this news?\n\nI was waaay off, there is a massive difference.\n\n\u201cWhile the M1 Mac can effectively run pre-trained models like LLaMA and Mistral for local tasks such as inference, chatting, and summarizing, it is not suitable for full-scale model training or fine-tuning. These tasks require significant resources, including hundreds of GBs of VRAM and high-end hardware like NVIDIA DGX desktops, which can cost over $10K. Most personal computers, including the M1 Mac, are not designed for such demanding processes. The distinction is between users who run smaller models locally, which is practical and efficient, and researchers who need to build or refine large models, requiring professional-grade equipment.\u201d",
      "score": -4,
      "created_utc": 1742361921.0,
      "replies": [
        {
          "id": "milzqgx",
          "author": "Elios000",
          "body": "M1 Mac cant run the whole model locally this can. this isnt about running the final AI code this for TRAINING the AI in the first place\n\"These desktop systems, first previewed as \"Project DIGITS\" in January, aim to bring AI capabilities to developers, researchers, and data scientists who need to prototype, fine-tune, and run large AI models locally\"",
          "score": 2,
          "created_utc": 1742388672.0,
          "replies": []
        }
      ]
    },
    {
      "id": "miju5vf",
      "author": "Left_on_Pause",
      "body": "I\u2019ll pick one up from Craigslist.",
      "score": -1,
      "created_utc": 1742350365.0,
      "replies": []
    },
    {
      "id": "milto2t",
      "author": "smulfragPL",
      "body": "people on here know so very little about ai lol",
      "score": -1,
      "created_utc": 1742386288.0,
      "replies": []
    },
    {
      "id": "mijoinj",
      "author": "brickyardjimmy",
      "body": "Now *this* is interesting.",
      "score": -9,
      "created_utc": 1742348418.0,
      "replies": [
        {
          "id": "mijyqhg",
          "author": "Johnson_N_B",
          "body": "Now *this* is pod racing!",
          "score": 4,
          "created_utc": 1742352007.0,
          "replies": [
            {
              "id": "mik039i",
              "author": "has_left_the_gam3",
              "body": "![gif](giphy|3owzWgnMr5vS37fBsc)",
              "score": 2,
              "created_utc": 1742352508.0,
              "replies": []
            }
          ]
        }
      ]
    }
  ]
}